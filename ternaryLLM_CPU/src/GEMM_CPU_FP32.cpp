#include "GEMM_CPU_FP32.hpp"

//////////////////////////
//        Column        //
//////////////////////////
// 
//Baseline
void GEMM_CPU_FP32_colMajor_Direct_OpenMP(float* X, std::int8_t* W1, float* result, int rows, int columns, int inners) {
#pragma omp parallel for
    for (int i = 0; i < rows; i++) {
        for (int j = 0; j < columns; j++) {
#pragma omp simd
            for (int k = 0; k < inners; k++) {
                result[j * rows + i] += X[k * rows + i] * W1[k * columns + j];
            } 
        }
    }
}


// Naive
void GEMM_CPU_FP32_colMajor_TCSC_Naive(float* X, int16_t* w_neg_row_ind, int32_t* w_neg_col_ptr, int16_t* w_pos_row_ind, int32_t* w_pos_col_ptr, float* result, int rows, int columns, int inners) {
    // #pragma omp parallel for
    for (int j = 0; j < columns; j++) {
        for (int i = 0; i < rows; i++) {
            float res = 0;

            int neg_start = w_neg_col_ptr[j];
            int pos_start = w_pos_col_ptr[j];

            int neg_end = w_neg_col_ptr[j + 1];
            int pos_end = w_pos_col_ptr[j + 1];

            int neg = neg_end - neg_start;
            int pos = pos_end - pos_start;

            for (int k = 0; k < pos; k++) {
                res += X[w_pos_row_ind[pos_start + k] * rows + i];
            }
            for (int k = 0; k < neg; k++) {
                res -= X[w_neg_row_ind[neg_start + k] * rows + i];
            }
            result[j * rows + i] = res;
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Naive_oneFor(float* X, int16_t* w_neg_row_ind, int32_t* w_neg_col_ptr, int16_t* w_pos_row_ind, int32_t* w_pos_col_ptr, float* result, int rows, int columns, int inners){
    // #pragma omp parallel for
    for (int j = 0; j < columns; j++) {
        for (int i = 0; i < rows; i++) {
            float res = 0;

            int neg_start = w_neg_col_ptr[j];
            int pos_start = w_pos_col_ptr[j];

            int neg_end = w_neg_col_ptr[j + 1];
            int pos_end = w_pos_col_ptr[j + 1];

            int neg = neg_end - neg_start;
            int pos = pos_end - pos_start;

            int min_neg_pos = std::min(neg, pos);

            for (int k = 0; k < min_neg_pos; k++) {
                res -= X[w_neg_row_ind[neg_start + k] * rows + i];
                res += X[w_pos_row_ind[pos_start + k] * rows + i];
            }
            for (int k = min_neg_pos; k < pos; k++) {
                res += X[w_pos_row_ind[pos_start + k] * rows + i];
            }
            for (int k = min_neg_pos; k < neg; k++) {
                res -= X[w_neg_row_ind[neg_start + k] * rows + i];
            }

            result[j * rows + i] = res;
        }
    }
}

//Optimized Unrolled and Blocked
void GEMM_CPU_FP32_colMajor_TCSC_Naive_oneFor_4x4_Unroll(float* X, int16_t* w_neg_row_ind, int32_t* w_neg_col_ptr, int16_t* w_pos_row_ind, int32_t* w_pos_col_ptr, float* result, int rows, int columns, int inners
) {
    // #pragma omp parallel for
    for (int j = 0; j < columns; j += 4) {

        //Pointer to where a column starts and ends
        int col_neg0 = w_neg_col_ptr[j];
        int col_neg1 = w_neg_col_ptr[j + 1];
        int col_neg2 = w_neg_col_ptr[j + 2];
        int col_neg3 = w_neg_col_ptr[j + 3];
        int col_neg4 = w_neg_col_ptr[j + 4];

        int col_pos0 = w_pos_col_ptr[j];
        int col_pos1 = w_pos_col_ptr[j + 1];
        int col_pos2 = w_pos_col_ptr[j + 2];
        int col_pos3 = w_pos_col_ptr[j + 3];
        int col_pos4 = w_pos_col_ptr[j + 4];

        //How many elements per column
        int nb_elem_neg0 = col_neg1 - col_neg0;
        int nb_elem_neg1 = col_neg2 - col_neg1;
        int nb_elem_neg2 = col_neg3 - col_neg2;
        int nb_elem_neg3 = col_neg4 - col_neg3;

        int nb_elem_pos0 = col_pos1 - col_pos0;
        int nb_elem_pos1 = col_pos2 - col_pos1;
        int nb_elem_pos2 = col_pos3 - col_pos2;
        int nb_elem_pos3 = col_pos4 - col_pos3;

        //Number of elements they share
        int min_neg_pos0 = std::min(nb_elem_neg0, nb_elem_pos0);
        int min_neg_pos1 = std::min(nb_elem_neg1, nb_elem_pos1);
        int min_neg_pos2 = std::min(nb_elem_neg2, nb_elem_pos2);
        int min_neg_pos3 = std::min(nb_elem_neg3, nb_elem_pos3);

        for (int i = 0; i < rows; i += 4) {
            float res00 = 0, res01 = 0, res02 = 0, res03 = 0;
            float res10 = 0, res11 = 0, res12 = 0, res13 = 0;
            float res20 = 0, res21 = 0, res22 = 0, res23 = 0;
            float res30 = 0, res31 = 0, res32 = 0, res33 = 0;

            //0
            //Number of elements they share
            for (int k = 0; k < min_neg_pos0; k++) {
                res00 -= X[w_neg_row_ind[col_neg0 + k] * rows + i];
                res01 -= X[w_neg_row_ind[col_neg0 + k] * rows + i + 1];
                res02 -= X[w_neg_row_ind[col_neg0 + k] * rows + i + 2];
                res03 -= X[w_neg_row_ind[col_neg0 + k] * rows + i + 3];
                res00 += X[w_pos_row_ind[col_pos0 + k] * rows + i];
                res01 += X[w_pos_row_ind[col_pos0 + k] * rows + i + 1];
                res02 += X[w_pos_row_ind[col_pos0 + k] * rows + i + 2];                        
                res03 += X[w_pos_row_ind[col_pos0 + k] * rows + i + 3];
            }
            //Rest of elements if there are more positive numbers
            for (int k = min_neg_pos0; k < nb_elem_pos0; k++) {
                res00 += X[w_pos_row_ind[col_pos0 + k] * rows + i];
                res01 += X[w_pos_row_ind[col_pos0 + k] * rows + i + 1];
                res02 += X[w_pos_row_ind[col_pos0 + k] * rows + i + 2];                        
                res03 += X[w_pos_row_ind[col_pos0 + k] * rows + i + 3];
            }
            //Rest of elements if there are more negative numbers
            for (int k = min_neg_pos0; k < nb_elem_neg0; k++) {
                res00 -= X[w_neg_row_ind[col_neg0 + k] * rows + i];
                res01 -= X[w_neg_row_ind[col_neg0 + k] * rows + i + 1];
                res02 -= X[w_neg_row_ind[col_neg0 + k] * rows + i + 2];
                res03 -= X[w_neg_row_ind[col_neg0 + k] * rows + i + 3];
            }
                
            //1
            for (int k = 0; k < min_neg_pos1; k++) {
                res10 -= X[w_neg_row_ind[col_neg1 + k] * rows + i];
                res11 -= X[w_neg_row_ind[col_neg1 + k] * rows + i + 1];
                res12 -= X[w_neg_row_ind[col_neg1 + k] * rows + i + 2];
                res13 -= X[w_neg_row_ind[col_neg1 + k] * rows + i + 3];
                res10 += X[w_pos_row_ind[col_pos1 + k] * rows + i];
                res11 += X[w_pos_row_ind[col_pos1 + k] * rows + i + 1];
                res12 += X[w_pos_row_ind[col_pos1 + k] * rows + i + 2];                        
                res13 += X[w_pos_row_ind[col_pos1 + k] * rows + i + 3];
            }
            for (int k = min_neg_pos1; k < nb_elem_pos1; k++) {
                res10 += X[w_pos_row_ind[col_pos1 + k] * rows + i];
                res11 += X[w_pos_row_ind[col_pos1 + k] * rows + i + 1];
                res12 += X[w_pos_row_ind[col_pos1 + k] * rows + i + 2];                        
                res13 += X[w_pos_row_ind[col_pos1 + k] * rows + i + 3];
            }
            for (int k = min_neg_pos1; k < nb_elem_neg1; k++) {
                res10 -= X[w_neg_row_ind[col_neg1 + k] * rows + i];
                res11 -= X[w_neg_row_ind[col_neg1 + k] * rows + i + 1];
                res12 -= X[w_neg_row_ind[col_neg1 + k] * rows + i + 2];
                res13 -= X[w_neg_row_ind[col_neg1 + k] * rows + i + 3];
            }

            //2
            for (int k = 0; k < min_neg_pos2; k++) {
                res20 -= X[w_neg_row_ind[col_neg2 + k] * rows + i];
                res21 -= X[w_neg_row_ind[col_neg2 + k] * rows + i + 1];
                res22 -= X[w_neg_row_ind[col_neg2 + k] * rows + i + 2];
                res23 -= X[w_neg_row_ind[col_neg2 + k] * rows + i + 3];
                res20 += X[w_pos_row_ind[col_pos2 + k] * rows + i];
                res21 += X[w_pos_row_ind[col_pos2 + k] * rows + i + 1];
                res22 += X[w_pos_row_ind[col_pos2 + k] * rows + i + 2];                        
                res23 += X[w_pos_row_ind[col_pos2 + k] * rows + i + 3];
            }
            for (int k = min_neg_pos2; k < nb_elem_pos2; k++) {
                res20 += X[w_pos_row_ind[col_pos2 + k] * rows + i];
                res21 += X[w_pos_row_ind[col_pos2 + k] * rows + i + 1];
                res22 += X[w_pos_row_ind[col_pos2 + k] * rows + i + 2];                        
                res23 += X[w_pos_row_ind[col_pos2 + k] * rows + i + 3];
            }
            for (int k = min_neg_pos2; k < nb_elem_neg2; k++) {
                res20 -= X[w_neg_row_ind[col_neg2 + k] * rows + i];
                res21 -= X[w_neg_row_ind[col_neg2 + k] * rows + i + 1];
                res22 -= X[w_neg_row_ind[col_neg2 + k] * rows + i + 2];
                res23 -= X[w_neg_row_ind[col_neg2 + k] * rows + i + 3];
            }

            //3
            for (int k = 0; k < min_neg_pos3; k++) {
                res30 -= X[w_neg_row_ind[col_neg3 + k] * rows + i];
                res31 -= X[w_neg_row_ind[col_neg3 + k] * rows + i + 1];
                res32 -= X[w_neg_row_ind[col_neg3 + k] * rows + i + 2];
                res33 -= X[w_neg_row_ind[col_neg3 + k] * rows + i + 3];
                res30 += X[w_pos_row_ind[col_pos3 + k] * rows + i];
                res31 += X[w_pos_row_ind[col_pos3 + k] * rows + i + 1];
                res32 += X[w_pos_row_ind[col_pos3 + k] * rows + i + 2];                        
                res33 += X[w_pos_row_ind[col_pos3 + k] * rows + i + 3];
            }
            for (int k = min_neg_pos3; k < nb_elem_pos3; k++) {
                res30 += X[w_pos_row_ind[col_pos3 + k] * rows + i];
                res31 += X[w_pos_row_ind[col_pos3 + k] * rows + i + 1];
                res32 += X[w_pos_row_ind[col_pos3 + k] * rows + i + 2];                        
                res33 += X[w_pos_row_ind[col_pos3 + k] * rows + i + 3];
            }
            for (int k = min_neg_pos3; k < nb_elem_neg3; k++) {
                res30 -= X[w_neg_row_ind[col_neg3 + k] * rows + i];
                res31 -= X[w_neg_row_ind[col_neg3 + k] * rows + i + 1];
                res32 -= X[w_neg_row_ind[col_neg3 + k] * rows + i + 2];
                res33 -= X[w_neg_row_ind[col_neg3 + k] * rows + i + 3];
            }

            result[j * rows + i] = res00;
            result[(j + 1) * rows + i] = res10;
            result[(j + 2) * rows + i] = res20;
            result[(j + 3) * rows + i] = res30;

            result[j * rows + i + 1] = res01;
            result[(j + 1) * rows + i + 1] = res11;
            result[(j + 2) * rows + i + 1] = res21;
            result[(j + 3) * rows + i + 1] = res31;

            result[j * rows + i + 2] = res02;
            result[(j + 1) * rows + i + 2] = res12;
            result[(j + 2) * rows + i + 2] = res22;
            result[(j + 3) * rows + i + 2] = res32;

            result[j * rows + i + 3] = res03;
            result[(j + 1) * rows + i + 3] = res13;
            result[(j + 2) * rows + i + 3] = res23;
            result[(j + 3) * rows + i + 3] = res33;
                            
        }
    }
}

//Optimized AVX: 8M x 1N 
void GEMM_CPU_FP32_colMajor_TCSC_Naive_oneFor_8x1_AVX2(float* X, int16_t* w_neg_row_ind, int32_t* w_neg_col_ptr, int16_t* w_pos_row_ind, int32_t* w_pos_col_ptr, float* result, int rows, int columns, int inners) {
//    #pragma omp parallel for
    for (int j = 0; j < columns; j++) {
        int neg_start = w_neg_col_ptr[j];
        int neg_end = w_neg_col_ptr[j + 1];

        int pos_start = w_pos_col_ptr[j];
        int pos_end = w_pos_col_ptr[j + 1];

        int nb_elem_neg = neg_end - neg_start;
        int nb_elem_pos = pos_end - pos_start;

        int min_neg_pos0 = std::min(nb_elem_neg, nb_elem_pos);

        for (int i = 0; i < rows; i += 8) {
            __m256 res00 = _mm256_set1_ps(0.0f);

            for (int k = 0; k < min_neg_pos0; k++) {
                __m256 neg_vec = _mm256_load_ps(&X[w_neg_row_ind[neg_start + k] * rows + i]);  
                __m256 pos_vec = _mm256_load_ps(&X[w_pos_row_ind[pos_start + k] * rows + i]);  
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos_vec, neg_vec));  
            }

            for (int k = min_neg_pos0; k < nb_elem_pos; k++) {
                __m256 pos_vec = _mm256_load_ps(&X[w_pos_row_ind[pos_start + k] * rows + i]); 
                res00 = _mm256_add_ps(res00, pos_vec); 
            }

            for (int k = min_neg_pos0; k < nb_elem_neg; k++) {
                __m256 neg_vec = _mm256_load_ps(&X[w_neg_row_ind[neg_start + k] * rows + i]);  
                res00 = _mm256_sub_ps(res00, neg_vec);  
            }

            _mm256_store_ps(&result[j * rows + i], res00);
        }
    }
}


void GEMM_CPU_FP32_colMajor_TCSC_Naive_oneFor_8x1_AVX2_OpenMP(float* X, int16_t* w_neg_row_ind, int32_t* w_neg_col_ptr, int16_t* w_pos_row_ind, int32_t* w_pos_col_ptr, float* result, int rows, int columns, int inners) {
#pragma omp parallel for
    for (int j = 0; j < columns; j++) {
        int neg_start = w_neg_col_ptr[j];
        int neg_end = w_neg_col_ptr[j + 1];

        int pos_start = w_pos_col_ptr[j];
        int pos_end = w_pos_col_ptr[j + 1];

        int nb_elem_neg = neg_end - neg_start;
        int nb_elem_pos = pos_end - pos_start;

        int min_neg_pos0 = std::min(nb_elem_neg, nb_elem_pos);

        for (int i = 0; i < rows; i += 8) {
            __m256 res00 = _mm256_set1_ps(0.0f);

            for (int k = 0; k < min_neg_pos0; k++) {
                __m256 neg_vec = _mm256_load_ps(&X[w_neg_row_ind[neg_start + k] * rows + i]);
                __m256 pos_vec = _mm256_load_ps(&X[w_pos_row_ind[pos_start + k] * rows + i]);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos_vec, neg_vec));
            }

            for (int k = min_neg_pos0; k < nb_elem_pos; k++) {
                __m256 pos_vec = _mm256_load_ps(&X[w_pos_row_ind[pos_start + k] * rows + i]);
                res00 = _mm256_add_ps(res00, pos_vec);
            }

            for (int k = min_neg_pos0; k < nb_elem_neg; k++) {
                __m256 neg_vec = _mm256_load_ps(&X[w_neg_row_ind[neg_start + k] * rows + i]);
                res00 = _mm256_sub_ps(res00, neg_vec);
            }

            _mm256_store_ps(&result[j * rows + i], res00);
        }
    }
}

// 8M x 4N
void GEMM_CPU_FP32_colMajor_TCSC_Naive_oneFor_8x4_AVX2(float* X, int16_t* w_neg_row_ind, int32_t* w_neg_col_ptr, int16_t* w_pos_row_ind, int32_t* w_pos_col_ptr, float* result, int rows, int columns, int inners) {
// #pragma omp parallel for
    for (int j = 0; j < columns; j+=4) {
        //Pointer to where a column starts and ends
        int col_neg0 = w_neg_col_ptr[j];
        int col_neg1 = w_neg_col_ptr[j + 1];
        int col_neg2 = w_neg_col_ptr[j + 2];
        int col_neg3 = w_neg_col_ptr[j + 3];
        int col_neg4 = w_neg_col_ptr[j + 4];

        int col_pos0 = w_pos_col_ptr[j];
        int col_pos1 = w_pos_col_ptr[j + 1];
        int col_pos2 = w_pos_col_ptr[j + 2];
        int col_pos3 = w_pos_col_ptr[j + 3];
        int col_pos4 = w_pos_col_ptr[j + 4];

        int min_end0 = std::min(col_pos1 - col_pos0, col_neg1 - col_neg0);
        int min_end1 = std::min(col_pos2 - col_pos1, col_neg2 - col_neg1);
        int min_end2 = std::min(col_pos3 - col_pos2, col_neg3 - col_neg2);
        int min_end3 = std::min(col_pos4 - col_pos3, col_neg4 - col_neg3);

        for (int i = 0; i < rows; i += 8) {
            __m256 res00 = _mm256_set1_ps(0.0f);
            __m256 res01 = _mm256_set1_ps(0.0f);
            __m256 res02 = _mm256_set1_ps(0.0f);
            __m256 res03 = _mm256_set1_ps(0.0f);

            for (int k = 0; k < min_end0; k++) {
                __m256 neg_vec = _mm256_load_ps(&X[w_neg_row_ind[col_neg0 + k] * rows + i]);
                __m256 pos_vec = _mm256_load_ps(&X[w_pos_row_ind[col_pos0 + k] * rows + i]);              
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos_vec, neg_vec));
            }
            for (int k = col_pos0 + min_end0; k < col_pos1; k++) {
                __m256 pos_vec = _mm256_load_ps(&X[w_pos_row_ind[k] * rows + i]);
                res00 = _mm256_add_ps(res00, pos_vec);
            }
            for (int k = col_neg0 + min_end0; k < col_neg1; k++) {
                __m256 neg_vec = _mm256_load_ps(&X[w_neg_row_ind[k] * rows + i]);
                res00 = _mm256_sub_ps(res00, neg_vec);
            }

            for (int k = 0; k < min_end1; k++) {
                __m256 neg_vec = _mm256_load_ps(&X[w_neg_row_ind[col_neg1 + k] * rows + i]);
                __m256 pos_vec = _mm256_load_ps(&X[w_pos_row_ind[col_pos1 + k] * rows + i]);
                res01 = _mm256_add_ps(res01, _mm256_sub_ps(pos_vec, neg_vec));
            }
            for (int k = col_pos1 + min_end1; k < col_pos2; k++) {
                __m256 pos_vec = _mm256_load_ps(&X[w_pos_row_ind[k] * rows + i]);
                res01 = _mm256_add_ps(res01, pos_vec);
            }
            for (int k = col_neg1 + min_end1; k < col_neg2; k++) {
                __m256 neg_vec = _mm256_load_ps(&X[w_neg_row_ind[k] * rows + i]);
                res01 = _mm256_sub_ps(res01, neg_vec);
            }

            for (int k = 0; k < min_end2; k++) {
                __m256 neg_vec = _mm256_load_ps(&X[w_neg_row_ind[col_neg2 + k] * rows + i]);
                __m256 pos_vec = _mm256_load_ps(&X[w_pos_row_ind[col_pos2 + k] * rows + i]);
                res02 = _mm256_add_ps(res02, _mm256_sub_ps(pos_vec, neg_vec));
            }
            for (int k = col_pos2 + min_end2; k < col_pos3; k++) {
                __m256 pos_vec = _mm256_load_ps(&X[w_pos_row_ind[k] * rows + i]);
                res02 = _mm256_add_ps(res02, pos_vec);
            }
            for (int k = col_neg2 + min_end2; k < col_neg3; k++) {
                __m256 neg_vec = _mm256_load_ps(&X[w_neg_row_ind[k] * rows + i]);
                res02 = _mm256_sub_ps(res02, neg_vec);
            }

            for (int k = 0; k < min_end3; k++) {
                __m256 neg_vec = _mm256_load_ps(&X[w_neg_row_ind[col_neg3 + k] * rows + i]);
                __m256 pos_vec = _mm256_load_ps(&X[w_pos_row_ind[col_pos3 + k] * rows + i]);
                res03 = _mm256_add_ps(res03, _mm256_sub_ps(pos_vec, neg_vec));
            }
            for (int k = col_pos3 + min_end3; k < col_pos4; k++) {
                __m256 pos_vec = _mm256_load_ps(&X[w_pos_row_ind[k] * rows + i]);
                res03 = _mm256_add_ps(res03, pos_vec);
            }
            for (int k = col_neg3 + min_end3; k < col_neg4; k++) {
                __m256 neg_vec = _mm256_load_ps(&X[w_neg_row_ind[k] * rows + i]);
                res03 = _mm256_sub_ps(res03, neg_vec);
            }

            _mm256_store_ps(&result[(j + 0) * rows + i], res00);
            _mm256_store_ps(&result[(j + 1) * rows + i], res01);
            _mm256_store_ps(&result[(j + 2) * rows + i], res02);
            _mm256_store_ps(&result[(j + 3) * rows + i], res03);
        }
    }
}


void GEMM_CPU_FP32_colMajor_TCSC_Naive_oneFor_8x4_AVX2_OpenMP(float* X, int16_t* w_neg_row_ind, int32_t* w_neg_col_ptr, int16_t* w_pos_row_ind, int32_t* w_pos_col_ptr, float* result, int rows, int columns, int inners) {
    #pragma omp parallel for
    for (int j = 0; j < columns; j += 4) {
        //Pointer to where a column starts and ends
        int col_neg0 = w_neg_col_ptr[j];
        int col_neg1 = w_neg_col_ptr[j + 1];
        int col_neg2 = w_neg_col_ptr[j + 2];
        int col_neg3 = w_neg_col_ptr[j + 3];
        int col_neg4 = w_neg_col_ptr[j + 4];

        int col_pos0 = w_pos_col_ptr[j];
        int col_pos1 = w_pos_col_ptr[j + 1];
        int col_pos2 = w_pos_col_ptr[j + 2];
        int col_pos3 = w_pos_col_ptr[j + 3];
        int col_pos4 = w_pos_col_ptr[j + 4];

        int min_end0 = std::min(col_pos1 - col_pos0, col_neg1 - col_neg0);
        int min_end1 = std::min(col_pos2 - col_pos1, col_neg2 - col_neg1);
        int min_end2 = std::min(col_pos3 - col_pos2, col_neg3 - col_neg2);
        int min_end3 = std::min(col_pos4 - col_pos3, col_neg4 - col_neg3);

        for (int i = 0; i < rows; i += 8) {
            __m256 res00 = _mm256_set1_ps(0.0f);
            __m256 res01 = _mm256_set1_ps(0.0f);
            __m256 res02 = _mm256_set1_ps(0.0f);
            __m256 res03 = _mm256_set1_ps(0.0f);

            for (int k = 0; k < min_end0; k++) {
                __m256 neg_vec = _mm256_load_ps(&X[w_neg_row_ind[col_neg0 + k] * rows + i]);
                __m256 pos_vec = _mm256_load_ps(&X[w_pos_row_ind[col_pos0 + k] * rows + i]);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos_vec, neg_vec));
            }
            for (int k = col_pos0 + min_end0; k < col_pos1; k++) {
                __m256 pos_vec = _mm256_load_ps(&X[w_pos_row_ind[k] * rows + i]);
                res00 = _mm256_add_ps(res00, pos_vec);
            }
            for (int k = col_neg0 + min_end0; k < col_neg1; k++) {
                __m256 neg_vec = _mm256_load_ps(&X[w_neg_row_ind[k] * rows + i]);
                res00 = _mm256_sub_ps(res00, neg_vec);
            }

            for (int k = 0; k < min_end1; k++) {
                __m256 neg_vec = _mm256_load_ps(&X[w_neg_row_ind[col_neg1 + k] * rows + i]);
                __m256 pos_vec = _mm256_load_ps(&X[w_pos_row_ind[col_pos1 + k] * rows + i]);
                res01 = _mm256_add_ps(res01, _mm256_sub_ps(pos_vec, neg_vec));
            }
            for (int k = col_pos1 + min_end1; k < col_pos2; k++) {
                __m256 pos_vec = _mm256_load_ps(&X[w_pos_row_ind[k] * rows + i]);
                res01 = _mm256_add_ps(res01, pos_vec);
            }
            for (int k = col_neg1 + min_end1; k < col_neg2; k++) {
                __m256 neg_vec = _mm256_load_ps(&X[w_neg_row_ind[k] * rows + i]);
                res01 = _mm256_sub_ps(res01, neg_vec);
            }

            for (int k = 0; k < min_end2; k++) {
                __m256 neg_vec = _mm256_load_ps(&X[w_neg_row_ind[col_neg2 + k] * rows + i]);
                __m256 pos_vec = _mm256_load_ps(&X[w_pos_row_ind[col_pos2 + k] * rows + i]);
                res02 = _mm256_add_ps(res02, _mm256_sub_ps(pos_vec, neg_vec));
            }
            for (int k = col_pos2 + min_end2; k < col_pos3; k++) {
                __m256 pos_vec = _mm256_load_ps(&X[w_pos_row_ind[k] * rows + i]);
                res02 = _mm256_add_ps(res02, pos_vec);
            }
            for (int k = col_neg2 + min_end2; k < col_neg3; k++) {
                __m256 neg_vec = _mm256_load_ps(&X[w_neg_row_ind[k] * rows + i]);
                res02 = _mm256_sub_ps(res02, neg_vec);
            }

            for (int k = 0; k < min_end3; k++) {
                __m256 neg_vec = _mm256_load_ps(&X[w_neg_row_ind[col_neg3 + k] * rows + i]);
                __m256 pos_vec = _mm256_load_ps(&X[w_pos_row_ind[col_pos3 + k] * rows + i]);
                res03 = _mm256_add_ps(res03, _mm256_sub_ps(pos_vec, neg_vec));
            }
            for (int k = col_pos3 + min_end3; k < col_pos4; k++) {
                __m256 pos_vec = _mm256_load_ps(&X[w_pos_row_ind[k] * rows + i]);
                res03 = _mm256_add_ps(res03, pos_vec);
            }
            for (int k = col_neg3 + min_end3; k < col_neg4; k++) {
                __m256 neg_vec = _mm256_load_ps(&X[w_neg_row_ind[k] * rows + i]);
                res03 = _mm256_sub_ps(res03, neg_vec);
            }

            _mm256_store_ps(&result[(j + 0) * rows + i], res00);
            _mm256_store_ps(&result[(j + 1) * rows + i], res01);
            _mm256_store_ps(&result[(j + 2) * rows + i], res02);
            _mm256_store_ps(&result[(j + 3) * rows + i], res03);
        }
    }
}


void GEMM_CPU_FP32_colMajor_TCSC_Merged_8x1_AVX2_OpenMP(const float* X, const int32_t* metadata, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL; j ++) {
        //Pointer to where a column starts and ends
        /*
        int align_start0 = metadata[j * 4 + 0];
        int align_end0  = metadata[j * 4 + 1];
        int remain_end0 = metadata[j * 4 + 2];
        int remain_val0 = metadata[j * 4 + 3];
        */
        const int32_t* k0 = &metadata[j * 4 + 0];
        for (int i = 0; i < M_ROW; i += 8) {
            __m256 res0 = _mm256_set1_ps(0.0f);

            for (int k = k0[0]; k < k0[1]; k += 2) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                __m256 neg_vec = _mm256_load_ps(&X[row_index[k + 1] * M_ROW + i]);
                res0 = _mm256_add_ps(res0, _mm256_sub_ps(pos_vec, neg_vec));
            }
            __m256 remain = _mm256_set1_ps(0.0f);
            for (int k = k0[1]; k < k0[2]; k++) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                remain = _mm256_add_ps(remain, pos_vec);
            }
            __m256 remain_val = _mm256_set1_ps(k0[3]);
            res0 = _mm256_add_ps(res0, _mm256_mul_ps(remain_val, remain));
            _mm256_store_ps(&result[(j + 0) * M_ROW + i], res0);
        }
    }
}


void GEMM_CPU_FP32_colMajor_TCSC_Merged_16x1_AVX2_OpenMP(const float* X, const int32_t* metadata, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL; j++) {
        //Pointer to where a column starts and ends
        /*
        int align_start0 = metadata[j * 4 + 0];
        int align_end0  = metadata[j * 4 + 1];
        int remain_end0 = metadata[j * 4 + 2];
        int remain_val0 = metadata[j * 4 + 3];
        */
        const int32_t* k0 = &metadata[j * 4 + 0];
        for (int i = 0; i < M_ROW; i += 16) {
            __m256 res0 = _mm256_set1_ps(0.0f);
            __m256 res1 = _mm256_set1_ps(0.0f);

            for (int k = k0[0]; k < k0[1]; k += 2) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                __m256 neg_vec = _mm256_load_ps(&X[row_index[k + 1] * M_ROW + i]);
                __m256 pos_vec1 = _mm256_load_ps(&X[row_index[k] * M_ROW + i + 8]);
                __m256 neg_vec1 = _mm256_load_ps(&X[row_index[k + 1] * M_ROW + i + 8]);
                res0 = _mm256_add_ps(res0, _mm256_sub_ps(pos_vec, neg_vec));
                res1 = _mm256_add_ps(res1, _mm256_sub_ps(pos_vec1, neg_vec1));
            }
            __m256 remain = _mm256_set1_ps(0.0f);
            __m256 remain1 = _mm256_set1_ps(0.0f);
            for (int k = k0[1]; k < k0[2]; k++) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                __m256 pos_vec1 = _mm256_load_ps(&X[row_index[k] * M_ROW + i + 8]);
                remain = _mm256_add_ps(remain, pos_vec);
                remain1 = _mm256_add_ps(remain1, pos_vec1);
            }
            __m256 remain_val = _mm256_set1_ps(k0[3]);
            res0 = _mm256_add_ps(res0, _mm256_mul_ps(remain_val, remain));
            res1 = _mm256_add_ps(res1, _mm256_mul_ps(remain_val, remain1));
            _mm256_store_ps(&result[(j + 0) * M_ROW + i], res0);
            _mm256_store_ps(&result[(j + 0) * M_ROW + i + 8], res1);
        }
    }
}


void GEMM_CPU_FP32_colMajor_TCSC_Merged_16x1_AVX2_if_OpenMP(const float* X, const int32_t* metadata, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL; j++) {
        //Pointer to where a column starts and ends
        /*
        int align_start0 = metadata[j * 4 + 0];
        int align_end0  = metadata[j * 4 + 1];
        int remain_end0 = metadata[j * 4 + 2];
        int remain_val0 = metadata[j * 4 + 3];
        */
        const int32_t* k0 = &metadata[j * 4 + 0];
        for (int i = 0; i < M_ROW; i += 16) {
            __m256 res0 = _mm256_set1_ps(0.0f);
            __m256 res1 = _mm256_set1_ps(0.0f);

            for (int k = k0[0]; k < k0[1]; k += 2) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                __m256 neg_vec = _mm256_load_ps(&X[row_index[k + 1] * M_ROW + i]);
                __m256 pos_vec1 = _mm256_load_ps(&X[row_index[k] * M_ROW + i + 8]);
                __m256 neg_vec1 = _mm256_load_ps(&X[row_index[k + 1] * M_ROW + i + 8]);
                res0 = _mm256_add_ps(res0, _mm256_sub_ps(pos_vec, neg_vec));
                res1 = _mm256_add_ps(res1, _mm256_sub_ps(pos_vec1, neg_vec1));
            }
            __m256 remain = _mm256_set1_ps(0.0f);
            __m256 remain1 = _mm256_set1_ps(0.0f);
            for (int k = k0[1]; k < k0[2]; k++) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                __m256 pos_vec1 = _mm256_load_ps(&X[row_index[k] * M_ROW + i + 8]);
                remain = _mm256_add_ps(remain, pos_vec);
                remain1 = _mm256_add_ps(remain1, pos_vec1);
            }
            if (k0[3] > 0) {
                res0 = _mm256_add_ps(res0, remain);
                res1 = _mm256_add_ps(res1, remain1);
            }
            else {
                res0 = _mm256_sub_ps(res0, remain);
                res1 = _mm256_sub_ps(res1, remain1);
            }
            _mm256_store_ps(&result[(j + 0) * M_ROW + i], res0);
            _mm256_store_ps(&result[(j + 0) * M_ROW + i + 8], res1);
        }
    }
}


void GEMM_CPU_FP32_colMajor_TCSC_Merged_8x4_AVX2_OpenMP(const float* X, const int32_t* metadata, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL; j += 4) {
        //Pointer to where a column starts and ends
        /*
        int align_start0 = metadata[j * 4 + 0];
        int align_end0  = metadata[j * 4 + 1];
        int remain_end0 = metadata[j * 4 + 2];
        int remain_val0 = metadata[j * 4 + 3];
        */
        const int32_t* k0 = &metadata[j * 4 + 0];
        const int32_t* k1 = &metadata[j * 4 + 4];
        const int32_t* k2 = &metadata[j * 4 + 8];
        const int32_t* k3 = &metadata[j * 4 +12];

        for (int i = 0; i < M_ROW; i += 8) {
            __m256 res0 = _mm256_set1_ps(0.0f);
            __m256 res1 = _mm256_set1_ps(0.0f);
            __m256 res2 = _mm256_set1_ps(0.0f);
            __m256 res3 = _mm256_set1_ps(0.0f);

            for (int k = k0[0]; k < k0[1]; k+=2) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k  ] * M_ROW + i]);
                __m256 neg_vec = _mm256_load_ps(&X[row_index[k+1] * M_ROW + i]);
                res0 = _mm256_add_ps(res0, _mm256_sub_ps(pos_vec, neg_vec));
            }
            __m256 remain = _mm256_set1_ps(0.0f);
            for (int k = k0[1]; k < k0[2]; k++) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                remain = _mm256_add_ps(remain, pos_vec);
            }
            __m256 remain_val = _mm256_set1_ps(k0[3]);
            res0 = _mm256_add_ps(res0, _mm256_mul_ps(remain_val, remain));

            for (int k = k1[0]; k < k1[1]; k += 2) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                __m256 neg_vec = _mm256_load_ps(&X[row_index[k + 1] * M_ROW + i]);
                res1 = _mm256_add_ps(res1, _mm256_sub_ps(pos_vec, neg_vec));
            }
            remain = _mm256_set1_ps(0.0f);
            for (int k = k1[1]; k < k1[2]; k++) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                remain = _mm256_add_ps(remain, pos_vec);
            }
            remain_val = _mm256_set1_ps(k1[3]);
            res1 = _mm256_add_ps(res1, _mm256_mul_ps(remain_val, remain));

            for (int k = k2[0]; k < k2[1]; k += 2) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                __m256 neg_vec = _mm256_load_ps(&X[row_index[k + 1] * M_ROW + i]);
                res2 = _mm256_add_ps(res2, _mm256_sub_ps(pos_vec, neg_vec));
            }
            remain = _mm256_set1_ps(0.0f);
            for (int k = k2[1]; k < k2[2]; k++) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                remain = _mm256_add_ps(remain, pos_vec);
            }
            remain_val = _mm256_set1_ps(k2[3]);
            res2 = _mm256_add_ps(res2, _mm256_mul_ps(remain_val, remain));

            for (int k = k3[0]; k < k3[1]; k += 2) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                __m256 neg_vec = _mm256_load_ps(&X[row_index[k + 1] * M_ROW + i]);
                res3 = _mm256_add_ps(res3, _mm256_sub_ps(pos_vec, neg_vec));
            }
            remain = _mm256_set1_ps(0.0f);
            for (int k = k3[1]; k < k3[2]; k++) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                remain = _mm256_add_ps(remain, pos_vec);
            }
            remain_val = _mm256_set1_ps(k3[3]);
            res3 = _mm256_add_ps(res3, _mm256_mul_ps(remain_val, remain));

            _mm256_store_ps(&result[(j + 0) * M_ROW + i], res0);
            _mm256_store_ps(&result[(j + 1) * M_ROW + i], res1);
            _mm256_store_ps(&result[(j + 2) * M_ROW + i], res2);
            _mm256_store_ps(&result[(j + 3) * M_ROW + i], res3);
        }
    }
}


void GEMM_CPU_FP32_colMajor_TCSC_Merged_8x4_AVX2_OpenMPi(const float* X, const int32_t* metadata, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
    for (int j = 0; j < N_COL; j += 4) {
        //Pointer to where a column starts and ends
        /*
        int align_start0 = metadata[j * 4 + 0];
        int align_end0  = metadata[j * 4 + 1];
        int remain_end0 = metadata[j * 4 + 2];
        int remain_val0 = metadata[j * 4 + 3];
        */
        const int32_t* k0 = &metadata[j * 4 + 0];
        const int32_t* k1 = &metadata[j * 4 + 4];
        const int32_t* k2 = &metadata[j * 4 + 8];
        const int32_t* k3 = &metadata[j * 4 + 12];
#pragma omp parallel for
        for (int i = 0; i < M_ROW; i += 8) {
            __m256 res0 = _mm256_set1_ps(0.0f);
            __m256 res1 = _mm256_set1_ps(0.0f);
            __m256 res2 = _mm256_set1_ps(0.0f);
            __m256 res3 = _mm256_set1_ps(0.0f);

            for (int k = k0[0]; k < k0[1]; k += 2) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                __m256 neg_vec = _mm256_load_ps(&X[row_index[k + 1] * M_ROW + i]);
                res0 = _mm256_add_ps(res0, _mm256_sub_ps(pos_vec, neg_vec));
            }
            __m256 remain = _mm256_set1_ps(0.0f);
            for (int k = k0[1]; k < k0[2]; k++) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                remain = _mm256_add_ps(remain, pos_vec);
            }
            __m256 remain_val = _mm256_set1_ps(k0[3]);
            res0 = _mm256_add_ps(res0, _mm256_mul_ps(remain_val, remain));

            for (int k = k1[0]; k < k1[1]; k += 2) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                __m256 neg_vec = _mm256_load_ps(&X[row_index[k + 1] * M_ROW + i]);
                res1 = _mm256_add_ps(res1, _mm256_sub_ps(pos_vec, neg_vec));
            }
            remain = _mm256_set1_ps(0.0f);
            for (int k = k1[1]; k < k1[2]; k++) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                remain = _mm256_add_ps(remain, pos_vec);
            }
            remain_val = _mm256_set1_ps(k1[3]);
            res1 = _mm256_add_ps(res1, _mm256_mul_ps(remain_val, remain));

            for (int k = k2[0]; k < k2[1]; k += 2) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                __m256 neg_vec = _mm256_load_ps(&X[row_index[k + 1] * M_ROW + i]);
                res2 = _mm256_add_ps(res2, _mm256_sub_ps(pos_vec, neg_vec));
            }
            remain = _mm256_set1_ps(0.0f);
            for (int k = k2[1]; k < k2[2]; k++) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                remain = _mm256_add_ps(remain, pos_vec);
            }
            remain_val = _mm256_set1_ps(k2[3]);
            res2 = _mm256_add_ps(res2, _mm256_mul_ps(remain_val, remain));

            for (int k = k3[0]; k < k3[1]; k += 2) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                __m256 neg_vec = _mm256_load_ps(&X[row_index[k + 1] * M_ROW + i]);
                res3 = _mm256_add_ps(res3, _mm256_sub_ps(pos_vec, neg_vec));
            }
            remain = _mm256_set1_ps(0.0f);
            for (int k = k3[1]; k < k3[2]; k++) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                remain = _mm256_add_ps(remain, pos_vec);
            }
            remain_val = _mm256_set1_ps(k3[3]);
            res3 = _mm256_add_ps(res3, _mm256_mul_ps(remain_val, remain));

            _mm256_store_ps(&result[(j + 0) * M_ROW + i], res0);
            _mm256_store_ps(&result[(j + 1) * M_ROW + i], res1);
            _mm256_store_ps(&result[(j + 2) * M_ROW + i], res2);
            _mm256_store_ps(&result[(j + 3) * M_ROW + i], res3);
        }
    }
}


void GEMM_CPU_FP32_colMajor_TCSC_Merged_8x4_AVX2_ij_OpenMP(const float* X, const int32_t* metadata, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int i = 0; i < M_ROW; i += 8) {
        for (int j = 0; j < N_COL; j += 4) {
            //Pointer to where a column starts and ends
            /*
            int align_start0 = metadata[j * 4 + 0];
            int align_end0  = metadata[j * 4 + 1];
            int remain_end0 = metadata[j * 4 + 2];
            int remain_val0 = metadata[j * 4 + 3];
            */
            const int32_t* k0 = &metadata[j * 4 + 0];
            const int32_t* k1 = &metadata[j * 4 + 4];
            const int32_t* k2 = &metadata[j * 4 + 8];
            const int32_t* k3 = &metadata[j * 4 + 12];

            __m256 res0 = _mm256_set1_ps(0.0f);
            __m256 res1 = _mm256_set1_ps(0.0f);
            __m256 res2 = _mm256_set1_ps(0.0f);
            __m256 res3 = _mm256_set1_ps(0.0f);

            for (int k = k0[0]; k < k0[1]; k += 2) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                __m256 neg_vec = _mm256_load_ps(&X[row_index[k + 1] * M_ROW + i]);
                res0 = _mm256_add_ps(res0, _mm256_sub_ps(pos_vec, neg_vec));
            }
            __m256 remain = _mm256_set1_ps(0.0f);
            for (int k = k0[1]; k < k0[2]; k++) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                remain = _mm256_add_ps(remain, pos_vec);
            }
            __m256 remain_val = _mm256_set1_ps(k0[3]);
            res0 = _mm256_add_ps(res0, _mm256_mul_ps(remain_val, remain));

            for (int k = k1[0]; k < k1[1]; k += 2) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                __m256 neg_vec = _mm256_load_ps(&X[row_index[k + 1] * M_ROW + i]);
                res1 = _mm256_add_ps(res1, _mm256_sub_ps(pos_vec, neg_vec));
            }
            remain = _mm256_set1_ps(0.0f);
            for (int k = k1[1]; k < k1[2]; k++) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                remain = _mm256_add_ps(remain, pos_vec);
            }
            remain_val = _mm256_set1_ps(k1[3]);
            res1 = _mm256_add_ps(res1, _mm256_mul_ps(remain_val, remain));

            for (int k = k2[0]; k < k2[1]; k += 2) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                __m256 neg_vec = _mm256_load_ps(&X[row_index[k + 1] * M_ROW + i]);
                res2 = _mm256_add_ps(res2, _mm256_sub_ps(pos_vec, neg_vec));
            }
            remain = _mm256_set1_ps(0.0f);
            for (int k = k2[1]; k < k2[2]; k++) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                remain = _mm256_add_ps(remain, pos_vec);
            }
            remain_val = _mm256_set1_ps(k2[3]);
            res2 = _mm256_add_ps(res2, _mm256_mul_ps(remain_val, remain));

            for (int k = k3[0]; k < k3[1]; k += 2) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                __m256 neg_vec = _mm256_load_ps(&X[row_index[k + 1] * M_ROW + i]);
                res3 = _mm256_add_ps(res3, _mm256_sub_ps(pos_vec, neg_vec));
            }
            remain = _mm256_set1_ps(0.0f);
            for (int k = k3[1]; k < k3[2]; k++) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                remain = _mm256_add_ps(remain, pos_vec);
            }
            remain_val = _mm256_set1_ps(k3[3]);
            res3 = _mm256_add_ps(res3, _mm256_mul_ps(remain_val, remain));

            _mm256_store_ps(&result[(j + 0) * M_ROW + i], res0);
            _mm256_store_ps(&result[(j + 1) * M_ROW + i], res1);
            _mm256_store_ps(&result[(j + 2) * M_ROW + i], res2);
            _mm256_store_ps(&result[(j + 3) * M_ROW + i], res3);
        }
    }
}


void GEMM_CPU_FP32_colMajor_TCSC_Merged_8x4_AVX2_ij_OpenMPj(const float* X, const int32_t* metadata, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
    for (int i = 0; i < M_ROW; i += 8) {

        #pragma omp parallel for
        for (int j = 0; j < N_COL; j += 4) {
            //Pointer to where a column starts and ends
            /*
            int align_start0 = metadata[j * 4 + 0];
            int align_end0  = metadata[j * 4 + 1];
            int remain_end0 = metadata[j * 4 + 2];
            int remain_val0 = metadata[j * 4 + 3];
            */
            const int32_t* k0 = &metadata[j * 4 + 0];
            const int32_t* k1 = &metadata[j * 4 + 4];
            const int32_t* k2 = &metadata[j * 4 + 8];
            const int32_t* k3 = &metadata[j * 4 + 12];

            __m256 res0 = _mm256_set1_ps(0.0f);
            __m256 res1 = _mm256_set1_ps(0.0f);
            __m256 res2 = _mm256_set1_ps(0.0f);
            __m256 res3 = _mm256_set1_ps(0.0f);

            for (int k = k0[0]; k < k0[1]; k += 2) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                __m256 neg_vec = _mm256_load_ps(&X[row_index[k + 1] * M_ROW + i]);
                res0 = _mm256_add_ps(res0, _mm256_sub_ps(pos_vec, neg_vec));
            }
            __m256 remain = _mm256_set1_ps(0.0f);
            for (int k = k0[1]; k < k0[2]; k++) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                remain = _mm256_add_ps(remain, pos_vec);
            }
            __m256 remain_val = _mm256_set1_ps(k0[3]);
            res0 = _mm256_add_ps(res0, _mm256_mul_ps(remain_val, remain));

            for (int k = k1[0]; k < k1[1]; k += 2) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                __m256 neg_vec = _mm256_load_ps(&X[row_index[k + 1] * M_ROW + i]);
                res1 = _mm256_add_ps(res1, _mm256_sub_ps(pos_vec, neg_vec));
            }
            remain = _mm256_set1_ps(0.0f);
            for (int k = k1[1]; k < k1[2]; k++) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                remain = _mm256_add_ps(remain, pos_vec);
            }
            remain_val = _mm256_set1_ps(k1[3]);
            res1 = _mm256_add_ps(res1, _mm256_mul_ps(remain_val, remain));

            for (int k = k2[0]; k < k2[1]; k += 2) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                __m256 neg_vec = _mm256_load_ps(&X[row_index[k + 1] * M_ROW + i]);
                res2 = _mm256_add_ps(res2, _mm256_sub_ps(pos_vec, neg_vec));
            }
            remain = _mm256_set1_ps(0.0f);
            for (int k = k2[1]; k < k2[2]; k++) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                remain = _mm256_add_ps(remain, pos_vec);
            }
            remain_val = _mm256_set1_ps(k2[3]);
            res2 = _mm256_add_ps(res2, _mm256_mul_ps(remain_val, remain));

            for (int k = k3[0]; k < k3[1]; k += 2) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                __m256 neg_vec = _mm256_load_ps(&X[row_index[k + 1] * M_ROW + i]);
                res3 = _mm256_add_ps(res3, _mm256_sub_ps(pos_vec, neg_vec));
            }
            remain = _mm256_set1_ps(0.0f);
            for (int k = k3[1]; k < k3[2]; k++) {
                __m256 pos_vec = _mm256_load_ps(&X[row_index[k] * M_ROW + i]);
                remain = _mm256_add_ps(remain, pos_vec);
            }
            remain_val = _mm256_set1_ps(k3[3]);
            res3 = _mm256_add_ps(res3, _mm256_mul_ps(remain_val, remain));

            _mm256_store_ps(&result[(j + 0) * M_ROW + i], res0);
            _mm256_store_ps(&result[(j + 1) * M_ROW + i], res1);
            _mm256_store_ps(&result[(j + 2) * M_ROW + i], res2);
            _mm256_store_ps(&result[(j + 3) * M_ROW + i], res3);
        }
    }
}


void GEMM_CPU_FP32_colMajor_TCSC_Merged_GroupMin_8xG4_AVX2_OpenMP(const float* X, const int32_t* metadata, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL/4; j ++) {
        /* Pointer to where a column starts and ends
        int align_start  = metadata[j * 10 + 0];
        int align_end    = metadata[j * 10 + 1];
        int +remain_end0 = metadata[j * 10 + 2];
        int -remain_end0 = metadata[j * 10 + 3];
        int +remain_end1 = metadata[j * 10 + 4];
        int -remain_end1 = metadata[j * 10 + 5];
        ...
        */
        // Group # = j, metadata per group = 10
        const int * groupData = &metadata[j * 10];

        for (int i = 0; i < M_ROW; i += 8) {
            __m256 res0 = _mm256_set1_ps(0.0f);
            __m256 res1 = _mm256_set1_ps(0.0f);
            __m256 res2 = _mm256_set1_ps(0.0f);
            __m256 res3 = _mm256_set1_ps(0.0f);

            for (int k = groupData[0]; k < groupData[1]; k += 8) {
                __m256 pos0 = _mm256_load_ps(&X[row_index[k + 0] * M_ROW + i]);
                __m256 neg0 = _mm256_load_ps(&X[row_index[k + 1] * M_ROW + i]);
                __m256 pos1 = _mm256_load_ps(&X[row_index[k + 2] * M_ROW + i]);
                __m256 neg1 = _mm256_load_ps(&X[row_index[k + 3] * M_ROW + i]);
                __m256 pos2 = _mm256_load_ps(&X[row_index[k + 4] * M_ROW + i]);
                __m256 neg2 = _mm256_load_ps(&X[row_index[k + 5] * M_ROW + i]);
                __m256 pos3 = _mm256_load_ps(&X[row_index[k + 6] * M_ROW + i]);
                __m256 neg3 = _mm256_load_ps(&X[row_index[k + 7] * M_ROW + i]);
                res0 = _mm256_add_ps(res0, _mm256_sub_ps(pos0, neg0));
                res1 = _mm256_add_ps(res1, _mm256_sub_ps(pos1, neg1));
                res2 = _mm256_add_ps(res2, _mm256_sub_ps(pos2, neg2));
                res3 = _mm256_add_ps(res3, _mm256_sub_ps(pos3, neg3));
            }

            __m256 pos0 = _mm256_set1_ps(0.0f);
            for (int k = groupData[1]; k < groupData[2]; k++) {
                pos0 = _mm256_add_ps(pos0, _mm256_load_ps(&X[row_index[k] * M_ROW + i]));
            }
            res0 = _mm256_add_ps(res0, pos0);
            __m256 neg0 = _mm256_set1_ps(0.0f);
            for (int k = groupData[2]; k < groupData[3]; k++) {
                neg0 = _mm256_add_ps(neg0, _mm256_load_ps(&X[row_index[k] * M_ROW + i]));
            }
            res0 = _mm256_sub_ps(res0, neg0);

            __m256 pos1 = _mm256_set1_ps(0.0f);
            for (int k = groupData[3]; k < groupData[4]; k++) {
                pos1 = _mm256_add_ps(pos1, _mm256_load_ps(&X[row_index[k] * M_ROW + i]));
            }
            res1 = _mm256_add_ps(res1, pos1);
            __m256 neg1 = _mm256_set1_ps(0.0f);
            for (int k = groupData[4]; k < groupData[5]; k++) {
                neg1 = _mm256_add_ps(neg1, _mm256_load_ps(&X[row_index[k] * M_ROW + i]));
            }
            res1 = _mm256_sub_ps(res1, neg1);

            __m256 pos2 = _mm256_set1_ps(0.0f);
            for (int k = groupData[5]; k < groupData[6]; k++) {
                pos2 = _mm256_add_ps(pos2, _mm256_load_ps(&X[row_index[k] * M_ROW + i]));
            }
            res2 = _mm256_add_ps(res2, pos2);
            __m256 neg2 = _mm256_set1_ps(0.0f);
            for (int k = groupData[6]; k < groupData[7]; k++) {
                neg2 = _mm256_add_ps(neg2, _mm256_load_ps(&X[row_index[k] * M_ROW + i]));
            }
            res2 = _mm256_sub_ps(res2, neg2);

            __m256 pos3 = _mm256_set1_ps(0.0f);
            for (int k = groupData[7]; k < groupData[8]; k++) {
                pos3 = _mm256_add_ps(pos3, _mm256_load_ps(&X[row_index[k] * M_ROW + i]));
            }
            res3 = _mm256_add_ps(res3, pos3);
            __m256 neg3 = _mm256_set1_ps(0.0f);
            for (int k = groupData[8]; k < groupData[9]; k++) {
                neg3 = _mm256_add_ps(neg3, _mm256_load_ps(&X[row_index[k] * M_ROW + i]));
            }
            res3 = _mm256_sub_ps(res3, neg3);

            _mm256_store_ps(&result[(j * 4 + 0) * M_ROW + i], res0);
            _mm256_store_ps(&result[(j * 4 + 1) * M_ROW + i], res1);
            _mm256_store_ps(&result[(j * 4 + 2) * M_ROW + i], res2);
            _mm256_store_ps(&result[(j * 4 + 3) * M_ROW + i], res3);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Merged_GroupMin_8xG8_AVX2_OpenMP(const float* X, const int32_t* metadata, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 8; j++) {
        const int* groupData  = &metadata[j * 18];
        for (int i = 0; i < M_ROW; i += 8) {
            __m256 res00 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res20 = _mm256_setzero_ps();
            __m256 res30 = _mm256_setzero_ps();
            __m256 res40 = _mm256_setzero_ps();
            __m256 res50 = _mm256_setzero_ps();
            __m256 res60 = _mm256_setzero_ps();
            __m256 res70 = _mm256_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 16) {
                __m256 pos00 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m256 neg00 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m256 pos10 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m256 neg10 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m256 pos20 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m256 neg20 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m256 pos30 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m256 neg30 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m256 pos40 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 0);
                __m256 neg40 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 0);
                __m256 pos50 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 0);
                __m256 neg50 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 0);
                __m256 pos60 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 0);
                __m256 neg60 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 0);
                __m256 pos70 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 0);
                __m256 neg70 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 0);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res20 = _mm256_add_ps(res20, _mm256_sub_ps(pos20, neg20));
                res30 = _mm256_add_ps(res30, _mm256_sub_ps(pos30, neg30));
                res40 = _mm256_add_ps(res40, _mm256_sub_ps(pos40, neg40));
                res50 = _mm256_add_ps(res50, _mm256_sub_ps(pos50, neg50));
                res60 = _mm256_add_ps(res60, _mm256_sub_ps(pos60, neg60));
                res70 = _mm256_add_ps(res70, _mm256_sub_ps(pos70, neg70));
            }
            for (int k = groupData[1]; k < groupData[2]; k++) {
                res00 = _mm256_add_ps(res00, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[2]; k < groupData[3]; k++) {
                res00 = _mm256_sub_ps(res00, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[3]; k < groupData[4]; k++) {
                res10 = _mm256_add_ps(res10, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[4]; k < groupData[5]; k++) {
                res10 = _mm256_sub_ps(res10, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[5]; k < groupData[6]; k++) {
                res20 = _mm256_add_ps(res20, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[6]; k < groupData[7]; k++) {
                res20 = _mm256_sub_ps(res20, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[7]; k < groupData[8]; k++) {
                res30 = _mm256_add_ps(res30, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[8]; k < groupData[9]; k++) {
                res30 = _mm256_sub_ps(res30, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[9]; k < groupData[10]; k++) {
                res40 = _mm256_add_ps(res40, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[10]; k < groupData[11]; k++) {
                res40 = _mm256_sub_ps(res40, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[11]; k < groupData[12]; k++) {
                res50 = _mm256_add_ps(res50, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[12]; k < groupData[13]; k++) {
                res50 = _mm256_sub_ps(res50, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[13]; k < groupData[14]; k++) {
                res60 = _mm256_add_ps(res60, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[14]; k < groupData[15]; k++) {
                res60 = _mm256_sub_ps(res60, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[15]; k < groupData[16]; k++) {
                res70 = _mm256_add_ps(res70, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[16]; k < groupData[17]; k++) {
                res70 = _mm256_sub_ps(res70, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            _mm256_store_ps(result + (j * 8 + 0) * M_ROW + i + 0, res00);
            _mm256_store_ps(result + (j * 8 + 1) * M_ROW + i + 0, res10);
            _mm256_store_ps(result + (j * 8 + 2) * M_ROW + i + 0, res20);
            _mm256_store_ps(result + (j * 8 + 3) * M_ROW + i + 0, res30);
            _mm256_store_ps(result + (j * 8 + 4) * M_ROW + i + 0, res40);
            _mm256_store_ps(result + (j * 8 + 5) * M_ROW + i + 0, res50);
            _mm256_store_ps(result + (j * 8 + 6) * M_ROW + i + 0, res60);
            _mm256_store_ps(result + (j * 8 + 7) * M_ROW + i + 0, res70);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Merged_GroupMin_8xG16_AVX2_OpenMP(const float* X, const int32_t* metadata, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 16; j++) {
        const int* groupData  = &metadata[j * 34];
        for (int i = 0; i < M_ROW; i += 8) {
            __m256 res00 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res20 = _mm256_setzero_ps();
            __m256 res30 = _mm256_setzero_ps();
            __m256 res40 = _mm256_setzero_ps();
            __m256 res50 = _mm256_setzero_ps();
            __m256 res60 = _mm256_setzero_ps();
            __m256 res70 = _mm256_setzero_ps();
            __m256 res80 = _mm256_setzero_ps();
            __m256 res90 = _mm256_setzero_ps();
            __m256 res100 = _mm256_setzero_ps();
            __m256 res110 = _mm256_setzero_ps();
            __m256 res120 = _mm256_setzero_ps();
            __m256 res130 = _mm256_setzero_ps();
            __m256 res140 = _mm256_setzero_ps();
            __m256 res150 = _mm256_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 32) {
                __m256 pos00 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m256 neg00 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m256 pos10 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m256 neg10 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m256 pos20 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m256 neg20 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m256 pos30 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m256 neg30 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m256 pos40 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 0);
                __m256 neg40 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 0);
                __m256 pos50 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 0);
                __m256 neg50 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 0);
                __m256 pos60 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 0);
                __m256 neg60 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 0);
                __m256 pos70 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 0);
                __m256 neg70 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 0);
                __m256 pos80 = _mm256_load_ps(X + row_index[k + 16] * M_ROW + i + 0);
                __m256 neg80 = _mm256_load_ps(X + row_index[k + 17] * M_ROW + i + 0);
                __m256 pos90 = _mm256_load_ps(X + row_index[k + 18] * M_ROW + i + 0);
                __m256 neg90 = _mm256_load_ps(X + row_index[k + 19] * M_ROW + i + 0);
                __m256 pos100 = _mm256_load_ps(X + row_index[k + 20] * M_ROW + i + 0);
                __m256 neg100 = _mm256_load_ps(X + row_index[k + 21] * M_ROW + i + 0);
                __m256 pos110 = _mm256_load_ps(X + row_index[k + 22] * M_ROW + i + 0);
                __m256 neg110 = _mm256_load_ps(X + row_index[k + 23] * M_ROW + i + 0);
                __m256 pos120 = _mm256_load_ps(X + row_index[k + 24] * M_ROW + i + 0);
                __m256 neg120 = _mm256_load_ps(X + row_index[k + 25] * M_ROW + i + 0);
                __m256 pos130 = _mm256_load_ps(X + row_index[k + 26] * M_ROW + i + 0);
                __m256 neg130 = _mm256_load_ps(X + row_index[k + 27] * M_ROW + i + 0);
                __m256 pos140 = _mm256_load_ps(X + row_index[k + 28] * M_ROW + i + 0);
                __m256 neg140 = _mm256_load_ps(X + row_index[k + 29] * M_ROW + i + 0);
                __m256 pos150 = _mm256_load_ps(X + row_index[k + 30] * M_ROW + i + 0);
                __m256 neg150 = _mm256_load_ps(X + row_index[k + 31] * M_ROW + i + 0);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res20 = _mm256_add_ps(res20, _mm256_sub_ps(pos20, neg20));
                res30 = _mm256_add_ps(res30, _mm256_sub_ps(pos30, neg30));
                res40 = _mm256_add_ps(res40, _mm256_sub_ps(pos40, neg40));
                res50 = _mm256_add_ps(res50, _mm256_sub_ps(pos50, neg50));
                res60 = _mm256_add_ps(res60, _mm256_sub_ps(pos60, neg60));
                res70 = _mm256_add_ps(res70, _mm256_sub_ps(pos70, neg70));
                res80 = _mm256_add_ps(res80, _mm256_sub_ps(pos80, neg80));
                res90 = _mm256_add_ps(res90, _mm256_sub_ps(pos90, neg90));
                res100 = _mm256_add_ps(res100, _mm256_sub_ps(pos100, neg100));
                res110 = _mm256_add_ps(res110, _mm256_sub_ps(pos110, neg110));
                res120 = _mm256_add_ps(res120, _mm256_sub_ps(pos120, neg120));
                res130 = _mm256_add_ps(res130, _mm256_sub_ps(pos130, neg130));
                res140 = _mm256_add_ps(res140, _mm256_sub_ps(pos140, neg140));
                res150 = _mm256_add_ps(res150, _mm256_sub_ps(pos150, neg150));
            }
            for (int k = groupData[1]; k < groupData[2]; k++) {
                res00 = _mm256_add_ps(res00, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[2]; k < groupData[3]; k++) {
                res00 = _mm256_sub_ps(res00, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[3]; k < groupData[4]; k++) {
                res10 = _mm256_add_ps(res10, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[4]; k < groupData[5]; k++) {
                res10 = _mm256_sub_ps(res10, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[5]; k < groupData[6]; k++) {
                res20 = _mm256_add_ps(res20, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[6]; k < groupData[7]; k++) {
                res20 = _mm256_sub_ps(res20, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[7]; k < groupData[8]; k++) {
                res30 = _mm256_add_ps(res30, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[8]; k < groupData[9]; k++) {
                res30 = _mm256_sub_ps(res30, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[9]; k < groupData[10]; k++) {
                res40 = _mm256_add_ps(res40, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[10]; k < groupData[11]; k++) {
                res40 = _mm256_sub_ps(res40, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[11]; k < groupData[12]; k++) {
                res50 = _mm256_add_ps(res50, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[12]; k < groupData[13]; k++) {
                res50 = _mm256_sub_ps(res50, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[13]; k < groupData[14]; k++) {
                res60 = _mm256_add_ps(res60, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[14]; k < groupData[15]; k++) {
                res60 = _mm256_sub_ps(res60, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[15]; k < groupData[16]; k++) {
                res70 = _mm256_add_ps(res70, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[16]; k < groupData[17]; k++) {
                res70 = _mm256_sub_ps(res70, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[17]; k < groupData[18]; k++) {
                res80 = _mm256_add_ps(res80, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[18]; k < groupData[19]; k++) {
                res80 = _mm256_sub_ps(res80, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[19]; k < groupData[20]; k++) {
                res90 = _mm256_add_ps(res90, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[20]; k < groupData[21]; k++) {
                res90 = _mm256_sub_ps(res90, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[21]; k < groupData[22]; k++) {
                res100 = _mm256_add_ps(res100, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[22]; k < groupData[23]; k++) {
                res100 = _mm256_sub_ps(res100, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[23]; k < groupData[24]; k++) {
                res110 = _mm256_add_ps(res110, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[24]; k < groupData[25]; k++) {
                res110 = _mm256_sub_ps(res110, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[25]; k < groupData[26]; k++) {
                res120 = _mm256_add_ps(res120, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[26]; k < groupData[27]; k++) {
                res120 = _mm256_sub_ps(res120, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[27]; k < groupData[28]; k++) {
                res130 = _mm256_add_ps(res130, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[28]; k < groupData[29]; k++) {
                res130 = _mm256_sub_ps(res130, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[29]; k < groupData[30]; k++) {
                res140 = _mm256_add_ps(res140, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[30]; k < groupData[31]; k++) {
                res140 = _mm256_sub_ps(res140, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[31]; k < groupData[32]; k++) {
                res150 = _mm256_add_ps(res150, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[32]; k < groupData[33]; k++) {
                res150 = _mm256_sub_ps(res150, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            _mm256_store_ps(result + (j * 16 + 0) * M_ROW + i + 0, res00);
            _mm256_store_ps(result + (j * 16 + 1) * M_ROW + i + 0, res10);
            _mm256_store_ps(result + (j * 16 + 2) * M_ROW + i + 0, res20);
            _mm256_store_ps(result + (j * 16 + 3) * M_ROW + i + 0, res30);
            _mm256_store_ps(result + (j * 16 + 4) * M_ROW + i + 0, res40);
            _mm256_store_ps(result + (j * 16 + 5) * M_ROW + i + 0, res50);
            _mm256_store_ps(result + (j * 16 + 6) * M_ROW + i + 0, res60);
            _mm256_store_ps(result + (j * 16 + 7) * M_ROW + i + 0, res70);
            _mm256_store_ps(result + (j * 16 + 8) * M_ROW + i + 0, res80);
            _mm256_store_ps(result + (j * 16 + 9) * M_ROW + i + 0, res90);
            _mm256_store_ps(result + (j * 16 + 10) * M_ROW + i + 0, res100);
            _mm256_store_ps(result + (j * 16 + 11) * M_ROW + i + 0, res110);
            _mm256_store_ps(result + (j * 16 + 12) * M_ROW + i + 0, res120);
            _mm256_store_ps(result + (j * 16 + 13) * M_ROW + i + 0, res130);
            _mm256_store_ps(result + (j * 16 + 14) * M_ROW + i + 0, res140);
            _mm256_store_ps(result + (j * 16 + 15) * M_ROW + i + 0, res150);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Merged_GroupMin_8xG32_AVX2_OpenMP(const float* X, const int32_t* metadata, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 32; j++) {
        const int* groupData  = &metadata[j * 66];
        for (int i = 0; i < M_ROW; i += 8) {
            __m256 res00 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res20 = _mm256_setzero_ps();
            __m256 res30 = _mm256_setzero_ps();
            __m256 res40 = _mm256_setzero_ps();
            __m256 res50 = _mm256_setzero_ps();
            __m256 res60 = _mm256_setzero_ps();
            __m256 res70 = _mm256_setzero_ps();
            __m256 res80 = _mm256_setzero_ps();
            __m256 res90 = _mm256_setzero_ps();
            __m256 res100 = _mm256_setzero_ps();
            __m256 res110 = _mm256_setzero_ps();
            __m256 res120 = _mm256_setzero_ps();
            __m256 res130 = _mm256_setzero_ps();
            __m256 res140 = _mm256_setzero_ps();
            __m256 res150 = _mm256_setzero_ps();
            __m256 res160 = _mm256_setzero_ps();
            __m256 res170 = _mm256_setzero_ps();
            __m256 res180 = _mm256_setzero_ps();
            __m256 res190 = _mm256_setzero_ps();
            __m256 res200 = _mm256_setzero_ps();
            __m256 res210 = _mm256_setzero_ps();
            __m256 res220 = _mm256_setzero_ps();
            __m256 res230 = _mm256_setzero_ps();
            __m256 res240 = _mm256_setzero_ps();
            __m256 res250 = _mm256_setzero_ps();
            __m256 res260 = _mm256_setzero_ps();
            __m256 res270 = _mm256_setzero_ps();
            __m256 res280 = _mm256_setzero_ps();
            __m256 res290 = _mm256_setzero_ps();
            __m256 res300 = _mm256_setzero_ps();
            __m256 res310 = _mm256_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 64) {
                __m256 pos00 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m256 neg00 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m256 pos10 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m256 neg10 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m256 pos20 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m256 neg20 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m256 pos30 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m256 neg30 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m256 pos40 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 0);
                __m256 neg40 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 0);
                __m256 pos50 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 0);
                __m256 neg50 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 0);
                __m256 pos60 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 0);
                __m256 neg60 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 0);
                __m256 pos70 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 0);
                __m256 neg70 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 0);
                __m256 pos80 = _mm256_load_ps(X + row_index[k + 16] * M_ROW + i + 0);
                __m256 neg80 = _mm256_load_ps(X + row_index[k + 17] * M_ROW + i + 0);
                __m256 pos90 = _mm256_load_ps(X + row_index[k + 18] * M_ROW + i + 0);
                __m256 neg90 = _mm256_load_ps(X + row_index[k + 19] * M_ROW + i + 0);
                __m256 pos100 = _mm256_load_ps(X + row_index[k + 20] * M_ROW + i + 0);
                __m256 neg100 = _mm256_load_ps(X + row_index[k + 21] * M_ROW + i + 0);
                __m256 pos110 = _mm256_load_ps(X + row_index[k + 22] * M_ROW + i + 0);
                __m256 neg110 = _mm256_load_ps(X + row_index[k + 23] * M_ROW + i + 0);
                __m256 pos120 = _mm256_load_ps(X + row_index[k + 24] * M_ROW + i + 0);
                __m256 neg120 = _mm256_load_ps(X + row_index[k + 25] * M_ROW + i + 0);
                __m256 pos130 = _mm256_load_ps(X + row_index[k + 26] * M_ROW + i + 0);
                __m256 neg130 = _mm256_load_ps(X + row_index[k + 27] * M_ROW + i + 0);
                __m256 pos140 = _mm256_load_ps(X + row_index[k + 28] * M_ROW + i + 0);
                __m256 neg140 = _mm256_load_ps(X + row_index[k + 29] * M_ROW + i + 0);
                __m256 pos150 = _mm256_load_ps(X + row_index[k + 30] * M_ROW + i + 0);
                __m256 neg150 = _mm256_load_ps(X + row_index[k + 31] * M_ROW + i + 0);
                __m256 pos160 = _mm256_load_ps(X + row_index[k + 32] * M_ROW + i + 0);
                __m256 neg160 = _mm256_load_ps(X + row_index[k + 33] * M_ROW + i + 0);
                __m256 pos170 = _mm256_load_ps(X + row_index[k + 34] * M_ROW + i + 0);
                __m256 neg170 = _mm256_load_ps(X + row_index[k + 35] * M_ROW + i + 0);
                __m256 pos180 = _mm256_load_ps(X + row_index[k + 36] * M_ROW + i + 0);
                __m256 neg180 = _mm256_load_ps(X + row_index[k + 37] * M_ROW + i + 0);
                __m256 pos190 = _mm256_load_ps(X + row_index[k + 38] * M_ROW + i + 0);
                __m256 neg190 = _mm256_load_ps(X + row_index[k + 39] * M_ROW + i + 0);
                __m256 pos200 = _mm256_load_ps(X + row_index[k + 40] * M_ROW + i + 0);
                __m256 neg200 = _mm256_load_ps(X + row_index[k + 41] * M_ROW + i + 0);
                __m256 pos210 = _mm256_load_ps(X + row_index[k + 42] * M_ROW + i + 0);
                __m256 neg210 = _mm256_load_ps(X + row_index[k + 43] * M_ROW + i + 0);
                __m256 pos220 = _mm256_load_ps(X + row_index[k + 44] * M_ROW + i + 0);
                __m256 neg220 = _mm256_load_ps(X + row_index[k + 45] * M_ROW + i + 0);
                __m256 pos230 = _mm256_load_ps(X + row_index[k + 46] * M_ROW + i + 0);
                __m256 neg230 = _mm256_load_ps(X + row_index[k + 47] * M_ROW + i + 0);
                __m256 pos240 = _mm256_load_ps(X + row_index[k + 48] * M_ROW + i + 0);
                __m256 neg240 = _mm256_load_ps(X + row_index[k + 49] * M_ROW + i + 0);
                __m256 pos250 = _mm256_load_ps(X + row_index[k + 50] * M_ROW + i + 0);
                __m256 neg250 = _mm256_load_ps(X + row_index[k + 51] * M_ROW + i + 0);
                __m256 pos260 = _mm256_load_ps(X + row_index[k + 52] * M_ROW + i + 0);
                __m256 neg260 = _mm256_load_ps(X + row_index[k + 53] * M_ROW + i + 0);
                __m256 pos270 = _mm256_load_ps(X + row_index[k + 54] * M_ROW + i + 0);
                __m256 neg270 = _mm256_load_ps(X + row_index[k + 55] * M_ROW + i + 0);
                __m256 pos280 = _mm256_load_ps(X + row_index[k + 56] * M_ROW + i + 0);
                __m256 neg280 = _mm256_load_ps(X + row_index[k + 57] * M_ROW + i + 0);
                __m256 pos290 = _mm256_load_ps(X + row_index[k + 58] * M_ROW + i + 0);
                __m256 neg290 = _mm256_load_ps(X + row_index[k + 59] * M_ROW + i + 0);
                __m256 pos300 = _mm256_load_ps(X + row_index[k + 60] * M_ROW + i + 0);
                __m256 neg300 = _mm256_load_ps(X + row_index[k + 61] * M_ROW + i + 0);
                __m256 pos310 = _mm256_load_ps(X + row_index[k + 62] * M_ROW + i + 0);
                __m256 neg310 = _mm256_load_ps(X + row_index[k + 63] * M_ROW + i + 0);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res20 = _mm256_add_ps(res20, _mm256_sub_ps(pos20, neg20));
                res30 = _mm256_add_ps(res30, _mm256_sub_ps(pos30, neg30));
                res40 = _mm256_add_ps(res40, _mm256_sub_ps(pos40, neg40));
                res50 = _mm256_add_ps(res50, _mm256_sub_ps(pos50, neg50));
                res60 = _mm256_add_ps(res60, _mm256_sub_ps(pos60, neg60));
                res70 = _mm256_add_ps(res70, _mm256_sub_ps(pos70, neg70));
                res80 = _mm256_add_ps(res80, _mm256_sub_ps(pos80, neg80));
                res90 = _mm256_add_ps(res90, _mm256_sub_ps(pos90, neg90));
                res100 = _mm256_add_ps(res100, _mm256_sub_ps(pos100, neg100));
                res110 = _mm256_add_ps(res110, _mm256_sub_ps(pos110, neg110));
                res120 = _mm256_add_ps(res120, _mm256_sub_ps(pos120, neg120));
                res130 = _mm256_add_ps(res130, _mm256_sub_ps(pos130, neg130));
                res140 = _mm256_add_ps(res140, _mm256_sub_ps(pos140, neg140));
                res150 = _mm256_add_ps(res150, _mm256_sub_ps(pos150, neg150));
                res160 = _mm256_add_ps(res160, _mm256_sub_ps(pos160, neg160));
                res170 = _mm256_add_ps(res170, _mm256_sub_ps(pos170, neg170));
                res180 = _mm256_add_ps(res180, _mm256_sub_ps(pos180, neg180));
                res190 = _mm256_add_ps(res190, _mm256_sub_ps(pos190, neg190));
                res200 = _mm256_add_ps(res200, _mm256_sub_ps(pos200, neg200));
                res210 = _mm256_add_ps(res210, _mm256_sub_ps(pos210, neg210));
                res220 = _mm256_add_ps(res220, _mm256_sub_ps(pos220, neg220));
                res230 = _mm256_add_ps(res230, _mm256_sub_ps(pos230, neg230));
                res240 = _mm256_add_ps(res240, _mm256_sub_ps(pos240, neg240));
                res250 = _mm256_add_ps(res250, _mm256_sub_ps(pos250, neg250));
                res260 = _mm256_add_ps(res260, _mm256_sub_ps(pos260, neg260));
                res270 = _mm256_add_ps(res270, _mm256_sub_ps(pos270, neg270));
                res280 = _mm256_add_ps(res280, _mm256_sub_ps(pos280, neg280));
                res290 = _mm256_add_ps(res290, _mm256_sub_ps(pos290, neg290));
                res300 = _mm256_add_ps(res300, _mm256_sub_ps(pos300, neg300));
                res310 = _mm256_add_ps(res310, _mm256_sub_ps(pos310, neg310));
            }
            for (int k = groupData[1]; k < groupData[2]; k++) {
                res00 = _mm256_add_ps(res00, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[2]; k < groupData[3]; k++) {
                res00 = _mm256_sub_ps(res00, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[3]; k < groupData[4]; k++) {
                res10 = _mm256_add_ps(res10, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[4]; k < groupData[5]; k++) {
                res10 = _mm256_sub_ps(res10, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[5]; k < groupData[6]; k++) {
                res20 = _mm256_add_ps(res20, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[6]; k < groupData[7]; k++) {
                res20 = _mm256_sub_ps(res20, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[7]; k < groupData[8]; k++) {
                res30 = _mm256_add_ps(res30, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[8]; k < groupData[9]; k++) {
                res30 = _mm256_sub_ps(res30, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[9]; k < groupData[10]; k++) {
                res40 = _mm256_add_ps(res40, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[10]; k < groupData[11]; k++) {
                res40 = _mm256_sub_ps(res40, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[11]; k < groupData[12]; k++) {
                res50 = _mm256_add_ps(res50, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[12]; k < groupData[13]; k++) {
                res50 = _mm256_sub_ps(res50, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[13]; k < groupData[14]; k++) {
                res60 = _mm256_add_ps(res60, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[14]; k < groupData[15]; k++) {
                res60 = _mm256_sub_ps(res60, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[15]; k < groupData[16]; k++) {
                res70 = _mm256_add_ps(res70, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[16]; k < groupData[17]; k++) {
                res70 = _mm256_sub_ps(res70, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[17]; k < groupData[18]; k++) {
                res80 = _mm256_add_ps(res80, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[18]; k < groupData[19]; k++) {
                res80 = _mm256_sub_ps(res80, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[19]; k < groupData[20]; k++) {
                res90 = _mm256_add_ps(res90, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[20]; k < groupData[21]; k++) {
                res90 = _mm256_sub_ps(res90, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[21]; k < groupData[22]; k++) {
                res100 = _mm256_add_ps(res100, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[22]; k < groupData[23]; k++) {
                res100 = _mm256_sub_ps(res100, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[23]; k < groupData[24]; k++) {
                res110 = _mm256_add_ps(res110, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[24]; k < groupData[25]; k++) {
                res110 = _mm256_sub_ps(res110, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[25]; k < groupData[26]; k++) {
                res120 = _mm256_add_ps(res120, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[26]; k < groupData[27]; k++) {
                res120 = _mm256_sub_ps(res120, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[27]; k < groupData[28]; k++) {
                res130 = _mm256_add_ps(res130, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[28]; k < groupData[29]; k++) {
                res130 = _mm256_sub_ps(res130, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[29]; k < groupData[30]; k++) {
                res140 = _mm256_add_ps(res140, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[30]; k < groupData[31]; k++) {
                res140 = _mm256_sub_ps(res140, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[31]; k < groupData[32]; k++) {
                res150 = _mm256_add_ps(res150, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[32]; k < groupData[33]; k++) {
                res150 = _mm256_sub_ps(res150, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[33]; k < groupData[34]; k++) {
                res160 = _mm256_add_ps(res160, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[34]; k < groupData[35]; k++) {
                res160 = _mm256_sub_ps(res160, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[35]; k < groupData[36]; k++) {
                res170 = _mm256_add_ps(res170, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[36]; k < groupData[37]; k++) {
                res170 = _mm256_sub_ps(res170, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[37]; k < groupData[38]; k++) {
                res180 = _mm256_add_ps(res180, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[38]; k < groupData[39]; k++) {
                res180 = _mm256_sub_ps(res180, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[39]; k < groupData[40]; k++) {
                res190 = _mm256_add_ps(res190, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[40]; k < groupData[41]; k++) {
                res190 = _mm256_sub_ps(res190, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[41]; k < groupData[42]; k++) {
                res200 = _mm256_add_ps(res200, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[42]; k < groupData[43]; k++) {
                res200 = _mm256_sub_ps(res200, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[43]; k < groupData[44]; k++) {
                res210 = _mm256_add_ps(res210, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[44]; k < groupData[45]; k++) {
                res210 = _mm256_sub_ps(res210, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[45]; k < groupData[46]; k++) {
                res220 = _mm256_add_ps(res220, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[46]; k < groupData[47]; k++) {
                res220 = _mm256_sub_ps(res220, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[47]; k < groupData[48]; k++) {
                res230 = _mm256_add_ps(res230, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[48]; k < groupData[49]; k++) {
                res230 = _mm256_sub_ps(res230, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[49]; k < groupData[50]; k++) {
                res240 = _mm256_add_ps(res240, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[50]; k < groupData[51]; k++) {
                res240 = _mm256_sub_ps(res240, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[51]; k < groupData[52]; k++) {
                res250 = _mm256_add_ps(res250, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[52]; k < groupData[53]; k++) {
                res250 = _mm256_sub_ps(res250, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[53]; k < groupData[54]; k++) {
                res260 = _mm256_add_ps(res260, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[54]; k < groupData[55]; k++) {
                res260 = _mm256_sub_ps(res260, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[55]; k < groupData[56]; k++) {
                res270 = _mm256_add_ps(res270, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[56]; k < groupData[57]; k++) {
                res270 = _mm256_sub_ps(res270, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[57]; k < groupData[58]; k++) {
                res280 = _mm256_add_ps(res280, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[58]; k < groupData[59]; k++) {
                res280 = _mm256_sub_ps(res280, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[59]; k < groupData[60]; k++) {
                res290 = _mm256_add_ps(res290, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[60]; k < groupData[61]; k++) {
                res290 = _mm256_sub_ps(res290, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[61]; k < groupData[62]; k++) {
                res300 = _mm256_add_ps(res300, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[62]; k < groupData[63]; k++) {
                res300 = _mm256_sub_ps(res300, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[63]; k < groupData[64]; k++) {
                res310 = _mm256_add_ps(res310, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[64]; k < groupData[65]; k++) {
                res310 = _mm256_sub_ps(res310, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            _mm256_store_ps(result + (j * 32 + 0) * M_ROW + i + 0, res00);
            _mm256_store_ps(result + (j * 32 + 1) * M_ROW + i + 0, res10);
            _mm256_store_ps(result + (j * 32 + 2) * M_ROW + i + 0, res20);
            _mm256_store_ps(result + (j * 32 + 3) * M_ROW + i + 0, res30);
            _mm256_store_ps(result + (j * 32 + 4) * M_ROW + i + 0, res40);
            _mm256_store_ps(result + (j * 32 + 5) * M_ROW + i + 0, res50);
            _mm256_store_ps(result + (j * 32 + 6) * M_ROW + i + 0, res60);
            _mm256_store_ps(result + (j * 32 + 7) * M_ROW + i + 0, res70);
            _mm256_store_ps(result + (j * 32 + 8) * M_ROW + i + 0, res80);
            _mm256_store_ps(result + (j * 32 + 9) * M_ROW + i + 0, res90);
            _mm256_store_ps(result + (j * 32 + 10) * M_ROW + i + 0, res100);
            _mm256_store_ps(result + (j * 32 + 11) * M_ROW + i + 0, res110);
            _mm256_store_ps(result + (j * 32 + 12) * M_ROW + i + 0, res120);
            _mm256_store_ps(result + (j * 32 + 13) * M_ROW + i + 0, res130);
            _mm256_store_ps(result + (j * 32 + 14) * M_ROW + i + 0, res140);
            _mm256_store_ps(result + (j * 32 + 15) * M_ROW + i + 0, res150);
            _mm256_store_ps(result + (j * 32 + 16) * M_ROW + i + 0, res160);
            _mm256_store_ps(result + (j * 32 + 17) * M_ROW + i + 0, res170);
            _mm256_store_ps(result + (j * 32 + 18) * M_ROW + i + 0, res180);
            _mm256_store_ps(result + (j * 32 + 19) * M_ROW + i + 0, res190);
            _mm256_store_ps(result + (j * 32 + 20) * M_ROW + i + 0, res200);
            _mm256_store_ps(result + (j * 32 + 21) * M_ROW + i + 0, res210);
            _mm256_store_ps(result + (j * 32 + 22) * M_ROW + i + 0, res220);
            _mm256_store_ps(result + (j * 32 + 23) * M_ROW + i + 0, res230);
            _mm256_store_ps(result + (j * 32 + 24) * M_ROW + i + 0, res240);
            _mm256_store_ps(result + (j * 32 + 25) * M_ROW + i + 0, res250);
            _mm256_store_ps(result + (j * 32 + 26) * M_ROW + i + 0, res260);
            _mm256_store_ps(result + (j * 32 + 27) * M_ROW + i + 0, res270);
            _mm256_store_ps(result + (j * 32 + 28) * M_ROW + i + 0, res280);
            _mm256_store_ps(result + (j * 32 + 29) * M_ROW + i + 0, res290);
            _mm256_store_ps(result + (j * 32 + 30) * M_ROW + i + 0, res300);
            _mm256_store_ps(result + (j * 32 + 31) * M_ROW + i + 0, res310);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Merged_GroupMin_16xG4_AVX2_OpenMP(const float* X, const int32_t* metadata, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 4; j++) {
        const int* groupData  = &metadata[j * 10];
        for (int i = 0; i < M_ROW; i += 16) {
            __m256 res00 = _mm256_setzero_ps();
            __m256 res01 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res11 = _mm256_setzero_ps();
            __m256 res20 = _mm256_setzero_ps();
            __m256 res21 = _mm256_setzero_ps();
            __m256 res30 = _mm256_setzero_ps();
            __m256 res31 = _mm256_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 8) {
                __m256 pos00 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m256 neg00 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m256 pos10 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m256 neg10 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m256 pos20 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m256 neg20 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m256 pos30 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m256 neg30 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m256 pos01 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 8);
                __m256 neg01 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 8);
                __m256 pos11 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 8);
                __m256 neg11 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 8);
                __m256 pos21 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 8);
                __m256 neg21 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 8);
                __m256 pos31 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 8);
                __m256 neg31 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 8);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res20 = _mm256_add_ps(res20, _mm256_sub_ps(pos20, neg20));
                res30 = _mm256_add_ps(res30, _mm256_sub_ps(pos30, neg30));
                res01 = _mm256_add_ps(res01, _mm256_sub_ps(pos01, neg01));
                res11 = _mm256_add_ps(res11, _mm256_sub_ps(pos11, neg11));
                res21 = _mm256_add_ps(res21, _mm256_sub_ps(pos21, neg21));
                res31 = _mm256_add_ps(res31, _mm256_sub_ps(pos31, neg31));
            }
            for (int k = groupData[1]; k < groupData[2]; k++) {
                res00 = _mm256_add_ps(res00, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res01 = _mm256_add_ps(res01, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[2]; k < groupData[3]; k++) {
                res00 = _mm256_sub_ps(res00, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res01 = _mm256_sub_ps(res01, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[3]; k < groupData[4]; k++) {
                res10 = _mm256_add_ps(res10, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res11 = _mm256_add_ps(res11, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[4]; k < groupData[5]; k++) {
                res10 = _mm256_sub_ps(res10, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res11 = _mm256_sub_ps(res11, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[5]; k < groupData[6]; k++) {
                res20 = _mm256_add_ps(res20, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res21 = _mm256_add_ps(res21, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[6]; k < groupData[7]; k++) {
                res20 = _mm256_sub_ps(res20, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res21 = _mm256_sub_ps(res21, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[7]; k < groupData[8]; k++) {
                res30 = _mm256_add_ps(res30, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res31 = _mm256_add_ps(res31, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[8]; k < groupData[9]; k++) {
                res30 = _mm256_sub_ps(res30, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res31 = _mm256_sub_ps(res31, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            _mm256_store_ps(result + (j * 4 + 0) * M_ROW + i + 0, res00);
            _mm256_store_ps(result + (j * 4 + 1) * M_ROW + i + 0, res10);
            _mm256_store_ps(result + (j * 4 + 2) * M_ROW + i + 0, res20);
            _mm256_store_ps(result + (j * 4 + 3) * M_ROW + i + 0, res30);
            _mm256_store_ps(result + (j * 4 + 0) * M_ROW + i + 8, res01);
            _mm256_store_ps(result + (j * 4 + 1) * M_ROW + i + 8, res11);
            _mm256_store_ps(result + (j * 4 + 2) * M_ROW + i + 8, res21);
            _mm256_store_ps(result + (j * 4 + 3) * M_ROW + i + 8, res31);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Merged_GroupMin_16xG8_AVX2_OpenMP(const float* X, const int32_t* metadata, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 8; j++) {
        const int* groupData  = &metadata[j * 18];
        for (int i = 0; i < M_ROW; i += 16) {
            __m256 res00 = _mm256_setzero_ps();
            __m256 res01 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res11 = _mm256_setzero_ps();
            __m256 res20 = _mm256_setzero_ps();
            __m256 res21 = _mm256_setzero_ps();
            __m256 res30 = _mm256_setzero_ps();
            __m256 res31 = _mm256_setzero_ps();
            __m256 res40 = _mm256_setzero_ps();
            __m256 res41 = _mm256_setzero_ps();
            __m256 res50 = _mm256_setzero_ps();
            __m256 res51 = _mm256_setzero_ps();
            __m256 res60 = _mm256_setzero_ps();
            __m256 res61 = _mm256_setzero_ps();
            __m256 res70 = _mm256_setzero_ps();
            __m256 res71 = _mm256_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 16) {
                __m256 pos00 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m256 neg00 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m256 pos10 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m256 neg10 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m256 pos20 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m256 neg20 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m256 pos30 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m256 neg30 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m256 pos40 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 0);
                __m256 neg40 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 0);
                __m256 pos50 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 0);
                __m256 neg50 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 0);
                __m256 pos60 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 0);
                __m256 neg60 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 0);
                __m256 pos70 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 0);
                __m256 neg70 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 0);
                __m256 pos01 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 8);
                __m256 neg01 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 8);
                __m256 pos11 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 8);
                __m256 neg11 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 8);
                __m256 pos21 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 8);
                __m256 neg21 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 8);
                __m256 pos31 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 8);
                __m256 neg31 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 8);
                __m256 pos41 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 8);
                __m256 neg41 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 8);
                __m256 pos51 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 8);
                __m256 neg51 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 8);
                __m256 pos61 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 8);
                __m256 neg61 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 8);
                __m256 pos71 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 8);
                __m256 neg71 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 8);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res20 = _mm256_add_ps(res20, _mm256_sub_ps(pos20, neg20));
                res30 = _mm256_add_ps(res30, _mm256_sub_ps(pos30, neg30));
                res40 = _mm256_add_ps(res40, _mm256_sub_ps(pos40, neg40));
                res50 = _mm256_add_ps(res50, _mm256_sub_ps(pos50, neg50));
                res60 = _mm256_add_ps(res60, _mm256_sub_ps(pos60, neg60));
                res70 = _mm256_add_ps(res70, _mm256_sub_ps(pos70, neg70));
                res01 = _mm256_add_ps(res01, _mm256_sub_ps(pos01, neg01));
                res11 = _mm256_add_ps(res11, _mm256_sub_ps(pos11, neg11));
                res21 = _mm256_add_ps(res21, _mm256_sub_ps(pos21, neg21));
                res31 = _mm256_add_ps(res31, _mm256_sub_ps(pos31, neg31));
                res41 = _mm256_add_ps(res41, _mm256_sub_ps(pos41, neg41));
                res51 = _mm256_add_ps(res51, _mm256_sub_ps(pos51, neg51));
                res61 = _mm256_add_ps(res61, _mm256_sub_ps(pos61, neg61));
                res71 = _mm256_add_ps(res71, _mm256_sub_ps(pos71, neg71));
            }
            for (int k = groupData[1]; k < groupData[2]; k++) {
                res00 = _mm256_add_ps(res00, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res01 = _mm256_add_ps(res01, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[2]; k < groupData[3]; k++) {
                res00 = _mm256_sub_ps(res00, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res01 = _mm256_sub_ps(res01, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[3]; k < groupData[4]; k++) {
                res10 = _mm256_add_ps(res10, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res11 = _mm256_add_ps(res11, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[4]; k < groupData[5]; k++) {
                res10 = _mm256_sub_ps(res10, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res11 = _mm256_sub_ps(res11, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[5]; k < groupData[6]; k++) {
                res20 = _mm256_add_ps(res20, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res21 = _mm256_add_ps(res21, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[6]; k < groupData[7]; k++) {
                res20 = _mm256_sub_ps(res20, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res21 = _mm256_sub_ps(res21, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[7]; k < groupData[8]; k++) {
                res30 = _mm256_add_ps(res30, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res31 = _mm256_add_ps(res31, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[8]; k < groupData[9]; k++) {
                res30 = _mm256_sub_ps(res30, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res31 = _mm256_sub_ps(res31, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[9]; k < groupData[10]; k++) {
                res40 = _mm256_add_ps(res40, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res41 = _mm256_add_ps(res41, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[10]; k < groupData[11]; k++) {
                res40 = _mm256_sub_ps(res40, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res41 = _mm256_sub_ps(res41, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[11]; k < groupData[12]; k++) {
                res50 = _mm256_add_ps(res50, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res51 = _mm256_add_ps(res51, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[12]; k < groupData[13]; k++) {
                res50 = _mm256_sub_ps(res50, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res51 = _mm256_sub_ps(res51, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[13]; k < groupData[14]; k++) {
                res60 = _mm256_add_ps(res60, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res61 = _mm256_add_ps(res61, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[14]; k < groupData[15]; k++) {
                res60 = _mm256_sub_ps(res60, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res61 = _mm256_sub_ps(res61, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[15]; k < groupData[16]; k++) {
                res70 = _mm256_add_ps(res70, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res71 = _mm256_add_ps(res71, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[16]; k < groupData[17]; k++) {
                res70 = _mm256_sub_ps(res70, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res71 = _mm256_sub_ps(res71, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            _mm256_store_ps(result + (j * 8 + 0) * M_ROW + i + 0, res00);
            _mm256_store_ps(result + (j * 8 + 1) * M_ROW + i + 0, res10);
            _mm256_store_ps(result + (j * 8 + 2) * M_ROW + i + 0, res20);
            _mm256_store_ps(result + (j * 8 + 3) * M_ROW + i + 0, res30);
            _mm256_store_ps(result + (j * 8 + 4) * M_ROW + i + 0, res40);
            _mm256_store_ps(result + (j * 8 + 5) * M_ROW + i + 0, res50);
            _mm256_store_ps(result + (j * 8 + 6) * M_ROW + i + 0, res60);
            _mm256_store_ps(result + (j * 8 + 7) * M_ROW + i + 0, res70);
            _mm256_store_ps(result + (j * 8 + 0) * M_ROW + i + 8, res01);
            _mm256_store_ps(result + (j * 8 + 1) * M_ROW + i + 8, res11);
            _mm256_store_ps(result + (j * 8 + 2) * M_ROW + i + 8, res21);
            _mm256_store_ps(result + (j * 8 + 3) * M_ROW + i + 8, res31);
            _mm256_store_ps(result + (j * 8 + 4) * M_ROW + i + 8, res41);
            _mm256_store_ps(result + (j * 8 + 5) * M_ROW + i + 8, res51);
            _mm256_store_ps(result + (j * 8 + 6) * M_ROW + i + 8, res61);
            _mm256_store_ps(result + (j * 8 + 7) * M_ROW + i + 8, res71);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Merged_GroupMin_16xG16_AVX2_OpenMP(const float* X, const int32_t* metadata, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 16; j++) {
        const int* groupData  = &metadata[j * 34];
        for (int i = 0; i < M_ROW; i += 16) {
            __m256 res00 = _mm256_setzero_ps();
            __m256 res01 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res11 = _mm256_setzero_ps();
            __m256 res20 = _mm256_setzero_ps();
            __m256 res21 = _mm256_setzero_ps();
            __m256 res30 = _mm256_setzero_ps();
            __m256 res31 = _mm256_setzero_ps();
            __m256 res40 = _mm256_setzero_ps();
            __m256 res41 = _mm256_setzero_ps();
            __m256 res50 = _mm256_setzero_ps();
            __m256 res51 = _mm256_setzero_ps();
            __m256 res60 = _mm256_setzero_ps();
            __m256 res61 = _mm256_setzero_ps();
            __m256 res70 = _mm256_setzero_ps();
            __m256 res71 = _mm256_setzero_ps();
            __m256 res80 = _mm256_setzero_ps();
            __m256 res81 = _mm256_setzero_ps();
            __m256 res90 = _mm256_setzero_ps();
            __m256 res91 = _mm256_setzero_ps();
            __m256 res100 = _mm256_setzero_ps();
            __m256 res101 = _mm256_setzero_ps();
            __m256 res110 = _mm256_setzero_ps();
            __m256 res111 = _mm256_setzero_ps();
            __m256 res120 = _mm256_setzero_ps();
            __m256 res121 = _mm256_setzero_ps();
            __m256 res130 = _mm256_setzero_ps();
            __m256 res131 = _mm256_setzero_ps();
            __m256 res140 = _mm256_setzero_ps();
            __m256 res141 = _mm256_setzero_ps();
            __m256 res150 = _mm256_setzero_ps();
            __m256 res151 = _mm256_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 32) {
                __m256 pos00 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m256 neg00 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m256 pos10 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m256 neg10 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m256 pos20 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m256 neg20 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m256 pos30 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m256 neg30 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m256 pos40 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 0);
                __m256 neg40 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 0);
                __m256 pos50 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 0);
                __m256 neg50 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 0);
                __m256 pos60 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 0);
                __m256 neg60 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 0);
                __m256 pos70 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 0);
                __m256 neg70 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 0);
                __m256 pos80 = _mm256_load_ps(X + row_index[k + 16] * M_ROW + i + 0);
                __m256 neg80 = _mm256_load_ps(X + row_index[k + 17] * M_ROW + i + 0);
                __m256 pos90 = _mm256_load_ps(X + row_index[k + 18] * M_ROW + i + 0);
                __m256 neg90 = _mm256_load_ps(X + row_index[k + 19] * M_ROW + i + 0);
                __m256 pos100 = _mm256_load_ps(X + row_index[k + 20] * M_ROW + i + 0);
                __m256 neg100 = _mm256_load_ps(X + row_index[k + 21] * M_ROW + i + 0);
                __m256 pos110 = _mm256_load_ps(X + row_index[k + 22] * M_ROW + i + 0);
                __m256 neg110 = _mm256_load_ps(X + row_index[k + 23] * M_ROW + i + 0);
                __m256 pos120 = _mm256_load_ps(X + row_index[k + 24] * M_ROW + i + 0);
                __m256 neg120 = _mm256_load_ps(X + row_index[k + 25] * M_ROW + i + 0);
                __m256 pos130 = _mm256_load_ps(X + row_index[k + 26] * M_ROW + i + 0);
                __m256 neg130 = _mm256_load_ps(X + row_index[k + 27] * M_ROW + i + 0);
                __m256 pos140 = _mm256_load_ps(X + row_index[k + 28] * M_ROW + i + 0);
                __m256 neg140 = _mm256_load_ps(X + row_index[k + 29] * M_ROW + i + 0);
                __m256 pos150 = _mm256_load_ps(X + row_index[k + 30] * M_ROW + i + 0);
                __m256 neg150 = _mm256_load_ps(X + row_index[k + 31] * M_ROW + i + 0);
                __m256 pos01 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 8);
                __m256 neg01 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 8);
                __m256 pos11 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 8);
                __m256 neg11 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 8);
                __m256 pos21 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 8);
                __m256 neg21 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 8);
                __m256 pos31 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 8);
                __m256 neg31 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 8);
                __m256 pos41 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 8);
                __m256 neg41 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 8);
                __m256 pos51 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 8);
                __m256 neg51 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 8);
                __m256 pos61 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 8);
                __m256 neg61 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 8);
                __m256 pos71 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 8);
                __m256 neg71 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 8);
                __m256 pos81 = _mm256_load_ps(X + row_index[k + 16] * M_ROW + i + 8);
                __m256 neg81 = _mm256_load_ps(X + row_index[k + 17] * M_ROW + i + 8);
                __m256 pos91 = _mm256_load_ps(X + row_index[k + 18] * M_ROW + i + 8);
                __m256 neg91 = _mm256_load_ps(X + row_index[k + 19] * M_ROW + i + 8);
                __m256 pos101 = _mm256_load_ps(X + row_index[k + 20] * M_ROW + i + 8);
                __m256 neg101 = _mm256_load_ps(X + row_index[k + 21] * M_ROW + i + 8);
                __m256 pos111 = _mm256_load_ps(X + row_index[k + 22] * M_ROW + i + 8);
                __m256 neg111 = _mm256_load_ps(X + row_index[k + 23] * M_ROW + i + 8);
                __m256 pos121 = _mm256_load_ps(X + row_index[k + 24] * M_ROW + i + 8);
                __m256 neg121 = _mm256_load_ps(X + row_index[k + 25] * M_ROW + i + 8);
                __m256 pos131 = _mm256_load_ps(X + row_index[k + 26] * M_ROW + i + 8);
                __m256 neg131 = _mm256_load_ps(X + row_index[k + 27] * M_ROW + i + 8);
                __m256 pos141 = _mm256_load_ps(X + row_index[k + 28] * M_ROW + i + 8);
                __m256 neg141 = _mm256_load_ps(X + row_index[k + 29] * M_ROW + i + 8);
                __m256 pos151 = _mm256_load_ps(X + row_index[k + 30] * M_ROW + i + 8);
                __m256 neg151 = _mm256_load_ps(X + row_index[k + 31] * M_ROW + i + 8);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res20 = _mm256_add_ps(res20, _mm256_sub_ps(pos20, neg20));
                res30 = _mm256_add_ps(res30, _mm256_sub_ps(pos30, neg30));
                res40 = _mm256_add_ps(res40, _mm256_sub_ps(pos40, neg40));
                res50 = _mm256_add_ps(res50, _mm256_sub_ps(pos50, neg50));
                res60 = _mm256_add_ps(res60, _mm256_sub_ps(pos60, neg60));
                res70 = _mm256_add_ps(res70, _mm256_sub_ps(pos70, neg70));
                res80 = _mm256_add_ps(res80, _mm256_sub_ps(pos80, neg80));
                res90 = _mm256_add_ps(res90, _mm256_sub_ps(pos90, neg90));
                res100 = _mm256_add_ps(res100, _mm256_sub_ps(pos100, neg100));
                res110 = _mm256_add_ps(res110, _mm256_sub_ps(pos110, neg110));
                res120 = _mm256_add_ps(res120, _mm256_sub_ps(pos120, neg120));
                res130 = _mm256_add_ps(res130, _mm256_sub_ps(pos130, neg130));
                res140 = _mm256_add_ps(res140, _mm256_sub_ps(pos140, neg140));
                res150 = _mm256_add_ps(res150, _mm256_sub_ps(pos150, neg150));
                res01 = _mm256_add_ps(res01, _mm256_sub_ps(pos01, neg01));
                res11 = _mm256_add_ps(res11, _mm256_sub_ps(pos11, neg11));
                res21 = _mm256_add_ps(res21, _mm256_sub_ps(pos21, neg21));
                res31 = _mm256_add_ps(res31, _mm256_sub_ps(pos31, neg31));
                res41 = _mm256_add_ps(res41, _mm256_sub_ps(pos41, neg41));
                res51 = _mm256_add_ps(res51, _mm256_sub_ps(pos51, neg51));
                res61 = _mm256_add_ps(res61, _mm256_sub_ps(pos61, neg61));
                res71 = _mm256_add_ps(res71, _mm256_sub_ps(pos71, neg71));
                res81 = _mm256_add_ps(res81, _mm256_sub_ps(pos81, neg81));
                res91 = _mm256_add_ps(res91, _mm256_sub_ps(pos91, neg91));
                res101 = _mm256_add_ps(res101, _mm256_sub_ps(pos101, neg101));
                res111 = _mm256_add_ps(res111, _mm256_sub_ps(pos111, neg111));
                res121 = _mm256_add_ps(res121, _mm256_sub_ps(pos121, neg121));
                res131 = _mm256_add_ps(res131, _mm256_sub_ps(pos131, neg131));
                res141 = _mm256_add_ps(res141, _mm256_sub_ps(pos141, neg141));
                res151 = _mm256_add_ps(res151, _mm256_sub_ps(pos151, neg151));
            }
            for (int k = groupData[1]; k < groupData[2]; k++) {
                res00 = _mm256_add_ps(res00, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res01 = _mm256_add_ps(res01, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[2]; k < groupData[3]; k++) {
                res00 = _mm256_sub_ps(res00, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res01 = _mm256_sub_ps(res01, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[3]; k < groupData[4]; k++) {
                res10 = _mm256_add_ps(res10, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res11 = _mm256_add_ps(res11, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[4]; k < groupData[5]; k++) {
                res10 = _mm256_sub_ps(res10, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res11 = _mm256_sub_ps(res11, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[5]; k < groupData[6]; k++) {
                res20 = _mm256_add_ps(res20, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res21 = _mm256_add_ps(res21, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[6]; k < groupData[7]; k++) {
                res20 = _mm256_sub_ps(res20, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res21 = _mm256_sub_ps(res21, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[7]; k < groupData[8]; k++) {
                res30 = _mm256_add_ps(res30, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res31 = _mm256_add_ps(res31, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[8]; k < groupData[9]; k++) {
                res30 = _mm256_sub_ps(res30, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res31 = _mm256_sub_ps(res31, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[9]; k < groupData[10]; k++) {
                res40 = _mm256_add_ps(res40, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res41 = _mm256_add_ps(res41, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[10]; k < groupData[11]; k++) {
                res40 = _mm256_sub_ps(res40, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res41 = _mm256_sub_ps(res41, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[11]; k < groupData[12]; k++) {
                res50 = _mm256_add_ps(res50, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res51 = _mm256_add_ps(res51, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[12]; k < groupData[13]; k++) {
                res50 = _mm256_sub_ps(res50, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res51 = _mm256_sub_ps(res51, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[13]; k < groupData[14]; k++) {
                res60 = _mm256_add_ps(res60, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res61 = _mm256_add_ps(res61, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[14]; k < groupData[15]; k++) {
                res60 = _mm256_sub_ps(res60, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res61 = _mm256_sub_ps(res61, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[15]; k < groupData[16]; k++) {
                res70 = _mm256_add_ps(res70, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res71 = _mm256_add_ps(res71, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[16]; k < groupData[17]; k++) {
                res70 = _mm256_sub_ps(res70, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res71 = _mm256_sub_ps(res71, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[17]; k < groupData[18]; k++) {
                res80 = _mm256_add_ps(res80, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res81 = _mm256_add_ps(res81, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[18]; k < groupData[19]; k++) {
                res80 = _mm256_sub_ps(res80, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res81 = _mm256_sub_ps(res81, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[19]; k < groupData[20]; k++) {
                res90 = _mm256_add_ps(res90, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res91 = _mm256_add_ps(res91, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[20]; k < groupData[21]; k++) {
                res90 = _mm256_sub_ps(res90, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res91 = _mm256_sub_ps(res91, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[21]; k < groupData[22]; k++) {
                res100 = _mm256_add_ps(res100, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res101 = _mm256_add_ps(res101, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[22]; k < groupData[23]; k++) {
                res100 = _mm256_sub_ps(res100, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res101 = _mm256_sub_ps(res101, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[23]; k < groupData[24]; k++) {
                res110 = _mm256_add_ps(res110, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res111 = _mm256_add_ps(res111, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[24]; k < groupData[25]; k++) {
                res110 = _mm256_sub_ps(res110, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res111 = _mm256_sub_ps(res111, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[25]; k < groupData[26]; k++) {
                res120 = _mm256_add_ps(res120, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res121 = _mm256_add_ps(res121, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[26]; k < groupData[27]; k++) {
                res120 = _mm256_sub_ps(res120, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res121 = _mm256_sub_ps(res121, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[27]; k < groupData[28]; k++) {
                res130 = _mm256_add_ps(res130, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res131 = _mm256_add_ps(res131, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[28]; k < groupData[29]; k++) {
                res130 = _mm256_sub_ps(res130, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res131 = _mm256_sub_ps(res131, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[29]; k < groupData[30]; k++) {
                res140 = _mm256_add_ps(res140, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res141 = _mm256_add_ps(res141, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[30]; k < groupData[31]; k++) {
                res140 = _mm256_sub_ps(res140, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res141 = _mm256_sub_ps(res141, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[31]; k < groupData[32]; k++) {
                res150 = _mm256_add_ps(res150, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res151 = _mm256_add_ps(res151, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[32]; k < groupData[33]; k++) {
                res150 = _mm256_sub_ps(res150, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res151 = _mm256_sub_ps(res151, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            _mm256_store_ps(result + (j * 16 + 0) * M_ROW + i + 0, res00);
            _mm256_store_ps(result + (j * 16 + 1) * M_ROW + i + 0, res10);
            _mm256_store_ps(result + (j * 16 + 2) * M_ROW + i + 0, res20);
            _mm256_store_ps(result + (j * 16 + 3) * M_ROW + i + 0, res30);
            _mm256_store_ps(result + (j * 16 + 4) * M_ROW + i + 0, res40);
            _mm256_store_ps(result + (j * 16 + 5) * M_ROW + i + 0, res50);
            _mm256_store_ps(result + (j * 16 + 6) * M_ROW + i + 0, res60);
            _mm256_store_ps(result + (j * 16 + 7) * M_ROW + i + 0, res70);
            _mm256_store_ps(result + (j * 16 + 8) * M_ROW + i + 0, res80);
            _mm256_store_ps(result + (j * 16 + 9) * M_ROW + i + 0, res90);
            _mm256_store_ps(result + (j * 16 + 10) * M_ROW + i + 0, res100);
            _mm256_store_ps(result + (j * 16 + 11) * M_ROW + i + 0, res110);
            _mm256_store_ps(result + (j * 16 + 12) * M_ROW + i + 0, res120);
            _mm256_store_ps(result + (j * 16 + 13) * M_ROW + i + 0, res130);
            _mm256_store_ps(result + (j * 16 + 14) * M_ROW + i + 0, res140);
            _mm256_store_ps(result + (j * 16 + 15) * M_ROW + i + 0, res150);
            _mm256_store_ps(result + (j * 16 + 0) * M_ROW + i + 8, res01);
            _mm256_store_ps(result + (j * 16 + 1) * M_ROW + i + 8, res11);
            _mm256_store_ps(result + (j * 16 + 2) * M_ROW + i + 8, res21);
            _mm256_store_ps(result + (j * 16 + 3) * M_ROW + i + 8, res31);
            _mm256_store_ps(result + (j * 16 + 4) * M_ROW + i + 8, res41);
            _mm256_store_ps(result + (j * 16 + 5) * M_ROW + i + 8, res51);
            _mm256_store_ps(result + (j * 16 + 6) * M_ROW + i + 8, res61);
            _mm256_store_ps(result + (j * 16 + 7) * M_ROW + i + 8, res71);
            _mm256_store_ps(result + (j * 16 + 8) * M_ROW + i + 8, res81);
            _mm256_store_ps(result + (j * 16 + 9) * M_ROW + i + 8, res91);
            _mm256_store_ps(result + (j * 16 + 10) * M_ROW + i + 8, res101);
            _mm256_store_ps(result + (j * 16 + 11) * M_ROW + i + 8, res111);
            _mm256_store_ps(result + (j * 16 + 12) * M_ROW + i + 8, res121);
            _mm256_store_ps(result + (j * 16 + 13) * M_ROW + i + 8, res131);
            _mm256_store_ps(result + (j * 16 + 14) * M_ROW + i + 8, res141);
            _mm256_store_ps(result + (j * 16 + 15) * M_ROW + i + 8, res151);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Merged_GroupMin_16xG32_AVX2_OpenMP(const float* X, const int32_t* metadata, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 32; j++) {
        const int* groupData  = &metadata[j * 66];
        for (int i = 0; i < M_ROW; i += 16) {
            __m256 res00 = _mm256_setzero_ps();
            __m256 res01 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res11 = _mm256_setzero_ps();
            __m256 res20 = _mm256_setzero_ps();
            __m256 res21 = _mm256_setzero_ps();
            __m256 res30 = _mm256_setzero_ps();
            __m256 res31 = _mm256_setzero_ps();
            __m256 res40 = _mm256_setzero_ps();
            __m256 res41 = _mm256_setzero_ps();
            __m256 res50 = _mm256_setzero_ps();
            __m256 res51 = _mm256_setzero_ps();
            __m256 res60 = _mm256_setzero_ps();
            __m256 res61 = _mm256_setzero_ps();
            __m256 res70 = _mm256_setzero_ps();
            __m256 res71 = _mm256_setzero_ps();
            __m256 res80 = _mm256_setzero_ps();
            __m256 res81 = _mm256_setzero_ps();
            __m256 res90 = _mm256_setzero_ps();
            __m256 res91 = _mm256_setzero_ps();
            __m256 res100 = _mm256_setzero_ps();
            __m256 res101 = _mm256_setzero_ps();
            __m256 res110 = _mm256_setzero_ps();
            __m256 res111 = _mm256_setzero_ps();
            __m256 res120 = _mm256_setzero_ps();
            __m256 res121 = _mm256_setzero_ps();
            __m256 res130 = _mm256_setzero_ps();
            __m256 res131 = _mm256_setzero_ps();
            __m256 res140 = _mm256_setzero_ps();
            __m256 res141 = _mm256_setzero_ps();
            __m256 res150 = _mm256_setzero_ps();
            __m256 res151 = _mm256_setzero_ps();
            __m256 res160 = _mm256_setzero_ps();
            __m256 res161 = _mm256_setzero_ps();
            __m256 res170 = _mm256_setzero_ps();
            __m256 res171 = _mm256_setzero_ps();
            __m256 res180 = _mm256_setzero_ps();
            __m256 res181 = _mm256_setzero_ps();
            __m256 res190 = _mm256_setzero_ps();
            __m256 res191 = _mm256_setzero_ps();
            __m256 res200 = _mm256_setzero_ps();
            __m256 res201 = _mm256_setzero_ps();
            __m256 res210 = _mm256_setzero_ps();
            __m256 res211 = _mm256_setzero_ps();
            __m256 res220 = _mm256_setzero_ps();
            __m256 res221 = _mm256_setzero_ps();
            __m256 res230 = _mm256_setzero_ps();
            __m256 res231 = _mm256_setzero_ps();
            __m256 res240 = _mm256_setzero_ps();
            __m256 res241 = _mm256_setzero_ps();
            __m256 res250 = _mm256_setzero_ps();
            __m256 res251 = _mm256_setzero_ps();
            __m256 res260 = _mm256_setzero_ps();
            __m256 res261 = _mm256_setzero_ps();
            __m256 res270 = _mm256_setzero_ps();
            __m256 res271 = _mm256_setzero_ps();
            __m256 res280 = _mm256_setzero_ps();
            __m256 res281 = _mm256_setzero_ps();
            __m256 res290 = _mm256_setzero_ps();
            __m256 res291 = _mm256_setzero_ps();
            __m256 res300 = _mm256_setzero_ps();
            __m256 res301 = _mm256_setzero_ps();
            __m256 res310 = _mm256_setzero_ps();
            __m256 res311 = _mm256_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 64) {
                __m256 pos00 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m256 neg00 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m256 pos10 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m256 neg10 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m256 pos20 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m256 neg20 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m256 pos30 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m256 neg30 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m256 pos40 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 0);
                __m256 neg40 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 0);
                __m256 pos50 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 0);
                __m256 neg50 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 0);
                __m256 pos60 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 0);
                __m256 neg60 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 0);
                __m256 pos70 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 0);
                __m256 neg70 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 0);
                __m256 pos80 = _mm256_load_ps(X + row_index[k + 16] * M_ROW + i + 0);
                __m256 neg80 = _mm256_load_ps(X + row_index[k + 17] * M_ROW + i + 0);
                __m256 pos90 = _mm256_load_ps(X + row_index[k + 18] * M_ROW + i + 0);
                __m256 neg90 = _mm256_load_ps(X + row_index[k + 19] * M_ROW + i + 0);
                __m256 pos100 = _mm256_load_ps(X + row_index[k + 20] * M_ROW + i + 0);
                __m256 neg100 = _mm256_load_ps(X + row_index[k + 21] * M_ROW + i + 0);
                __m256 pos110 = _mm256_load_ps(X + row_index[k + 22] * M_ROW + i + 0);
                __m256 neg110 = _mm256_load_ps(X + row_index[k + 23] * M_ROW + i + 0);
                __m256 pos120 = _mm256_load_ps(X + row_index[k + 24] * M_ROW + i + 0);
                __m256 neg120 = _mm256_load_ps(X + row_index[k + 25] * M_ROW + i + 0);
                __m256 pos130 = _mm256_load_ps(X + row_index[k + 26] * M_ROW + i + 0);
                __m256 neg130 = _mm256_load_ps(X + row_index[k + 27] * M_ROW + i + 0);
                __m256 pos140 = _mm256_load_ps(X + row_index[k + 28] * M_ROW + i + 0);
                __m256 neg140 = _mm256_load_ps(X + row_index[k + 29] * M_ROW + i + 0);
                __m256 pos150 = _mm256_load_ps(X + row_index[k + 30] * M_ROW + i + 0);
                __m256 neg150 = _mm256_load_ps(X + row_index[k + 31] * M_ROW + i + 0);
                __m256 pos160 = _mm256_load_ps(X + row_index[k + 32] * M_ROW + i + 0);
                __m256 neg160 = _mm256_load_ps(X + row_index[k + 33] * M_ROW + i + 0);
                __m256 pos170 = _mm256_load_ps(X + row_index[k + 34] * M_ROW + i + 0);
                __m256 neg170 = _mm256_load_ps(X + row_index[k + 35] * M_ROW + i + 0);
                __m256 pos180 = _mm256_load_ps(X + row_index[k + 36] * M_ROW + i + 0);
                __m256 neg180 = _mm256_load_ps(X + row_index[k + 37] * M_ROW + i + 0);
                __m256 pos190 = _mm256_load_ps(X + row_index[k + 38] * M_ROW + i + 0);
                __m256 neg190 = _mm256_load_ps(X + row_index[k + 39] * M_ROW + i + 0);
                __m256 pos200 = _mm256_load_ps(X + row_index[k + 40] * M_ROW + i + 0);
                __m256 neg200 = _mm256_load_ps(X + row_index[k + 41] * M_ROW + i + 0);
                __m256 pos210 = _mm256_load_ps(X + row_index[k + 42] * M_ROW + i + 0);
                __m256 neg210 = _mm256_load_ps(X + row_index[k + 43] * M_ROW + i + 0);
                __m256 pos220 = _mm256_load_ps(X + row_index[k + 44] * M_ROW + i + 0);
                __m256 neg220 = _mm256_load_ps(X + row_index[k + 45] * M_ROW + i + 0);
                __m256 pos230 = _mm256_load_ps(X + row_index[k + 46] * M_ROW + i + 0);
                __m256 neg230 = _mm256_load_ps(X + row_index[k + 47] * M_ROW + i + 0);
                __m256 pos240 = _mm256_load_ps(X + row_index[k + 48] * M_ROW + i + 0);
                __m256 neg240 = _mm256_load_ps(X + row_index[k + 49] * M_ROW + i + 0);
                __m256 pos250 = _mm256_load_ps(X + row_index[k + 50] * M_ROW + i + 0);
                __m256 neg250 = _mm256_load_ps(X + row_index[k + 51] * M_ROW + i + 0);
                __m256 pos260 = _mm256_load_ps(X + row_index[k + 52] * M_ROW + i + 0);
                __m256 neg260 = _mm256_load_ps(X + row_index[k + 53] * M_ROW + i + 0);
                __m256 pos270 = _mm256_load_ps(X + row_index[k + 54] * M_ROW + i + 0);
                __m256 neg270 = _mm256_load_ps(X + row_index[k + 55] * M_ROW + i + 0);
                __m256 pos280 = _mm256_load_ps(X + row_index[k + 56] * M_ROW + i + 0);
                __m256 neg280 = _mm256_load_ps(X + row_index[k + 57] * M_ROW + i + 0);
                __m256 pos290 = _mm256_load_ps(X + row_index[k + 58] * M_ROW + i + 0);
                __m256 neg290 = _mm256_load_ps(X + row_index[k + 59] * M_ROW + i + 0);
                __m256 pos300 = _mm256_load_ps(X + row_index[k + 60] * M_ROW + i + 0);
                __m256 neg300 = _mm256_load_ps(X + row_index[k + 61] * M_ROW + i + 0);
                __m256 pos310 = _mm256_load_ps(X + row_index[k + 62] * M_ROW + i + 0);
                __m256 neg310 = _mm256_load_ps(X + row_index[k + 63] * M_ROW + i + 0);
                __m256 pos01 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 8);
                __m256 neg01 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 8);
                __m256 pos11 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 8);
                __m256 neg11 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 8);
                __m256 pos21 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 8);
                __m256 neg21 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 8);
                __m256 pos31 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 8);
                __m256 neg31 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 8);
                __m256 pos41 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 8);
                __m256 neg41 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 8);
                __m256 pos51 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 8);
                __m256 neg51 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 8);
                __m256 pos61 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 8);
                __m256 neg61 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 8);
                __m256 pos71 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 8);
                __m256 neg71 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 8);
                __m256 pos81 = _mm256_load_ps(X + row_index[k + 16] * M_ROW + i + 8);
                __m256 neg81 = _mm256_load_ps(X + row_index[k + 17] * M_ROW + i + 8);
                __m256 pos91 = _mm256_load_ps(X + row_index[k + 18] * M_ROW + i + 8);
                __m256 neg91 = _mm256_load_ps(X + row_index[k + 19] * M_ROW + i + 8);
                __m256 pos101 = _mm256_load_ps(X + row_index[k + 20] * M_ROW + i + 8);
                __m256 neg101 = _mm256_load_ps(X + row_index[k + 21] * M_ROW + i + 8);
                __m256 pos111 = _mm256_load_ps(X + row_index[k + 22] * M_ROW + i + 8);
                __m256 neg111 = _mm256_load_ps(X + row_index[k + 23] * M_ROW + i + 8);
                __m256 pos121 = _mm256_load_ps(X + row_index[k + 24] * M_ROW + i + 8);
                __m256 neg121 = _mm256_load_ps(X + row_index[k + 25] * M_ROW + i + 8);
                __m256 pos131 = _mm256_load_ps(X + row_index[k + 26] * M_ROW + i + 8);
                __m256 neg131 = _mm256_load_ps(X + row_index[k + 27] * M_ROW + i + 8);
                __m256 pos141 = _mm256_load_ps(X + row_index[k + 28] * M_ROW + i + 8);
                __m256 neg141 = _mm256_load_ps(X + row_index[k + 29] * M_ROW + i + 8);
                __m256 pos151 = _mm256_load_ps(X + row_index[k + 30] * M_ROW + i + 8);
                __m256 neg151 = _mm256_load_ps(X + row_index[k + 31] * M_ROW + i + 8);
                __m256 pos161 = _mm256_load_ps(X + row_index[k + 32] * M_ROW + i + 8);
                __m256 neg161 = _mm256_load_ps(X + row_index[k + 33] * M_ROW + i + 8);
                __m256 pos171 = _mm256_load_ps(X + row_index[k + 34] * M_ROW + i + 8);
                __m256 neg171 = _mm256_load_ps(X + row_index[k + 35] * M_ROW + i + 8);
                __m256 pos181 = _mm256_load_ps(X + row_index[k + 36] * M_ROW + i + 8);
                __m256 neg181 = _mm256_load_ps(X + row_index[k + 37] * M_ROW + i + 8);
                __m256 pos191 = _mm256_load_ps(X + row_index[k + 38] * M_ROW + i + 8);
                __m256 neg191 = _mm256_load_ps(X + row_index[k + 39] * M_ROW + i + 8);
                __m256 pos201 = _mm256_load_ps(X + row_index[k + 40] * M_ROW + i + 8);
                __m256 neg201 = _mm256_load_ps(X + row_index[k + 41] * M_ROW + i + 8);
                __m256 pos211 = _mm256_load_ps(X + row_index[k + 42] * M_ROW + i + 8);
                __m256 neg211 = _mm256_load_ps(X + row_index[k + 43] * M_ROW + i + 8);
                __m256 pos221 = _mm256_load_ps(X + row_index[k + 44] * M_ROW + i + 8);
                __m256 neg221 = _mm256_load_ps(X + row_index[k + 45] * M_ROW + i + 8);
                __m256 pos231 = _mm256_load_ps(X + row_index[k + 46] * M_ROW + i + 8);
                __m256 neg231 = _mm256_load_ps(X + row_index[k + 47] * M_ROW + i + 8);
                __m256 pos241 = _mm256_load_ps(X + row_index[k + 48] * M_ROW + i + 8);
                __m256 neg241 = _mm256_load_ps(X + row_index[k + 49] * M_ROW + i + 8);
                __m256 pos251 = _mm256_load_ps(X + row_index[k + 50] * M_ROW + i + 8);
                __m256 neg251 = _mm256_load_ps(X + row_index[k + 51] * M_ROW + i + 8);
                __m256 pos261 = _mm256_load_ps(X + row_index[k + 52] * M_ROW + i + 8);
                __m256 neg261 = _mm256_load_ps(X + row_index[k + 53] * M_ROW + i + 8);
                __m256 pos271 = _mm256_load_ps(X + row_index[k + 54] * M_ROW + i + 8);
                __m256 neg271 = _mm256_load_ps(X + row_index[k + 55] * M_ROW + i + 8);
                __m256 pos281 = _mm256_load_ps(X + row_index[k + 56] * M_ROW + i + 8);
                __m256 neg281 = _mm256_load_ps(X + row_index[k + 57] * M_ROW + i + 8);
                __m256 pos291 = _mm256_load_ps(X + row_index[k + 58] * M_ROW + i + 8);
                __m256 neg291 = _mm256_load_ps(X + row_index[k + 59] * M_ROW + i + 8);
                __m256 pos301 = _mm256_load_ps(X + row_index[k + 60] * M_ROW + i + 8);
                __m256 neg301 = _mm256_load_ps(X + row_index[k + 61] * M_ROW + i + 8);
                __m256 pos311 = _mm256_load_ps(X + row_index[k + 62] * M_ROW + i + 8);
                __m256 neg311 = _mm256_load_ps(X + row_index[k + 63] * M_ROW + i + 8);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res20 = _mm256_add_ps(res20, _mm256_sub_ps(pos20, neg20));
                res30 = _mm256_add_ps(res30, _mm256_sub_ps(pos30, neg30));
                res40 = _mm256_add_ps(res40, _mm256_sub_ps(pos40, neg40));
                res50 = _mm256_add_ps(res50, _mm256_sub_ps(pos50, neg50));
                res60 = _mm256_add_ps(res60, _mm256_sub_ps(pos60, neg60));
                res70 = _mm256_add_ps(res70, _mm256_sub_ps(pos70, neg70));
                res80 = _mm256_add_ps(res80, _mm256_sub_ps(pos80, neg80));
                res90 = _mm256_add_ps(res90, _mm256_sub_ps(pos90, neg90));
                res100 = _mm256_add_ps(res100, _mm256_sub_ps(pos100, neg100));
                res110 = _mm256_add_ps(res110, _mm256_sub_ps(pos110, neg110));
                res120 = _mm256_add_ps(res120, _mm256_sub_ps(pos120, neg120));
                res130 = _mm256_add_ps(res130, _mm256_sub_ps(pos130, neg130));
                res140 = _mm256_add_ps(res140, _mm256_sub_ps(pos140, neg140));
                res150 = _mm256_add_ps(res150, _mm256_sub_ps(pos150, neg150));
                res160 = _mm256_add_ps(res160, _mm256_sub_ps(pos160, neg160));
                res170 = _mm256_add_ps(res170, _mm256_sub_ps(pos170, neg170));
                res180 = _mm256_add_ps(res180, _mm256_sub_ps(pos180, neg180));
                res190 = _mm256_add_ps(res190, _mm256_sub_ps(pos190, neg190));
                res200 = _mm256_add_ps(res200, _mm256_sub_ps(pos200, neg200));
                res210 = _mm256_add_ps(res210, _mm256_sub_ps(pos210, neg210));
                res220 = _mm256_add_ps(res220, _mm256_sub_ps(pos220, neg220));
                res230 = _mm256_add_ps(res230, _mm256_sub_ps(pos230, neg230));
                res240 = _mm256_add_ps(res240, _mm256_sub_ps(pos240, neg240));
                res250 = _mm256_add_ps(res250, _mm256_sub_ps(pos250, neg250));
                res260 = _mm256_add_ps(res260, _mm256_sub_ps(pos260, neg260));
                res270 = _mm256_add_ps(res270, _mm256_sub_ps(pos270, neg270));
                res280 = _mm256_add_ps(res280, _mm256_sub_ps(pos280, neg280));
                res290 = _mm256_add_ps(res290, _mm256_sub_ps(pos290, neg290));
                res300 = _mm256_add_ps(res300, _mm256_sub_ps(pos300, neg300));
                res310 = _mm256_add_ps(res310, _mm256_sub_ps(pos310, neg310));
                res01 = _mm256_add_ps(res01, _mm256_sub_ps(pos01, neg01));
                res11 = _mm256_add_ps(res11, _mm256_sub_ps(pos11, neg11));
                res21 = _mm256_add_ps(res21, _mm256_sub_ps(pos21, neg21));
                res31 = _mm256_add_ps(res31, _mm256_sub_ps(pos31, neg31));
                res41 = _mm256_add_ps(res41, _mm256_sub_ps(pos41, neg41));
                res51 = _mm256_add_ps(res51, _mm256_sub_ps(pos51, neg51));
                res61 = _mm256_add_ps(res61, _mm256_sub_ps(pos61, neg61));
                res71 = _mm256_add_ps(res71, _mm256_sub_ps(pos71, neg71));
                res81 = _mm256_add_ps(res81, _mm256_sub_ps(pos81, neg81));
                res91 = _mm256_add_ps(res91, _mm256_sub_ps(pos91, neg91));
                res101 = _mm256_add_ps(res101, _mm256_sub_ps(pos101, neg101));
                res111 = _mm256_add_ps(res111, _mm256_sub_ps(pos111, neg111));
                res121 = _mm256_add_ps(res121, _mm256_sub_ps(pos121, neg121));
                res131 = _mm256_add_ps(res131, _mm256_sub_ps(pos131, neg131));
                res141 = _mm256_add_ps(res141, _mm256_sub_ps(pos141, neg141));
                res151 = _mm256_add_ps(res151, _mm256_sub_ps(pos151, neg151));
                res161 = _mm256_add_ps(res161, _mm256_sub_ps(pos161, neg161));
                res171 = _mm256_add_ps(res171, _mm256_sub_ps(pos171, neg171));
                res181 = _mm256_add_ps(res181, _mm256_sub_ps(pos181, neg181));
                res191 = _mm256_add_ps(res191, _mm256_sub_ps(pos191, neg191));
                res201 = _mm256_add_ps(res201, _mm256_sub_ps(pos201, neg201));
                res211 = _mm256_add_ps(res211, _mm256_sub_ps(pos211, neg211));
                res221 = _mm256_add_ps(res221, _mm256_sub_ps(pos221, neg221));
                res231 = _mm256_add_ps(res231, _mm256_sub_ps(pos231, neg231));
                res241 = _mm256_add_ps(res241, _mm256_sub_ps(pos241, neg241));
                res251 = _mm256_add_ps(res251, _mm256_sub_ps(pos251, neg251));
                res261 = _mm256_add_ps(res261, _mm256_sub_ps(pos261, neg261));
                res271 = _mm256_add_ps(res271, _mm256_sub_ps(pos271, neg271));
                res281 = _mm256_add_ps(res281, _mm256_sub_ps(pos281, neg281));
                res291 = _mm256_add_ps(res291, _mm256_sub_ps(pos291, neg291));
                res301 = _mm256_add_ps(res301, _mm256_sub_ps(pos301, neg301));
                res311 = _mm256_add_ps(res311, _mm256_sub_ps(pos311, neg311));
            }
            for (int k = groupData[1]; k < groupData[2]; k++) {
                res00 = _mm256_add_ps(res00, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res01 = _mm256_add_ps(res01, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[2]; k < groupData[3]; k++) {
                res00 = _mm256_sub_ps(res00, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res01 = _mm256_sub_ps(res01, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[3]; k < groupData[4]; k++) {
                res10 = _mm256_add_ps(res10, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res11 = _mm256_add_ps(res11, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[4]; k < groupData[5]; k++) {
                res10 = _mm256_sub_ps(res10, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res11 = _mm256_sub_ps(res11, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[5]; k < groupData[6]; k++) {
                res20 = _mm256_add_ps(res20, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res21 = _mm256_add_ps(res21, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[6]; k < groupData[7]; k++) {
                res20 = _mm256_sub_ps(res20, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res21 = _mm256_sub_ps(res21, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[7]; k < groupData[8]; k++) {
                res30 = _mm256_add_ps(res30, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res31 = _mm256_add_ps(res31, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[8]; k < groupData[9]; k++) {
                res30 = _mm256_sub_ps(res30, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res31 = _mm256_sub_ps(res31, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[9]; k < groupData[10]; k++) {
                res40 = _mm256_add_ps(res40, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res41 = _mm256_add_ps(res41, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[10]; k < groupData[11]; k++) {
                res40 = _mm256_sub_ps(res40, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res41 = _mm256_sub_ps(res41, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[11]; k < groupData[12]; k++) {
                res50 = _mm256_add_ps(res50, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res51 = _mm256_add_ps(res51, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[12]; k < groupData[13]; k++) {
                res50 = _mm256_sub_ps(res50, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res51 = _mm256_sub_ps(res51, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[13]; k < groupData[14]; k++) {
                res60 = _mm256_add_ps(res60, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res61 = _mm256_add_ps(res61, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[14]; k < groupData[15]; k++) {
                res60 = _mm256_sub_ps(res60, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res61 = _mm256_sub_ps(res61, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[15]; k < groupData[16]; k++) {
                res70 = _mm256_add_ps(res70, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res71 = _mm256_add_ps(res71, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[16]; k < groupData[17]; k++) {
                res70 = _mm256_sub_ps(res70, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res71 = _mm256_sub_ps(res71, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[17]; k < groupData[18]; k++) {
                res80 = _mm256_add_ps(res80, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res81 = _mm256_add_ps(res81, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[18]; k < groupData[19]; k++) {
                res80 = _mm256_sub_ps(res80, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res81 = _mm256_sub_ps(res81, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[19]; k < groupData[20]; k++) {
                res90 = _mm256_add_ps(res90, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res91 = _mm256_add_ps(res91, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[20]; k < groupData[21]; k++) {
                res90 = _mm256_sub_ps(res90, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res91 = _mm256_sub_ps(res91, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[21]; k < groupData[22]; k++) {
                res100 = _mm256_add_ps(res100, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res101 = _mm256_add_ps(res101, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[22]; k < groupData[23]; k++) {
                res100 = _mm256_sub_ps(res100, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res101 = _mm256_sub_ps(res101, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[23]; k < groupData[24]; k++) {
                res110 = _mm256_add_ps(res110, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res111 = _mm256_add_ps(res111, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[24]; k < groupData[25]; k++) {
                res110 = _mm256_sub_ps(res110, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res111 = _mm256_sub_ps(res111, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[25]; k < groupData[26]; k++) {
                res120 = _mm256_add_ps(res120, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res121 = _mm256_add_ps(res121, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[26]; k < groupData[27]; k++) {
                res120 = _mm256_sub_ps(res120, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res121 = _mm256_sub_ps(res121, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[27]; k < groupData[28]; k++) {
                res130 = _mm256_add_ps(res130, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res131 = _mm256_add_ps(res131, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[28]; k < groupData[29]; k++) {
                res130 = _mm256_sub_ps(res130, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res131 = _mm256_sub_ps(res131, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[29]; k < groupData[30]; k++) {
                res140 = _mm256_add_ps(res140, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res141 = _mm256_add_ps(res141, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[30]; k < groupData[31]; k++) {
                res140 = _mm256_sub_ps(res140, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res141 = _mm256_sub_ps(res141, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[31]; k < groupData[32]; k++) {
                res150 = _mm256_add_ps(res150, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res151 = _mm256_add_ps(res151, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[32]; k < groupData[33]; k++) {
                res150 = _mm256_sub_ps(res150, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res151 = _mm256_sub_ps(res151, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[33]; k < groupData[34]; k++) {
                res160 = _mm256_add_ps(res160, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res161 = _mm256_add_ps(res161, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[34]; k < groupData[35]; k++) {
                res160 = _mm256_sub_ps(res160, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res161 = _mm256_sub_ps(res161, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[35]; k < groupData[36]; k++) {
                res170 = _mm256_add_ps(res170, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res171 = _mm256_add_ps(res171, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[36]; k < groupData[37]; k++) {
                res170 = _mm256_sub_ps(res170, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res171 = _mm256_sub_ps(res171, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[37]; k < groupData[38]; k++) {
                res180 = _mm256_add_ps(res180, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res181 = _mm256_add_ps(res181, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[38]; k < groupData[39]; k++) {
                res180 = _mm256_sub_ps(res180, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res181 = _mm256_sub_ps(res181, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[39]; k < groupData[40]; k++) {
                res190 = _mm256_add_ps(res190, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res191 = _mm256_add_ps(res191, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[40]; k < groupData[41]; k++) {
                res190 = _mm256_sub_ps(res190, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res191 = _mm256_sub_ps(res191, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[41]; k < groupData[42]; k++) {
                res200 = _mm256_add_ps(res200, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res201 = _mm256_add_ps(res201, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[42]; k < groupData[43]; k++) {
                res200 = _mm256_sub_ps(res200, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res201 = _mm256_sub_ps(res201, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[43]; k < groupData[44]; k++) {
                res210 = _mm256_add_ps(res210, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res211 = _mm256_add_ps(res211, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[44]; k < groupData[45]; k++) {
                res210 = _mm256_sub_ps(res210, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res211 = _mm256_sub_ps(res211, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[45]; k < groupData[46]; k++) {
                res220 = _mm256_add_ps(res220, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res221 = _mm256_add_ps(res221, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[46]; k < groupData[47]; k++) {
                res220 = _mm256_sub_ps(res220, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res221 = _mm256_sub_ps(res221, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[47]; k < groupData[48]; k++) {
                res230 = _mm256_add_ps(res230, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res231 = _mm256_add_ps(res231, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[48]; k < groupData[49]; k++) {
                res230 = _mm256_sub_ps(res230, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res231 = _mm256_sub_ps(res231, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[49]; k < groupData[50]; k++) {
                res240 = _mm256_add_ps(res240, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res241 = _mm256_add_ps(res241, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[50]; k < groupData[51]; k++) {
                res240 = _mm256_sub_ps(res240, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res241 = _mm256_sub_ps(res241, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[51]; k < groupData[52]; k++) {
                res250 = _mm256_add_ps(res250, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res251 = _mm256_add_ps(res251, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[52]; k < groupData[53]; k++) {
                res250 = _mm256_sub_ps(res250, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res251 = _mm256_sub_ps(res251, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[53]; k < groupData[54]; k++) {
                res260 = _mm256_add_ps(res260, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res261 = _mm256_add_ps(res261, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[54]; k < groupData[55]; k++) {
                res260 = _mm256_sub_ps(res260, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res261 = _mm256_sub_ps(res261, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[55]; k < groupData[56]; k++) {
                res270 = _mm256_add_ps(res270, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res271 = _mm256_add_ps(res271, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[56]; k < groupData[57]; k++) {
                res270 = _mm256_sub_ps(res270, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res271 = _mm256_sub_ps(res271, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[57]; k < groupData[58]; k++) {
                res280 = _mm256_add_ps(res280, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res281 = _mm256_add_ps(res281, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[58]; k < groupData[59]; k++) {
                res280 = _mm256_sub_ps(res280, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res281 = _mm256_sub_ps(res281, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[59]; k < groupData[60]; k++) {
                res290 = _mm256_add_ps(res290, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res291 = _mm256_add_ps(res291, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[60]; k < groupData[61]; k++) {
                res290 = _mm256_sub_ps(res290, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res291 = _mm256_sub_ps(res291, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[61]; k < groupData[62]; k++) {
                res300 = _mm256_add_ps(res300, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res301 = _mm256_add_ps(res301, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[62]; k < groupData[63]; k++) {
                res300 = _mm256_sub_ps(res300, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res301 = _mm256_sub_ps(res301, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[63]; k < groupData[64]; k++) {
                res310 = _mm256_add_ps(res310, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res311 = _mm256_add_ps(res311, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            for (int k = groupData[64]; k < groupData[65]; k++) {
                res310 = _mm256_sub_ps(res310, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res311 = _mm256_sub_ps(res311, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
            }
            _mm256_store_ps(result + (j * 32 + 0) * M_ROW + i + 0, res00);
            _mm256_store_ps(result + (j * 32 + 1) * M_ROW + i + 0, res10);
            _mm256_store_ps(result + (j * 32 + 2) * M_ROW + i + 0, res20);
            _mm256_store_ps(result + (j * 32 + 3) * M_ROW + i + 0, res30);
            _mm256_store_ps(result + (j * 32 + 4) * M_ROW + i + 0, res40);
            _mm256_store_ps(result + (j * 32 + 5) * M_ROW + i + 0, res50);
            _mm256_store_ps(result + (j * 32 + 6) * M_ROW + i + 0, res60);
            _mm256_store_ps(result + (j * 32 + 7) * M_ROW + i + 0, res70);
            _mm256_store_ps(result + (j * 32 + 8) * M_ROW + i + 0, res80);
            _mm256_store_ps(result + (j * 32 + 9) * M_ROW + i + 0, res90);
            _mm256_store_ps(result + (j * 32 + 10) * M_ROW + i + 0, res100);
            _mm256_store_ps(result + (j * 32 + 11) * M_ROW + i + 0, res110);
            _mm256_store_ps(result + (j * 32 + 12) * M_ROW + i + 0, res120);
            _mm256_store_ps(result + (j * 32 + 13) * M_ROW + i + 0, res130);
            _mm256_store_ps(result + (j * 32 + 14) * M_ROW + i + 0, res140);
            _mm256_store_ps(result + (j * 32 + 15) * M_ROW + i + 0, res150);
            _mm256_store_ps(result + (j * 32 + 16) * M_ROW + i + 0, res160);
            _mm256_store_ps(result + (j * 32 + 17) * M_ROW + i + 0, res170);
            _mm256_store_ps(result + (j * 32 + 18) * M_ROW + i + 0, res180);
            _mm256_store_ps(result + (j * 32 + 19) * M_ROW + i + 0, res190);
            _mm256_store_ps(result + (j * 32 + 20) * M_ROW + i + 0, res200);
            _mm256_store_ps(result + (j * 32 + 21) * M_ROW + i + 0, res210);
            _mm256_store_ps(result + (j * 32 + 22) * M_ROW + i + 0, res220);
            _mm256_store_ps(result + (j * 32 + 23) * M_ROW + i + 0, res230);
            _mm256_store_ps(result + (j * 32 + 24) * M_ROW + i + 0, res240);
            _mm256_store_ps(result + (j * 32 + 25) * M_ROW + i + 0, res250);
            _mm256_store_ps(result + (j * 32 + 26) * M_ROW + i + 0, res260);
            _mm256_store_ps(result + (j * 32 + 27) * M_ROW + i + 0, res270);
            _mm256_store_ps(result + (j * 32 + 28) * M_ROW + i + 0, res280);
            _mm256_store_ps(result + (j * 32 + 29) * M_ROW + i + 0, res290);
            _mm256_store_ps(result + (j * 32 + 30) * M_ROW + i + 0, res300);
            _mm256_store_ps(result + (j * 32 + 31) * M_ROW + i + 0, res310);
            _mm256_store_ps(result + (j * 32 + 0) * M_ROW + i + 8, res01);
            _mm256_store_ps(result + (j * 32 + 1) * M_ROW + i + 8, res11);
            _mm256_store_ps(result + (j * 32 + 2) * M_ROW + i + 8, res21);
            _mm256_store_ps(result + (j * 32 + 3) * M_ROW + i + 8, res31);
            _mm256_store_ps(result + (j * 32 + 4) * M_ROW + i + 8, res41);
            _mm256_store_ps(result + (j * 32 + 5) * M_ROW + i + 8, res51);
            _mm256_store_ps(result + (j * 32 + 6) * M_ROW + i + 8, res61);
            _mm256_store_ps(result + (j * 32 + 7) * M_ROW + i + 8, res71);
            _mm256_store_ps(result + (j * 32 + 8) * M_ROW + i + 8, res81);
            _mm256_store_ps(result + (j * 32 + 9) * M_ROW + i + 8, res91);
            _mm256_store_ps(result + (j * 32 + 10) * M_ROW + i + 8, res101);
            _mm256_store_ps(result + (j * 32 + 11) * M_ROW + i + 8, res111);
            _mm256_store_ps(result + (j * 32 + 12) * M_ROW + i + 8, res121);
            _mm256_store_ps(result + (j * 32 + 13) * M_ROW + i + 8, res131);
            _mm256_store_ps(result + (j * 32 + 14) * M_ROW + i + 8, res141);
            _mm256_store_ps(result + (j * 32 + 15) * M_ROW + i + 8, res151);
            _mm256_store_ps(result + (j * 32 + 16) * M_ROW + i + 8, res161);
            _mm256_store_ps(result + (j * 32 + 17) * M_ROW + i + 8, res171);
            _mm256_store_ps(result + (j * 32 + 18) * M_ROW + i + 8, res181);
            _mm256_store_ps(result + (j * 32 + 19) * M_ROW + i + 8, res191);
            _mm256_store_ps(result + (j * 32 + 20) * M_ROW + i + 8, res201);
            _mm256_store_ps(result + (j * 32 + 21) * M_ROW + i + 8, res211);
            _mm256_store_ps(result + (j * 32 + 22) * M_ROW + i + 8, res221);
            _mm256_store_ps(result + (j * 32 + 23) * M_ROW + i + 8, res231);
            _mm256_store_ps(result + (j * 32 + 24) * M_ROW + i + 8, res241);
            _mm256_store_ps(result + (j * 32 + 25) * M_ROW + i + 8, res251);
            _mm256_store_ps(result + (j * 32 + 26) * M_ROW + i + 8, res261);
            _mm256_store_ps(result + (j * 32 + 27) * M_ROW + i + 8, res271);
            _mm256_store_ps(result + (j * 32 + 28) * M_ROW + i + 8, res281);
            _mm256_store_ps(result + (j * 32 + 29) * M_ROW + i + 8, res291);
            _mm256_store_ps(result + (j * 32 + 30) * M_ROW + i + 8, res301);
            _mm256_store_ps(result + (j * 32 + 31) * M_ROW + i + 8, res311);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Merged_GroupMin_32xG4_AVX2_OpenMP(const float* X, const int32_t* metadata, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 4; j++) {
        const int* groupData  = &metadata[j * 10];
        for (int i = 0; i < M_ROW; i += 32) {
            __m256 res00 = _mm256_setzero_ps();
            __m256 res01 = _mm256_setzero_ps();
            __m256 res02 = _mm256_setzero_ps();
            __m256 res03 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res11 = _mm256_setzero_ps();
            __m256 res12 = _mm256_setzero_ps();
            __m256 res13 = _mm256_setzero_ps();
            __m256 res20 = _mm256_setzero_ps();
            __m256 res21 = _mm256_setzero_ps();
            __m256 res22 = _mm256_setzero_ps();
            __m256 res23 = _mm256_setzero_ps();
            __m256 res30 = _mm256_setzero_ps();
            __m256 res31 = _mm256_setzero_ps();
            __m256 res32 = _mm256_setzero_ps();
            __m256 res33 = _mm256_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 8) {
                __m256 pos00 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m256 neg00 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m256 pos10 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m256 neg10 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m256 pos20 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m256 neg20 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m256 pos30 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m256 neg30 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m256 pos01 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 8);
                __m256 neg01 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 8);
                __m256 pos11 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 8);
                __m256 neg11 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 8);
                __m256 pos21 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 8);
                __m256 neg21 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 8);
                __m256 pos31 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 8);
                __m256 neg31 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 8);
                __m256 pos02 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 16);
                __m256 neg02 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 16);
                __m256 pos12 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 16);
                __m256 neg12 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 16);
                __m256 pos22 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 16);
                __m256 neg22 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 16);
                __m256 pos32 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 16);
                __m256 neg32 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 16);
                __m256 pos03 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 24);
                __m256 neg03 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 24);
                __m256 pos13 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 24);
                __m256 neg13 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 24);
                __m256 pos23 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 24);
                __m256 neg23 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 24);
                __m256 pos33 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 24);
                __m256 neg33 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 24);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res20 = _mm256_add_ps(res20, _mm256_sub_ps(pos20, neg20));
                res30 = _mm256_add_ps(res30, _mm256_sub_ps(pos30, neg30));
                res01 = _mm256_add_ps(res01, _mm256_sub_ps(pos01, neg01));
                res11 = _mm256_add_ps(res11, _mm256_sub_ps(pos11, neg11));
                res21 = _mm256_add_ps(res21, _mm256_sub_ps(pos21, neg21));
                res31 = _mm256_add_ps(res31, _mm256_sub_ps(pos31, neg31));
                res02 = _mm256_add_ps(res02, _mm256_sub_ps(pos02, neg02));
                res12 = _mm256_add_ps(res12, _mm256_sub_ps(pos12, neg12));
                res22 = _mm256_add_ps(res22, _mm256_sub_ps(pos22, neg22));
                res32 = _mm256_add_ps(res32, _mm256_sub_ps(pos32, neg32));
                res03 = _mm256_add_ps(res03, _mm256_sub_ps(pos03, neg03));
                res13 = _mm256_add_ps(res13, _mm256_sub_ps(pos13, neg13));
                res23 = _mm256_add_ps(res23, _mm256_sub_ps(pos23, neg23));
                res33 = _mm256_add_ps(res33, _mm256_sub_ps(pos33, neg33));
            }
            for (int k = groupData[1]; k < groupData[2]; k++) {
                res00 = _mm256_add_ps(res00, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res01 = _mm256_add_ps(res01, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res02 = _mm256_add_ps(res02, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res03 = _mm256_add_ps(res03, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[2]; k < groupData[3]; k++) {
                res00 = _mm256_sub_ps(res00, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res01 = _mm256_sub_ps(res01, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res02 = _mm256_sub_ps(res02, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res03 = _mm256_sub_ps(res03, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[3]; k < groupData[4]; k++) {
                res10 = _mm256_add_ps(res10, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res11 = _mm256_add_ps(res11, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res12 = _mm256_add_ps(res12, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res13 = _mm256_add_ps(res13, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[4]; k < groupData[5]; k++) {
                res10 = _mm256_sub_ps(res10, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res11 = _mm256_sub_ps(res11, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res12 = _mm256_sub_ps(res12, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res13 = _mm256_sub_ps(res13, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[5]; k < groupData[6]; k++) {
                res20 = _mm256_add_ps(res20, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res21 = _mm256_add_ps(res21, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res22 = _mm256_add_ps(res22, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res23 = _mm256_add_ps(res23, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[6]; k < groupData[7]; k++) {
                res20 = _mm256_sub_ps(res20, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res21 = _mm256_sub_ps(res21, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res22 = _mm256_sub_ps(res22, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res23 = _mm256_sub_ps(res23, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[7]; k < groupData[8]; k++) {
                res30 = _mm256_add_ps(res30, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res31 = _mm256_add_ps(res31, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res32 = _mm256_add_ps(res32, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res33 = _mm256_add_ps(res33, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[8]; k < groupData[9]; k++) {
                res30 = _mm256_sub_ps(res30, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res31 = _mm256_sub_ps(res31, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res32 = _mm256_sub_ps(res32, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res33 = _mm256_sub_ps(res33, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            _mm256_store_ps(result + (j * 4 + 0) * M_ROW + i + 0, res00);
            _mm256_store_ps(result + (j * 4 + 1) * M_ROW + i + 0, res10);
            _mm256_store_ps(result + (j * 4 + 2) * M_ROW + i + 0, res20);
            _mm256_store_ps(result + (j * 4 + 3) * M_ROW + i + 0, res30);
            _mm256_store_ps(result + (j * 4 + 0) * M_ROW + i + 8, res01);
            _mm256_store_ps(result + (j * 4 + 1) * M_ROW + i + 8, res11);
            _mm256_store_ps(result + (j * 4 + 2) * M_ROW + i + 8, res21);
            _mm256_store_ps(result + (j * 4 + 3) * M_ROW + i + 8, res31);
            _mm256_store_ps(result + (j * 4 + 0) * M_ROW + i + 16, res02);
            _mm256_store_ps(result + (j * 4 + 1) * M_ROW + i + 16, res12);
            _mm256_store_ps(result + (j * 4 + 2) * M_ROW + i + 16, res22);
            _mm256_store_ps(result + (j * 4 + 3) * M_ROW + i + 16, res32);
            _mm256_store_ps(result + (j * 4 + 0) * M_ROW + i + 24, res03);
            _mm256_store_ps(result + (j * 4 + 1) * M_ROW + i + 24, res13);
            _mm256_store_ps(result + (j * 4 + 2) * M_ROW + i + 24, res23);
            _mm256_store_ps(result + (j * 4 + 3) * M_ROW + i + 24, res33);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Merged_GroupMin_32xG8_AVX2_OpenMP(const float* X, const int32_t* metadata, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 8; j++) {
        const int* groupData  = &metadata[j * 18];
        for (int i = 0; i < M_ROW; i += 32) {
            __m256 res00 = _mm256_setzero_ps();
            __m256 res01 = _mm256_setzero_ps();
            __m256 res02 = _mm256_setzero_ps();
            __m256 res03 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res11 = _mm256_setzero_ps();
            __m256 res12 = _mm256_setzero_ps();
            __m256 res13 = _mm256_setzero_ps();
            __m256 res20 = _mm256_setzero_ps();
            __m256 res21 = _mm256_setzero_ps();
            __m256 res22 = _mm256_setzero_ps();
            __m256 res23 = _mm256_setzero_ps();
            __m256 res30 = _mm256_setzero_ps();
            __m256 res31 = _mm256_setzero_ps();
            __m256 res32 = _mm256_setzero_ps();
            __m256 res33 = _mm256_setzero_ps();
            __m256 res40 = _mm256_setzero_ps();
            __m256 res41 = _mm256_setzero_ps();
            __m256 res42 = _mm256_setzero_ps();
            __m256 res43 = _mm256_setzero_ps();
            __m256 res50 = _mm256_setzero_ps();
            __m256 res51 = _mm256_setzero_ps();
            __m256 res52 = _mm256_setzero_ps();
            __m256 res53 = _mm256_setzero_ps();
            __m256 res60 = _mm256_setzero_ps();
            __m256 res61 = _mm256_setzero_ps();
            __m256 res62 = _mm256_setzero_ps();
            __m256 res63 = _mm256_setzero_ps();
            __m256 res70 = _mm256_setzero_ps();
            __m256 res71 = _mm256_setzero_ps();
            __m256 res72 = _mm256_setzero_ps();
            __m256 res73 = _mm256_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 16) {
                __m256 pos00 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m256 neg00 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m256 pos10 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m256 neg10 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m256 pos20 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m256 neg20 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m256 pos30 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m256 neg30 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m256 pos40 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 0);
                __m256 neg40 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 0);
                __m256 pos50 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 0);
                __m256 neg50 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 0);
                __m256 pos60 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 0);
                __m256 neg60 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 0);
                __m256 pos70 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 0);
                __m256 neg70 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 0);
                __m256 pos01 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 8);
                __m256 neg01 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 8);
                __m256 pos11 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 8);
                __m256 neg11 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 8);
                __m256 pos21 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 8);
                __m256 neg21 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 8);
                __m256 pos31 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 8);
                __m256 neg31 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 8);
                __m256 pos41 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 8);
                __m256 neg41 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 8);
                __m256 pos51 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 8);
                __m256 neg51 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 8);
                __m256 pos61 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 8);
                __m256 neg61 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 8);
                __m256 pos71 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 8);
                __m256 neg71 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 8);
                __m256 pos02 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 16);
                __m256 neg02 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 16);
                __m256 pos12 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 16);
                __m256 neg12 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 16);
                __m256 pos22 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 16);
                __m256 neg22 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 16);
                __m256 pos32 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 16);
                __m256 neg32 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 16);
                __m256 pos42 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 16);
                __m256 neg42 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 16);
                __m256 pos52 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 16);
                __m256 neg52 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 16);
                __m256 pos62 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 16);
                __m256 neg62 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 16);
                __m256 pos72 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 16);
                __m256 neg72 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 16);
                __m256 pos03 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 24);
                __m256 neg03 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 24);
                __m256 pos13 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 24);
                __m256 neg13 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 24);
                __m256 pos23 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 24);
                __m256 neg23 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 24);
                __m256 pos33 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 24);
                __m256 neg33 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 24);
                __m256 pos43 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 24);
                __m256 neg43 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 24);
                __m256 pos53 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 24);
                __m256 neg53 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 24);
                __m256 pos63 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 24);
                __m256 neg63 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 24);
                __m256 pos73 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 24);
                __m256 neg73 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 24);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res20 = _mm256_add_ps(res20, _mm256_sub_ps(pos20, neg20));
                res30 = _mm256_add_ps(res30, _mm256_sub_ps(pos30, neg30));
                res40 = _mm256_add_ps(res40, _mm256_sub_ps(pos40, neg40));
                res50 = _mm256_add_ps(res50, _mm256_sub_ps(pos50, neg50));
                res60 = _mm256_add_ps(res60, _mm256_sub_ps(pos60, neg60));
                res70 = _mm256_add_ps(res70, _mm256_sub_ps(pos70, neg70));
                res01 = _mm256_add_ps(res01, _mm256_sub_ps(pos01, neg01));
                res11 = _mm256_add_ps(res11, _mm256_sub_ps(pos11, neg11));
                res21 = _mm256_add_ps(res21, _mm256_sub_ps(pos21, neg21));
                res31 = _mm256_add_ps(res31, _mm256_sub_ps(pos31, neg31));
                res41 = _mm256_add_ps(res41, _mm256_sub_ps(pos41, neg41));
                res51 = _mm256_add_ps(res51, _mm256_sub_ps(pos51, neg51));
                res61 = _mm256_add_ps(res61, _mm256_sub_ps(pos61, neg61));
                res71 = _mm256_add_ps(res71, _mm256_sub_ps(pos71, neg71));
                res02 = _mm256_add_ps(res02, _mm256_sub_ps(pos02, neg02));
                res12 = _mm256_add_ps(res12, _mm256_sub_ps(pos12, neg12));
                res22 = _mm256_add_ps(res22, _mm256_sub_ps(pos22, neg22));
                res32 = _mm256_add_ps(res32, _mm256_sub_ps(pos32, neg32));
                res42 = _mm256_add_ps(res42, _mm256_sub_ps(pos42, neg42));
                res52 = _mm256_add_ps(res52, _mm256_sub_ps(pos52, neg52));
                res62 = _mm256_add_ps(res62, _mm256_sub_ps(pos62, neg62));
                res72 = _mm256_add_ps(res72, _mm256_sub_ps(pos72, neg72));
                res03 = _mm256_add_ps(res03, _mm256_sub_ps(pos03, neg03));
                res13 = _mm256_add_ps(res13, _mm256_sub_ps(pos13, neg13));
                res23 = _mm256_add_ps(res23, _mm256_sub_ps(pos23, neg23));
                res33 = _mm256_add_ps(res33, _mm256_sub_ps(pos33, neg33));
                res43 = _mm256_add_ps(res43, _mm256_sub_ps(pos43, neg43));
                res53 = _mm256_add_ps(res53, _mm256_sub_ps(pos53, neg53));
                res63 = _mm256_add_ps(res63, _mm256_sub_ps(pos63, neg63));
                res73 = _mm256_add_ps(res73, _mm256_sub_ps(pos73, neg73));
            }
            for (int k = groupData[1]; k < groupData[2]; k++) {
                res00 = _mm256_add_ps(res00, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res01 = _mm256_add_ps(res01, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res02 = _mm256_add_ps(res02, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res03 = _mm256_add_ps(res03, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[2]; k < groupData[3]; k++) {
                res00 = _mm256_sub_ps(res00, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res01 = _mm256_sub_ps(res01, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res02 = _mm256_sub_ps(res02, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res03 = _mm256_sub_ps(res03, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[3]; k < groupData[4]; k++) {
                res10 = _mm256_add_ps(res10, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res11 = _mm256_add_ps(res11, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res12 = _mm256_add_ps(res12, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res13 = _mm256_add_ps(res13, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[4]; k < groupData[5]; k++) {
                res10 = _mm256_sub_ps(res10, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res11 = _mm256_sub_ps(res11, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res12 = _mm256_sub_ps(res12, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res13 = _mm256_sub_ps(res13, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[5]; k < groupData[6]; k++) {
                res20 = _mm256_add_ps(res20, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res21 = _mm256_add_ps(res21, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res22 = _mm256_add_ps(res22, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res23 = _mm256_add_ps(res23, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[6]; k < groupData[7]; k++) {
                res20 = _mm256_sub_ps(res20, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res21 = _mm256_sub_ps(res21, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res22 = _mm256_sub_ps(res22, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res23 = _mm256_sub_ps(res23, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[7]; k < groupData[8]; k++) {
                res30 = _mm256_add_ps(res30, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res31 = _mm256_add_ps(res31, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res32 = _mm256_add_ps(res32, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res33 = _mm256_add_ps(res33, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[8]; k < groupData[9]; k++) {
                res30 = _mm256_sub_ps(res30, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res31 = _mm256_sub_ps(res31, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res32 = _mm256_sub_ps(res32, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res33 = _mm256_sub_ps(res33, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[9]; k < groupData[10]; k++) {
                res40 = _mm256_add_ps(res40, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res41 = _mm256_add_ps(res41, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res42 = _mm256_add_ps(res42, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res43 = _mm256_add_ps(res43, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[10]; k < groupData[11]; k++) {
                res40 = _mm256_sub_ps(res40, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res41 = _mm256_sub_ps(res41, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res42 = _mm256_sub_ps(res42, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res43 = _mm256_sub_ps(res43, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[11]; k < groupData[12]; k++) {
                res50 = _mm256_add_ps(res50, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res51 = _mm256_add_ps(res51, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res52 = _mm256_add_ps(res52, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res53 = _mm256_add_ps(res53, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[12]; k < groupData[13]; k++) {
                res50 = _mm256_sub_ps(res50, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res51 = _mm256_sub_ps(res51, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res52 = _mm256_sub_ps(res52, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res53 = _mm256_sub_ps(res53, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[13]; k < groupData[14]; k++) {
                res60 = _mm256_add_ps(res60, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res61 = _mm256_add_ps(res61, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res62 = _mm256_add_ps(res62, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res63 = _mm256_add_ps(res63, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[14]; k < groupData[15]; k++) {
                res60 = _mm256_sub_ps(res60, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res61 = _mm256_sub_ps(res61, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res62 = _mm256_sub_ps(res62, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res63 = _mm256_sub_ps(res63, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[15]; k < groupData[16]; k++) {
                res70 = _mm256_add_ps(res70, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res71 = _mm256_add_ps(res71, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res72 = _mm256_add_ps(res72, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res73 = _mm256_add_ps(res73, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[16]; k < groupData[17]; k++) {
                res70 = _mm256_sub_ps(res70, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res71 = _mm256_sub_ps(res71, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res72 = _mm256_sub_ps(res72, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res73 = _mm256_sub_ps(res73, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            _mm256_store_ps(result + (j * 8 + 0) * M_ROW + i + 0, res00);
            _mm256_store_ps(result + (j * 8 + 1) * M_ROW + i + 0, res10);
            _mm256_store_ps(result + (j * 8 + 2) * M_ROW + i + 0, res20);
            _mm256_store_ps(result + (j * 8 + 3) * M_ROW + i + 0, res30);
            _mm256_store_ps(result + (j * 8 + 4) * M_ROW + i + 0, res40);
            _mm256_store_ps(result + (j * 8 + 5) * M_ROW + i + 0, res50);
            _mm256_store_ps(result + (j * 8 + 6) * M_ROW + i + 0, res60);
            _mm256_store_ps(result + (j * 8 + 7) * M_ROW + i + 0, res70);
            _mm256_store_ps(result + (j * 8 + 0) * M_ROW + i + 8, res01);
            _mm256_store_ps(result + (j * 8 + 1) * M_ROW + i + 8, res11);
            _mm256_store_ps(result + (j * 8 + 2) * M_ROW + i + 8, res21);
            _mm256_store_ps(result + (j * 8 + 3) * M_ROW + i + 8, res31);
            _mm256_store_ps(result + (j * 8 + 4) * M_ROW + i + 8, res41);
            _mm256_store_ps(result + (j * 8 + 5) * M_ROW + i + 8, res51);
            _mm256_store_ps(result + (j * 8 + 6) * M_ROW + i + 8, res61);
            _mm256_store_ps(result + (j * 8 + 7) * M_ROW + i + 8, res71);
            _mm256_store_ps(result + (j * 8 + 0) * M_ROW + i + 16, res02);
            _mm256_store_ps(result + (j * 8 + 1) * M_ROW + i + 16, res12);
            _mm256_store_ps(result + (j * 8 + 2) * M_ROW + i + 16, res22);
            _mm256_store_ps(result + (j * 8 + 3) * M_ROW + i + 16, res32);
            _mm256_store_ps(result + (j * 8 + 4) * M_ROW + i + 16, res42);
            _mm256_store_ps(result + (j * 8 + 5) * M_ROW + i + 16, res52);
            _mm256_store_ps(result + (j * 8 + 6) * M_ROW + i + 16, res62);
            _mm256_store_ps(result + (j * 8 + 7) * M_ROW + i + 16, res72);
            _mm256_store_ps(result + (j * 8 + 0) * M_ROW + i + 24, res03);
            _mm256_store_ps(result + (j * 8 + 1) * M_ROW + i + 24, res13);
            _mm256_store_ps(result + (j * 8 + 2) * M_ROW + i + 24, res23);
            _mm256_store_ps(result + (j * 8 + 3) * M_ROW + i + 24, res33);
            _mm256_store_ps(result + (j * 8 + 4) * M_ROW + i + 24, res43);
            _mm256_store_ps(result + (j * 8 + 5) * M_ROW + i + 24, res53);
            _mm256_store_ps(result + (j * 8 + 6) * M_ROW + i + 24, res63);
            _mm256_store_ps(result + (j * 8 + 7) * M_ROW + i + 24, res73);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Merged_GroupMin_32xG16_AVX2_OpenMP(const float* X, const int32_t* metadata, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 16; j++) {
        const int* groupData  = &metadata[j * 34];
        for (int i = 0; i < M_ROW; i += 32) {
            __m256 res00 = _mm256_setzero_ps();
            __m256 res01 = _mm256_setzero_ps();
            __m256 res02 = _mm256_setzero_ps();
            __m256 res03 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res11 = _mm256_setzero_ps();
            __m256 res12 = _mm256_setzero_ps();
            __m256 res13 = _mm256_setzero_ps();
            __m256 res20 = _mm256_setzero_ps();
            __m256 res21 = _mm256_setzero_ps();
            __m256 res22 = _mm256_setzero_ps();
            __m256 res23 = _mm256_setzero_ps();
            __m256 res30 = _mm256_setzero_ps();
            __m256 res31 = _mm256_setzero_ps();
            __m256 res32 = _mm256_setzero_ps();
            __m256 res33 = _mm256_setzero_ps();
            __m256 res40 = _mm256_setzero_ps();
            __m256 res41 = _mm256_setzero_ps();
            __m256 res42 = _mm256_setzero_ps();
            __m256 res43 = _mm256_setzero_ps();
            __m256 res50 = _mm256_setzero_ps();
            __m256 res51 = _mm256_setzero_ps();
            __m256 res52 = _mm256_setzero_ps();
            __m256 res53 = _mm256_setzero_ps();
            __m256 res60 = _mm256_setzero_ps();
            __m256 res61 = _mm256_setzero_ps();
            __m256 res62 = _mm256_setzero_ps();
            __m256 res63 = _mm256_setzero_ps();
            __m256 res70 = _mm256_setzero_ps();
            __m256 res71 = _mm256_setzero_ps();
            __m256 res72 = _mm256_setzero_ps();
            __m256 res73 = _mm256_setzero_ps();
            __m256 res80 = _mm256_setzero_ps();
            __m256 res81 = _mm256_setzero_ps();
            __m256 res82 = _mm256_setzero_ps();
            __m256 res83 = _mm256_setzero_ps();
            __m256 res90 = _mm256_setzero_ps();
            __m256 res91 = _mm256_setzero_ps();
            __m256 res92 = _mm256_setzero_ps();
            __m256 res93 = _mm256_setzero_ps();
            __m256 res100 = _mm256_setzero_ps();
            __m256 res101 = _mm256_setzero_ps();
            __m256 res102 = _mm256_setzero_ps();
            __m256 res103 = _mm256_setzero_ps();
            __m256 res110 = _mm256_setzero_ps();
            __m256 res111 = _mm256_setzero_ps();
            __m256 res112 = _mm256_setzero_ps();
            __m256 res113 = _mm256_setzero_ps();
            __m256 res120 = _mm256_setzero_ps();
            __m256 res121 = _mm256_setzero_ps();
            __m256 res122 = _mm256_setzero_ps();
            __m256 res123 = _mm256_setzero_ps();
            __m256 res130 = _mm256_setzero_ps();
            __m256 res131 = _mm256_setzero_ps();
            __m256 res132 = _mm256_setzero_ps();
            __m256 res133 = _mm256_setzero_ps();
            __m256 res140 = _mm256_setzero_ps();
            __m256 res141 = _mm256_setzero_ps();
            __m256 res142 = _mm256_setzero_ps();
            __m256 res143 = _mm256_setzero_ps();
            __m256 res150 = _mm256_setzero_ps();
            __m256 res151 = _mm256_setzero_ps();
            __m256 res152 = _mm256_setzero_ps();
            __m256 res153 = _mm256_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 32) {
                __m256 pos00 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m256 neg00 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m256 pos10 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m256 neg10 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m256 pos20 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m256 neg20 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m256 pos30 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m256 neg30 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m256 pos40 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 0);
                __m256 neg40 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 0);
                __m256 pos50 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 0);
                __m256 neg50 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 0);
                __m256 pos60 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 0);
                __m256 neg60 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 0);
                __m256 pos70 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 0);
                __m256 neg70 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 0);
                __m256 pos80 = _mm256_load_ps(X + row_index[k + 16] * M_ROW + i + 0);
                __m256 neg80 = _mm256_load_ps(X + row_index[k + 17] * M_ROW + i + 0);
                __m256 pos90 = _mm256_load_ps(X + row_index[k + 18] * M_ROW + i + 0);
                __m256 neg90 = _mm256_load_ps(X + row_index[k + 19] * M_ROW + i + 0);
                __m256 pos100 = _mm256_load_ps(X + row_index[k + 20] * M_ROW + i + 0);
                __m256 neg100 = _mm256_load_ps(X + row_index[k + 21] * M_ROW + i + 0);
                __m256 pos110 = _mm256_load_ps(X + row_index[k + 22] * M_ROW + i + 0);
                __m256 neg110 = _mm256_load_ps(X + row_index[k + 23] * M_ROW + i + 0);
                __m256 pos120 = _mm256_load_ps(X + row_index[k + 24] * M_ROW + i + 0);
                __m256 neg120 = _mm256_load_ps(X + row_index[k + 25] * M_ROW + i + 0);
                __m256 pos130 = _mm256_load_ps(X + row_index[k + 26] * M_ROW + i + 0);
                __m256 neg130 = _mm256_load_ps(X + row_index[k + 27] * M_ROW + i + 0);
                __m256 pos140 = _mm256_load_ps(X + row_index[k + 28] * M_ROW + i + 0);
                __m256 neg140 = _mm256_load_ps(X + row_index[k + 29] * M_ROW + i + 0);
                __m256 pos150 = _mm256_load_ps(X + row_index[k + 30] * M_ROW + i + 0);
                __m256 neg150 = _mm256_load_ps(X + row_index[k + 31] * M_ROW + i + 0);
                __m256 pos01 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 8);
                __m256 neg01 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 8);
                __m256 pos11 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 8);
                __m256 neg11 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 8);
                __m256 pos21 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 8);
                __m256 neg21 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 8);
                __m256 pos31 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 8);
                __m256 neg31 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 8);
                __m256 pos41 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 8);
                __m256 neg41 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 8);
                __m256 pos51 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 8);
                __m256 neg51 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 8);
                __m256 pos61 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 8);
                __m256 neg61 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 8);
                __m256 pos71 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 8);
                __m256 neg71 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 8);
                __m256 pos81 = _mm256_load_ps(X + row_index[k + 16] * M_ROW + i + 8);
                __m256 neg81 = _mm256_load_ps(X + row_index[k + 17] * M_ROW + i + 8);
                __m256 pos91 = _mm256_load_ps(X + row_index[k + 18] * M_ROW + i + 8);
                __m256 neg91 = _mm256_load_ps(X + row_index[k + 19] * M_ROW + i + 8);
                __m256 pos101 = _mm256_load_ps(X + row_index[k + 20] * M_ROW + i + 8);
                __m256 neg101 = _mm256_load_ps(X + row_index[k + 21] * M_ROW + i + 8);
                __m256 pos111 = _mm256_load_ps(X + row_index[k + 22] * M_ROW + i + 8);
                __m256 neg111 = _mm256_load_ps(X + row_index[k + 23] * M_ROW + i + 8);
                __m256 pos121 = _mm256_load_ps(X + row_index[k + 24] * M_ROW + i + 8);
                __m256 neg121 = _mm256_load_ps(X + row_index[k + 25] * M_ROW + i + 8);
                __m256 pos131 = _mm256_load_ps(X + row_index[k + 26] * M_ROW + i + 8);
                __m256 neg131 = _mm256_load_ps(X + row_index[k + 27] * M_ROW + i + 8);
                __m256 pos141 = _mm256_load_ps(X + row_index[k + 28] * M_ROW + i + 8);
                __m256 neg141 = _mm256_load_ps(X + row_index[k + 29] * M_ROW + i + 8);
                __m256 pos151 = _mm256_load_ps(X + row_index[k + 30] * M_ROW + i + 8);
                __m256 neg151 = _mm256_load_ps(X + row_index[k + 31] * M_ROW + i + 8);
                __m256 pos02 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 16);
                __m256 neg02 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 16);
                __m256 pos12 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 16);
                __m256 neg12 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 16);
                __m256 pos22 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 16);
                __m256 neg22 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 16);
                __m256 pos32 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 16);
                __m256 neg32 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 16);
                __m256 pos42 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 16);
                __m256 neg42 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 16);
                __m256 pos52 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 16);
                __m256 neg52 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 16);
                __m256 pos62 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 16);
                __m256 neg62 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 16);
                __m256 pos72 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 16);
                __m256 neg72 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 16);
                __m256 pos82 = _mm256_load_ps(X + row_index[k + 16] * M_ROW + i + 16);
                __m256 neg82 = _mm256_load_ps(X + row_index[k + 17] * M_ROW + i + 16);
                __m256 pos92 = _mm256_load_ps(X + row_index[k + 18] * M_ROW + i + 16);
                __m256 neg92 = _mm256_load_ps(X + row_index[k + 19] * M_ROW + i + 16);
                __m256 pos102 = _mm256_load_ps(X + row_index[k + 20] * M_ROW + i + 16);
                __m256 neg102 = _mm256_load_ps(X + row_index[k + 21] * M_ROW + i + 16);
                __m256 pos112 = _mm256_load_ps(X + row_index[k + 22] * M_ROW + i + 16);
                __m256 neg112 = _mm256_load_ps(X + row_index[k + 23] * M_ROW + i + 16);
                __m256 pos122 = _mm256_load_ps(X + row_index[k + 24] * M_ROW + i + 16);
                __m256 neg122 = _mm256_load_ps(X + row_index[k + 25] * M_ROW + i + 16);
                __m256 pos132 = _mm256_load_ps(X + row_index[k + 26] * M_ROW + i + 16);
                __m256 neg132 = _mm256_load_ps(X + row_index[k + 27] * M_ROW + i + 16);
                __m256 pos142 = _mm256_load_ps(X + row_index[k + 28] * M_ROW + i + 16);
                __m256 neg142 = _mm256_load_ps(X + row_index[k + 29] * M_ROW + i + 16);
                __m256 pos152 = _mm256_load_ps(X + row_index[k + 30] * M_ROW + i + 16);
                __m256 neg152 = _mm256_load_ps(X + row_index[k + 31] * M_ROW + i + 16);
                __m256 pos03 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 24);
                __m256 neg03 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 24);
                __m256 pos13 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 24);
                __m256 neg13 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 24);
                __m256 pos23 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 24);
                __m256 neg23 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 24);
                __m256 pos33 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 24);
                __m256 neg33 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 24);
                __m256 pos43 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 24);
                __m256 neg43 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 24);
                __m256 pos53 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 24);
                __m256 neg53 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 24);
                __m256 pos63 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 24);
                __m256 neg63 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 24);
                __m256 pos73 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 24);
                __m256 neg73 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 24);
                __m256 pos83 = _mm256_load_ps(X + row_index[k + 16] * M_ROW + i + 24);
                __m256 neg83 = _mm256_load_ps(X + row_index[k + 17] * M_ROW + i + 24);
                __m256 pos93 = _mm256_load_ps(X + row_index[k + 18] * M_ROW + i + 24);
                __m256 neg93 = _mm256_load_ps(X + row_index[k + 19] * M_ROW + i + 24);
                __m256 pos103 = _mm256_load_ps(X + row_index[k + 20] * M_ROW + i + 24);
                __m256 neg103 = _mm256_load_ps(X + row_index[k + 21] * M_ROW + i + 24);
                __m256 pos113 = _mm256_load_ps(X + row_index[k + 22] * M_ROW + i + 24);
                __m256 neg113 = _mm256_load_ps(X + row_index[k + 23] * M_ROW + i + 24);
                __m256 pos123 = _mm256_load_ps(X + row_index[k + 24] * M_ROW + i + 24);
                __m256 neg123 = _mm256_load_ps(X + row_index[k + 25] * M_ROW + i + 24);
                __m256 pos133 = _mm256_load_ps(X + row_index[k + 26] * M_ROW + i + 24);
                __m256 neg133 = _mm256_load_ps(X + row_index[k + 27] * M_ROW + i + 24);
                __m256 pos143 = _mm256_load_ps(X + row_index[k + 28] * M_ROW + i + 24);
                __m256 neg143 = _mm256_load_ps(X + row_index[k + 29] * M_ROW + i + 24);
                __m256 pos153 = _mm256_load_ps(X + row_index[k + 30] * M_ROW + i + 24);
                __m256 neg153 = _mm256_load_ps(X + row_index[k + 31] * M_ROW + i + 24);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res20 = _mm256_add_ps(res20, _mm256_sub_ps(pos20, neg20));
                res30 = _mm256_add_ps(res30, _mm256_sub_ps(pos30, neg30));
                res40 = _mm256_add_ps(res40, _mm256_sub_ps(pos40, neg40));
                res50 = _mm256_add_ps(res50, _mm256_sub_ps(pos50, neg50));
                res60 = _mm256_add_ps(res60, _mm256_sub_ps(pos60, neg60));
                res70 = _mm256_add_ps(res70, _mm256_sub_ps(pos70, neg70));
                res80 = _mm256_add_ps(res80, _mm256_sub_ps(pos80, neg80));
                res90 = _mm256_add_ps(res90, _mm256_sub_ps(pos90, neg90));
                res100 = _mm256_add_ps(res100, _mm256_sub_ps(pos100, neg100));
                res110 = _mm256_add_ps(res110, _mm256_sub_ps(pos110, neg110));
                res120 = _mm256_add_ps(res120, _mm256_sub_ps(pos120, neg120));
                res130 = _mm256_add_ps(res130, _mm256_sub_ps(pos130, neg130));
                res140 = _mm256_add_ps(res140, _mm256_sub_ps(pos140, neg140));
                res150 = _mm256_add_ps(res150, _mm256_sub_ps(pos150, neg150));
                res01 = _mm256_add_ps(res01, _mm256_sub_ps(pos01, neg01));
                res11 = _mm256_add_ps(res11, _mm256_sub_ps(pos11, neg11));
                res21 = _mm256_add_ps(res21, _mm256_sub_ps(pos21, neg21));
                res31 = _mm256_add_ps(res31, _mm256_sub_ps(pos31, neg31));
                res41 = _mm256_add_ps(res41, _mm256_sub_ps(pos41, neg41));
                res51 = _mm256_add_ps(res51, _mm256_sub_ps(pos51, neg51));
                res61 = _mm256_add_ps(res61, _mm256_sub_ps(pos61, neg61));
                res71 = _mm256_add_ps(res71, _mm256_sub_ps(pos71, neg71));
                res81 = _mm256_add_ps(res81, _mm256_sub_ps(pos81, neg81));
                res91 = _mm256_add_ps(res91, _mm256_sub_ps(pos91, neg91));
                res101 = _mm256_add_ps(res101, _mm256_sub_ps(pos101, neg101));
                res111 = _mm256_add_ps(res111, _mm256_sub_ps(pos111, neg111));
                res121 = _mm256_add_ps(res121, _mm256_sub_ps(pos121, neg121));
                res131 = _mm256_add_ps(res131, _mm256_sub_ps(pos131, neg131));
                res141 = _mm256_add_ps(res141, _mm256_sub_ps(pos141, neg141));
                res151 = _mm256_add_ps(res151, _mm256_sub_ps(pos151, neg151));
                res02 = _mm256_add_ps(res02, _mm256_sub_ps(pos02, neg02));
                res12 = _mm256_add_ps(res12, _mm256_sub_ps(pos12, neg12));
                res22 = _mm256_add_ps(res22, _mm256_sub_ps(pos22, neg22));
                res32 = _mm256_add_ps(res32, _mm256_sub_ps(pos32, neg32));
                res42 = _mm256_add_ps(res42, _mm256_sub_ps(pos42, neg42));
                res52 = _mm256_add_ps(res52, _mm256_sub_ps(pos52, neg52));
                res62 = _mm256_add_ps(res62, _mm256_sub_ps(pos62, neg62));
                res72 = _mm256_add_ps(res72, _mm256_sub_ps(pos72, neg72));
                res82 = _mm256_add_ps(res82, _mm256_sub_ps(pos82, neg82));
                res92 = _mm256_add_ps(res92, _mm256_sub_ps(pos92, neg92));
                res102 = _mm256_add_ps(res102, _mm256_sub_ps(pos102, neg102));
                res112 = _mm256_add_ps(res112, _mm256_sub_ps(pos112, neg112));
                res122 = _mm256_add_ps(res122, _mm256_sub_ps(pos122, neg122));
                res132 = _mm256_add_ps(res132, _mm256_sub_ps(pos132, neg132));
                res142 = _mm256_add_ps(res142, _mm256_sub_ps(pos142, neg142));
                res152 = _mm256_add_ps(res152, _mm256_sub_ps(pos152, neg152));
                res03 = _mm256_add_ps(res03, _mm256_sub_ps(pos03, neg03));
                res13 = _mm256_add_ps(res13, _mm256_sub_ps(pos13, neg13));
                res23 = _mm256_add_ps(res23, _mm256_sub_ps(pos23, neg23));
                res33 = _mm256_add_ps(res33, _mm256_sub_ps(pos33, neg33));
                res43 = _mm256_add_ps(res43, _mm256_sub_ps(pos43, neg43));
                res53 = _mm256_add_ps(res53, _mm256_sub_ps(pos53, neg53));
                res63 = _mm256_add_ps(res63, _mm256_sub_ps(pos63, neg63));
                res73 = _mm256_add_ps(res73, _mm256_sub_ps(pos73, neg73));
                res83 = _mm256_add_ps(res83, _mm256_sub_ps(pos83, neg83));
                res93 = _mm256_add_ps(res93, _mm256_sub_ps(pos93, neg93));
                res103 = _mm256_add_ps(res103, _mm256_sub_ps(pos103, neg103));
                res113 = _mm256_add_ps(res113, _mm256_sub_ps(pos113, neg113));
                res123 = _mm256_add_ps(res123, _mm256_sub_ps(pos123, neg123));
                res133 = _mm256_add_ps(res133, _mm256_sub_ps(pos133, neg133));
                res143 = _mm256_add_ps(res143, _mm256_sub_ps(pos143, neg143));
                res153 = _mm256_add_ps(res153, _mm256_sub_ps(pos153, neg153));
            }
            for (int k = groupData[1]; k < groupData[2]; k++) {
                res00 = _mm256_add_ps(res00, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res01 = _mm256_add_ps(res01, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res02 = _mm256_add_ps(res02, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res03 = _mm256_add_ps(res03, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[2]; k < groupData[3]; k++) {
                res00 = _mm256_sub_ps(res00, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res01 = _mm256_sub_ps(res01, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res02 = _mm256_sub_ps(res02, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res03 = _mm256_sub_ps(res03, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[3]; k < groupData[4]; k++) {
                res10 = _mm256_add_ps(res10, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res11 = _mm256_add_ps(res11, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res12 = _mm256_add_ps(res12, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res13 = _mm256_add_ps(res13, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[4]; k < groupData[5]; k++) {
                res10 = _mm256_sub_ps(res10, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res11 = _mm256_sub_ps(res11, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res12 = _mm256_sub_ps(res12, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res13 = _mm256_sub_ps(res13, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[5]; k < groupData[6]; k++) {
                res20 = _mm256_add_ps(res20, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res21 = _mm256_add_ps(res21, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res22 = _mm256_add_ps(res22, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res23 = _mm256_add_ps(res23, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[6]; k < groupData[7]; k++) {
                res20 = _mm256_sub_ps(res20, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res21 = _mm256_sub_ps(res21, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res22 = _mm256_sub_ps(res22, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res23 = _mm256_sub_ps(res23, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[7]; k < groupData[8]; k++) {
                res30 = _mm256_add_ps(res30, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res31 = _mm256_add_ps(res31, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res32 = _mm256_add_ps(res32, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res33 = _mm256_add_ps(res33, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[8]; k < groupData[9]; k++) {
                res30 = _mm256_sub_ps(res30, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res31 = _mm256_sub_ps(res31, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res32 = _mm256_sub_ps(res32, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res33 = _mm256_sub_ps(res33, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[9]; k < groupData[10]; k++) {
                res40 = _mm256_add_ps(res40, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res41 = _mm256_add_ps(res41, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res42 = _mm256_add_ps(res42, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res43 = _mm256_add_ps(res43, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[10]; k < groupData[11]; k++) {
                res40 = _mm256_sub_ps(res40, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res41 = _mm256_sub_ps(res41, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res42 = _mm256_sub_ps(res42, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res43 = _mm256_sub_ps(res43, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[11]; k < groupData[12]; k++) {
                res50 = _mm256_add_ps(res50, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res51 = _mm256_add_ps(res51, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res52 = _mm256_add_ps(res52, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res53 = _mm256_add_ps(res53, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[12]; k < groupData[13]; k++) {
                res50 = _mm256_sub_ps(res50, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res51 = _mm256_sub_ps(res51, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res52 = _mm256_sub_ps(res52, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res53 = _mm256_sub_ps(res53, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[13]; k < groupData[14]; k++) {
                res60 = _mm256_add_ps(res60, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res61 = _mm256_add_ps(res61, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res62 = _mm256_add_ps(res62, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res63 = _mm256_add_ps(res63, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[14]; k < groupData[15]; k++) {
                res60 = _mm256_sub_ps(res60, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res61 = _mm256_sub_ps(res61, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res62 = _mm256_sub_ps(res62, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res63 = _mm256_sub_ps(res63, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[15]; k < groupData[16]; k++) {
                res70 = _mm256_add_ps(res70, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res71 = _mm256_add_ps(res71, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res72 = _mm256_add_ps(res72, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res73 = _mm256_add_ps(res73, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[16]; k < groupData[17]; k++) {
                res70 = _mm256_sub_ps(res70, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res71 = _mm256_sub_ps(res71, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res72 = _mm256_sub_ps(res72, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res73 = _mm256_sub_ps(res73, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[17]; k < groupData[18]; k++) {
                res80 = _mm256_add_ps(res80, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res81 = _mm256_add_ps(res81, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res82 = _mm256_add_ps(res82, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res83 = _mm256_add_ps(res83, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[18]; k < groupData[19]; k++) {
                res80 = _mm256_sub_ps(res80, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res81 = _mm256_sub_ps(res81, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res82 = _mm256_sub_ps(res82, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res83 = _mm256_sub_ps(res83, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[19]; k < groupData[20]; k++) {
                res90 = _mm256_add_ps(res90, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res91 = _mm256_add_ps(res91, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res92 = _mm256_add_ps(res92, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res93 = _mm256_add_ps(res93, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[20]; k < groupData[21]; k++) {
                res90 = _mm256_sub_ps(res90, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res91 = _mm256_sub_ps(res91, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res92 = _mm256_sub_ps(res92, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res93 = _mm256_sub_ps(res93, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[21]; k < groupData[22]; k++) {
                res100 = _mm256_add_ps(res100, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res101 = _mm256_add_ps(res101, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res102 = _mm256_add_ps(res102, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res103 = _mm256_add_ps(res103, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[22]; k < groupData[23]; k++) {
                res100 = _mm256_sub_ps(res100, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res101 = _mm256_sub_ps(res101, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res102 = _mm256_sub_ps(res102, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res103 = _mm256_sub_ps(res103, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[23]; k < groupData[24]; k++) {
                res110 = _mm256_add_ps(res110, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res111 = _mm256_add_ps(res111, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res112 = _mm256_add_ps(res112, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res113 = _mm256_add_ps(res113, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[24]; k < groupData[25]; k++) {
                res110 = _mm256_sub_ps(res110, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res111 = _mm256_sub_ps(res111, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res112 = _mm256_sub_ps(res112, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res113 = _mm256_sub_ps(res113, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[25]; k < groupData[26]; k++) {
                res120 = _mm256_add_ps(res120, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res121 = _mm256_add_ps(res121, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res122 = _mm256_add_ps(res122, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res123 = _mm256_add_ps(res123, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[26]; k < groupData[27]; k++) {
                res120 = _mm256_sub_ps(res120, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res121 = _mm256_sub_ps(res121, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res122 = _mm256_sub_ps(res122, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res123 = _mm256_sub_ps(res123, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[27]; k < groupData[28]; k++) {
                res130 = _mm256_add_ps(res130, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res131 = _mm256_add_ps(res131, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res132 = _mm256_add_ps(res132, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res133 = _mm256_add_ps(res133, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[28]; k < groupData[29]; k++) {
                res130 = _mm256_sub_ps(res130, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res131 = _mm256_sub_ps(res131, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res132 = _mm256_sub_ps(res132, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res133 = _mm256_sub_ps(res133, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[29]; k < groupData[30]; k++) {
                res140 = _mm256_add_ps(res140, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res141 = _mm256_add_ps(res141, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res142 = _mm256_add_ps(res142, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res143 = _mm256_add_ps(res143, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[30]; k < groupData[31]; k++) {
                res140 = _mm256_sub_ps(res140, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res141 = _mm256_sub_ps(res141, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res142 = _mm256_sub_ps(res142, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res143 = _mm256_sub_ps(res143, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[31]; k < groupData[32]; k++) {
                res150 = _mm256_add_ps(res150, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res151 = _mm256_add_ps(res151, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res152 = _mm256_add_ps(res152, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res153 = _mm256_add_ps(res153, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[32]; k < groupData[33]; k++) {
                res150 = _mm256_sub_ps(res150, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res151 = _mm256_sub_ps(res151, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res152 = _mm256_sub_ps(res152, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res153 = _mm256_sub_ps(res153, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            _mm256_store_ps(result + (j * 16 + 0) * M_ROW + i + 0, res00);
            _mm256_store_ps(result + (j * 16 + 1) * M_ROW + i + 0, res10);
            _mm256_store_ps(result + (j * 16 + 2) * M_ROW + i + 0, res20);
            _mm256_store_ps(result + (j * 16 + 3) * M_ROW + i + 0, res30);
            _mm256_store_ps(result + (j * 16 + 4) * M_ROW + i + 0, res40);
            _mm256_store_ps(result + (j * 16 + 5) * M_ROW + i + 0, res50);
            _mm256_store_ps(result + (j * 16 + 6) * M_ROW + i + 0, res60);
            _mm256_store_ps(result + (j * 16 + 7) * M_ROW + i + 0, res70);
            _mm256_store_ps(result + (j * 16 + 8) * M_ROW + i + 0, res80);
            _mm256_store_ps(result + (j * 16 + 9) * M_ROW + i + 0, res90);
            _mm256_store_ps(result + (j * 16 + 10) * M_ROW + i + 0, res100);
            _mm256_store_ps(result + (j * 16 + 11) * M_ROW + i + 0, res110);
            _mm256_store_ps(result + (j * 16 + 12) * M_ROW + i + 0, res120);
            _mm256_store_ps(result + (j * 16 + 13) * M_ROW + i + 0, res130);
            _mm256_store_ps(result + (j * 16 + 14) * M_ROW + i + 0, res140);
            _mm256_store_ps(result + (j * 16 + 15) * M_ROW + i + 0, res150);
            _mm256_store_ps(result + (j * 16 + 0) * M_ROW + i + 8, res01);
            _mm256_store_ps(result + (j * 16 + 1) * M_ROW + i + 8, res11);
            _mm256_store_ps(result + (j * 16 + 2) * M_ROW + i + 8, res21);
            _mm256_store_ps(result + (j * 16 + 3) * M_ROW + i + 8, res31);
            _mm256_store_ps(result + (j * 16 + 4) * M_ROW + i + 8, res41);
            _mm256_store_ps(result + (j * 16 + 5) * M_ROW + i + 8, res51);
            _mm256_store_ps(result + (j * 16 + 6) * M_ROW + i + 8, res61);
            _mm256_store_ps(result + (j * 16 + 7) * M_ROW + i + 8, res71);
            _mm256_store_ps(result + (j * 16 + 8) * M_ROW + i + 8, res81);
            _mm256_store_ps(result + (j * 16 + 9) * M_ROW + i + 8, res91);
            _mm256_store_ps(result + (j * 16 + 10) * M_ROW + i + 8, res101);
            _mm256_store_ps(result + (j * 16 + 11) * M_ROW + i + 8, res111);
            _mm256_store_ps(result + (j * 16 + 12) * M_ROW + i + 8, res121);
            _mm256_store_ps(result + (j * 16 + 13) * M_ROW + i + 8, res131);
            _mm256_store_ps(result + (j * 16 + 14) * M_ROW + i + 8, res141);
            _mm256_store_ps(result + (j * 16 + 15) * M_ROW + i + 8, res151);
            _mm256_store_ps(result + (j * 16 + 0) * M_ROW + i + 16, res02);
            _mm256_store_ps(result + (j * 16 + 1) * M_ROW + i + 16, res12);
            _mm256_store_ps(result + (j * 16 + 2) * M_ROW + i + 16, res22);
            _mm256_store_ps(result + (j * 16 + 3) * M_ROW + i + 16, res32);
            _mm256_store_ps(result + (j * 16 + 4) * M_ROW + i + 16, res42);
            _mm256_store_ps(result + (j * 16 + 5) * M_ROW + i + 16, res52);
            _mm256_store_ps(result + (j * 16 + 6) * M_ROW + i + 16, res62);
            _mm256_store_ps(result + (j * 16 + 7) * M_ROW + i + 16, res72);
            _mm256_store_ps(result + (j * 16 + 8) * M_ROW + i + 16, res82);
            _mm256_store_ps(result + (j * 16 + 9) * M_ROW + i + 16, res92);
            _mm256_store_ps(result + (j * 16 + 10) * M_ROW + i + 16, res102);
            _mm256_store_ps(result + (j * 16 + 11) * M_ROW + i + 16, res112);
            _mm256_store_ps(result + (j * 16 + 12) * M_ROW + i + 16, res122);
            _mm256_store_ps(result + (j * 16 + 13) * M_ROW + i + 16, res132);
            _mm256_store_ps(result + (j * 16 + 14) * M_ROW + i + 16, res142);
            _mm256_store_ps(result + (j * 16 + 15) * M_ROW + i + 16, res152);
            _mm256_store_ps(result + (j * 16 + 0) * M_ROW + i + 24, res03);
            _mm256_store_ps(result + (j * 16 + 1) * M_ROW + i + 24, res13);
            _mm256_store_ps(result + (j * 16 + 2) * M_ROW + i + 24, res23);
            _mm256_store_ps(result + (j * 16 + 3) * M_ROW + i + 24, res33);
            _mm256_store_ps(result + (j * 16 + 4) * M_ROW + i + 24, res43);
            _mm256_store_ps(result + (j * 16 + 5) * M_ROW + i + 24, res53);
            _mm256_store_ps(result + (j * 16 + 6) * M_ROW + i + 24, res63);
            _mm256_store_ps(result + (j * 16 + 7) * M_ROW + i + 24, res73);
            _mm256_store_ps(result + (j * 16 + 8) * M_ROW + i + 24, res83);
            _mm256_store_ps(result + (j * 16 + 9) * M_ROW + i + 24, res93);
            _mm256_store_ps(result + (j * 16 + 10) * M_ROW + i + 24, res103);
            _mm256_store_ps(result + (j * 16 + 11) * M_ROW + i + 24, res113);
            _mm256_store_ps(result + (j * 16 + 12) * M_ROW + i + 24, res123);
            _mm256_store_ps(result + (j * 16 + 13) * M_ROW + i + 24, res133);
            _mm256_store_ps(result + (j * 16 + 14) * M_ROW + i + 24, res143);
            _mm256_store_ps(result + (j * 16 + 15) * M_ROW + i + 24, res153);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Merged_GroupMin_32xG32_AVX2_OpenMP(const float* X, const int32_t* metadata, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 32; j++) {
        const int* groupData  = &metadata[j * 66];
        for (int i = 0; i < M_ROW; i += 32) {
            __m256 res00 = _mm256_setzero_ps();
            __m256 res01 = _mm256_setzero_ps();
            __m256 res02 = _mm256_setzero_ps();
            __m256 res03 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res11 = _mm256_setzero_ps();
            __m256 res12 = _mm256_setzero_ps();
            __m256 res13 = _mm256_setzero_ps();
            __m256 res20 = _mm256_setzero_ps();
            __m256 res21 = _mm256_setzero_ps();
            __m256 res22 = _mm256_setzero_ps();
            __m256 res23 = _mm256_setzero_ps();
            __m256 res30 = _mm256_setzero_ps();
            __m256 res31 = _mm256_setzero_ps();
            __m256 res32 = _mm256_setzero_ps();
            __m256 res33 = _mm256_setzero_ps();
            __m256 res40 = _mm256_setzero_ps();
            __m256 res41 = _mm256_setzero_ps();
            __m256 res42 = _mm256_setzero_ps();
            __m256 res43 = _mm256_setzero_ps();
            __m256 res50 = _mm256_setzero_ps();
            __m256 res51 = _mm256_setzero_ps();
            __m256 res52 = _mm256_setzero_ps();
            __m256 res53 = _mm256_setzero_ps();
            __m256 res60 = _mm256_setzero_ps();
            __m256 res61 = _mm256_setzero_ps();
            __m256 res62 = _mm256_setzero_ps();
            __m256 res63 = _mm256_setzero_ps();
            __m256 res70 = _mm256_setzero_ps();
            __m256 res71 = _mm256_setzero_ps();
            __m256 res72 = _mm256_setzero_ps();
            __m256 res73 = _mm256_setzero_ps();
            __m256 res80 = _mm256_setzero_ps();
            __m256 res81 = _mm256_setzero_ps();
            __m256 res82 = _mm256_setzero_ps();
            __m256 res83 = _mm256_setzero_ps();
            __m256 res90 = _mm256_setzero_ps();
            __m256 res91 = _mm256_setzero_ps();
            __m256 res92 = _mm256_setzero_ps();
            __m256 res93 = _mm256_setzero_ps();
            __m256 res100 = _mm256_setzero_ps();
            __m256 res101 = _mm256_setzero_ps();
            __m256 res102 = _mm256_setzero_ps();
            __m256 res103 = _mm256_setzero_ps();
            __m256 res110 = _mm256_setzero_ps();
            __m256 res111 = _mm256_setzero_ps();
            __m256 res112 = _mm256_setzero_ps();
            __m256 res113 = _mm256_setzero_ps();
            __m256 res120 = _mm256_setzero_ps();
            __m256 res121 = _mm256_setzero_ps();
            __m256 res122 = _mm256_setzero_ps();
            __m256 res123 = _mm256_setzero_ps();
            __m256 res130 = _mm256_setzero_ps();
            __m256 res131 = _mm256_setzero_ps();
            __m256 res132 = _mm256_setzero_ps();
            __m256 res133 = _mm256_setzero_ps();
            __m256 res140 = _mm256_setzero_ps();
            __m256 res141 = _mm256_setzero_ps();
            __m256 res142 = _mm256_setzero_ps();
            __m256 res143 = _mm256_setzero_ps();
            __m256 res150 = _mm256_setzero_ps();
            __m256 res151 = _mm256_setzero_ps();
            __m256 res152 = _mm256_setzero_ps();
            __m256 res153 = _mm256_setzero_ps();
            __m256 res160 = _mm256_setzero_ps();
            __m256 res161 = _mm256_setzero_ps();
            __m256 res162 = _mm256_setzero_ps();
            __m256 res163 = _mm256_setzero_ps();
            __m256 res170 = _mm256_setzero_ps();
            __m256 res171 = _mm256_setzero_ps();
            __m256 res172 = _mm256_setzero_ps();
            __m256 res173 = _mm256_setzero_ps();
            __m256 res180 = _mm256_setzero_ps();
            __m256 res181 = _mm256_setzero_ps();
            __m256 res182 = _mm256_setzero_ps();
            __m256 res183 = _mm256_setzero_ps();
            __m256 res190 = _mm256_setzero_ps();
            __m256 res191 = _mm256_setzero_ps();
            __m256 res192 = _mm256_setzero_ps();
            __m256 res193 = _mm256_setzero_ps();
            __m256 res200 = _mm256_setzero_ps();
            __m256 res201 = _mm256_setzero_ps();
            __m256 res202 = _mm256_setzero_ps();
            __m256 res203 = _mm256_setzero_ps();
            __m256 res210 = _mm256_setzero_ps();
            __m256 res211 = _mm256_setzero_ps();
            __m256 res212 = _mm256_setzero_ps();
            __m256 res213 = _mm256_setzero_ps();
            __m256 res220 = _mm256_setzero_ps();
            __m256 res221 = _mm256_setzero_ps();
            __m256 res222 = _mm256_setzero_ps();
            __m256 res223 = _mm256_setzero_ps();
            __m256 res230 = _mm256_setzero_ps();
            __m256 res231 = _mm256_setzero_ps();
            __m256 res232 = _mm256_setzero_ps();
            __m256 res233 = _mm256_setzero_ps();
            __m256 res240 = _mm256_setzero_ps();
            __m256 res241 = _mm256_setzero_ps();
            __m256 res242 = _mm256_setzero_ps();
            __m256 res243 = _mm256_setzero_ps();
            __m256 res250 = _mm256_setzero_ps();
            __m256 res251 = _mm256_setzero_ps();
            __m256 res252 = _mm256_setzero_ps();
            __m256 res253 = _mm256_setzero_ps();
            __m256 res260 = _mm256_setzero_ps();
            __m256 res261 = _mm256_setzero_ps();
            __m256 res262 = _mm256_setzero_ps();
            __m256 res263 = _mm256_setzero_ps();
            __m256 res270 = _mm256_setzero_ps();
            __m256 res271 = _mm256_setzero_ps();
            __m256 res272 = _mm256_setzero_ps();
            __m256 res273 = _mm256_setzero_ps();
            __m256 res280 = _mm256_setzero_ps();
            __m256 res281 = _mm256_setzero_ps();
            __m256 res282 = _mm256_setzero_ps();
            __m256 res283 = _mm256_setzero_ps();
            __m256 res290 = _mm256_setzero_ps();
            __m256 res291 = _mm256_setzero_ps();
            __m256 res292 = _mm256_setzero_ps();
            __m256 res293 = _mm256_setzero_ps();
            __m256 res300 = _mm256_setzero_ps();
            __m256 res301 = _mm256_setzero_ps();
            __m256 res302 = _mm256_setzero_ps();
            __m256 res303 = _mm256_setzero_ps();
            __m256 res310 = _mm256_setzero_ps();
            __m256 res311 = _mm256_setzero_ps();
            __m256 res312 = _mm256_setzero_ps();
            __m256 res313 = _mm256_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 64) {
                __m256 pos00 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m256 neg00 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m256 pos10 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m256 neg10 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m256 pos20 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m256 neg20 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m256 pos30 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m256 neg30 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m256 pos40 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 0);
                __m256 neg40 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 0);
                __m256 pos50 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 0);
                __m256 neg50 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 0);
                __m256 pos60 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 0);
                __m256 neg60 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 0);
                __m256 pos70 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 0);
                __m256 neg70 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 0);
                __m256 pos80 = _mm256_load_ps(X + row_index[k + 16] * M_ROW + i + 0);
                __m256 neg80 = _mm256_load_ps(X + row_index[k + 17] * M_ROW + i + 0);
                __m256 pos90 = _mm256_load_ps(X + row_index[k + 18] * M_ROW + i + 0);
                __m256 neg90 = _mm256_load_ps(X + row_index[k + 19] * M_ROW + i + 0);
                __m256 pos100 = _mm256_load_ps(X + row_index[k + 20] * M_ROW + i + 0);
                __m256 neg100 = _mm256_load_ps(X + row_index[k + 21] * M_ROW + i + 0);
                __m256 pos110 = _mm256_load_ps(X + row_index[k + 22] * M_ROW + i + 0);
                __m256 neg110 = _mm256_load_ps(X + row_index[k + 23] * M_ROW + i + 0);
                __m256 pos120 = _mm256_load_ps(X + row_index[k + 24] * M_ROW + i + 0);
                __m256 neg120 = _mm256_load_ps(X + row_index[k + 25] * M_ROW + i + 0);
                __m256 pos130 = _mm256_load_ps(X + row_index[k + 26] * M_ROW + i + 0);
                __m256 neg130 = _mm256_load_ps(X + row_index[k + 27] * M_ROW + i + 0);
                __m256 pos140 = _mm256_load_ps(X + row_index[k + 28] * M_ROW + i + 0);
                __m256 neg140 = _mm256_load_ps(X + row_index[k + 29] * M_ROW + i + 0);
                __m256 pos150 = _mm256_load_ps(X + row_index[k + 30] * M_ROW + i + 0);
                __m256 neg150 = _mm256_load_ps(X + row_index[k + 31] * M_ROW + i + 0);
                __m256 pos160 = _mm256_load_ps(X + row_index[k + 32] * M_ROW + i + 0);
                __m256 neg160 = _mm256_load_ps(X + row_index[k + 33] * M_ROW + i + 0);
                __m256 pos170 = _mm256_load_ps(X + row_index[k + 34] * M_ROW + i + 0);
                __m256 neg170 = _mm256_load_ps(X + row_index[k + 35] * M_ROW + i + 0);
                __m256 pos180 = _mm256_load_ps(X + row_index[k + 36] * M_ROW + i + 0);
                __m256 neg180 = _mm256_load_ps(X + row_index[k + 37] * M_ROW + i + 0);
                __m256 pos190 = _mm256_load_ps(X + row_index[k + 38] * M_ROW + i + 0);
                __m256 neg190 = _mm256_load_ps(X + row_index[k + 39] * M_ROW + i + 0);
                __m256 pos200 = _mm256_load_ps(X + row_index[k + 40] * M_ROW + i + 0);
                __m256 neg200 = _mm256_load_ps(X + row_index[k + 41] * M_ROW + i + 0);
                __m256 pos210 = _mm256_load_ps(X + row_index[k + 42] * M_ROW + i + 0);
                __m256 neg210 = _mm256_load_ps(X + row_index[k + 43] * M_ROW + i + 0);
                __m256 pos220 = _mm256_load_ps(X + row_index[k + 44] * M_ROW + i + 0);
                __m256 neg220 = _mm256_load_ps(X + row_index[k + 45] * M_ROW + i + 0);
                __m256 pos230 = _mm256_load_ps(X + row_index[k + 46] * M_ROW + i + 0);
                __m256 neg230 = _mm256_load_ps(X + row_index[k + 47] * M_ROW + i + 0);
                __m256 pos240 = _mm256_load_ps(X + row_index[k + 48] * M_ROW + i + 0);
                __m256 neg240 = _mm256_load_ps(X + row_index[k + 49] * M_ROW + i + 0);
                __m256 pos250 = _mm256_load_ps(X + row_index[k + 50] * M_ROW + i + 0);
                __m256 neg250 = _mm256_load_ps(X + row_index[k + 51] * M_ROW + i + 0);
                __m256 pos260 = _mm256_load_ps(X + row_index[k + 52] * M_ROW + i + 0);
                __m256 neg260 = _mm256_load_ps(X + row_index[k + 53] * M_ROW + i + 0);
                __m256 pos270 = _mm256_load_ps(X + row_index[k + 54] * M_ROW + i + 0);
                __m256 neg270 = _mm256_load_ps(X + row_index[k + 55] * M_ROW + i + 0);
                __m256 pos280 = _mm256_load_ps(X + row_index[k + 56] * M_ROW + i + 0);
                __m256 neg280 = _mm256_load_ps(X + row_index[k + 57] * M_ROW + i + 0);
                __m256 pos290 = _mm256_load_ps(X + row_index[k + 58] * M_ROW + i + 0);
                __m256 neg290 = _mm256_load_ps(X + row_index[k + 59] * M_ROW + i + 0);
                __m256 pos300 = _mm256_load_ps(X + row_index[k + 60] * M_ROW + i + 0);
                __m256 neg300 = _mm256_load_ps(X + row_index[k + 61] * M_ROW + i + 0);
                __m256 pos310 = _mm256_load_ps(X + row_index[k + 62] * M_ROW + i + 0);
                __m256 neg310 = _mm256_load_ps(X + row_index[k + 63] * M_ROW + i + 0);
                __m256 pos01 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 8);
                __m256 neg01 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 8);
                __m256 pos11 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 8);
                __m256 neg11 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 8);
                __m256 pos21 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 8);
                __m256 neg21 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 8);
                __m256 pos31 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 8);
                __m256 neg31 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 8);
                __m256 pos41 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 8);
                __m256 neg41 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 8);
                __m256 pos51 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 8);
                __m256 neg51 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 8);
                __m256 pos61 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 8);
                __m256 neg61 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 8);
                __m256 pos71 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 8);
                __m256 neg71 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 8);
                __m256 pos81 = _mm256_load_ps(X + row_index[k + 16] * M_ROW + i + 8);
                __m256 neg81 = _mm256_load_ps(X + row_index[k + 17] * M_ROW + i + 8);
                __m256 pos91 = _mm256_load_ps(X + row_index[k + 18] * M_ROW + i + 8);
                __m256 neg91 = _mm256_load_ps(X + row_index[k + 19] * M_ROW + i + 8);
                __m256 pos101 = _mm256_load_ps(X + row_index[k + 20] * M_ROW + i + 8);
                __m256 neg101 = _mm256_load_ps(X + row_index[k + 21] * M_ROW + i + 8);
                __m256 pos111 = _mm256_load_ps(X + row_index[k + 22] * M_ROW + i + 8);
                __m256 neg111 = _mm256_load_ps(X + row_index[k + 23] * M_ROW + i + 8);
                __m256 pos121 = _mm256_load_ps(X + row_index[k + 24] * M_ROW + i + 8);
                __m256 neg121 = _mm256_load_ps(X + row_index[k + 25] * M_ROW + i + 8);
                __m256 pos131 = _mm256_load_ps(X + row_index[k + 26] * M_ROW + i + 8);
                __m256 neg131 = _mm256_load_ps(X + row_index[k + 27] * M_ROW + i + 8);
                __m256 pos141 = _mm256_load_ps(X + row_index[k + 28] * M_ROW + i + 8);
                __m256 neg141 = _mm256_load_ps(X + row_index[k + 29] * M_ROW + i + 8);
                __m256 pos151 = _mm256_load_ps(X + row_index[k + 30] * M_ROW + i + 8);
                __m256 neg151 = _mm256_load_ps(X + row_index[k + 31] * M_ROW + i + 8);
                __m256 pos161 = _mm256_load_ps(X + row_index[k + 32] * M_ROW + i + 8);
                __m256 neg161 = _mm256_load_ps(X + row_index[k + 33] * M_ROW + i + 8);
                __m256 pos171 = _mm256_load_ps(X + row_index[k + 34] * M_ROW + i + 8);
                __m256 neg171 = _mm256_load_ps(X + row_index[k + 35] * M_ROW + i + 8);
                __m256 pos181 = _mm256_load_ps(X + row_index[k + 36] * M_ROW + i + 8);
                __m256 neg181 = _mm256_load_ps(X + row_index[k + 37] * M_ROW + i + 8);
                __m256 pos191 = _mm256_load_ps(X + row_index[k + 38] * M_ROW + i + 8);
                __m256 neg191 = _mm256_load_ps(X + row_index[k + 39] * M_ROW + i + 8);
                __m256 pos201 = _mm256_load_ps(X + row_index[k + 40] * M_ROW + i + 8);
                __m256 neg201 = _mm256_load_ps(X + row_index[k + 41] * M_ROW + i + 8);
                __m256 pos211 = _mm256_load_ps(X + row_index[k + 42] * M_ROW + i + 8);
                __m256 neg211 = _mm256_load_ps(X + row_index[k + 43] * M_ROW + i + 8);
                __m256 pos221 = _mm256_load_ps(X + row_index[k + 44] * M_ROW + i + 8);
                __m256 neg221 = _mm256_load_ps(X + row_index[k + 45] * M_ROW + i + 8);
                __m256 pos231 = _mm256_load_ps(X + row_index[k + 46] * M_ROW + i + 8);
                __m256 neg231 = _mm256_load_ps(X + row_index[k + 47] * M_ROW + i + 8);
                __m256 pos241 = _mm256_load_ps(X + row_index[k + 48] * M_ROW + i + 8);
                __m256 neg241 = _mm256_load_ps(X + row_index[k + 49] * M_ROW + i + 8);
                __m256 pos251 = _mm256_load_ps(X + row_index[k + 50] * M_ROW + i + 8);
                __m256 neg251 = _mm256_load_ps(X + row_index[k + 51] * M_ROW + i + 8);
                __m256 pos261 = _mm256_load_ps(X + row_index[k + 52] * M_ROW + i + 8);
                __m256 neg261 = _mm256_load_ps(X + row_index[k + 53] * M_ROW + i + 8);
                __m256 pos271 = _mm256_load_ps(X + row_index[k + 54] * M_ROW + i + 8);
                __m256 neg271 = _mm256_load_ps(X + row_index[k + 55] * M_ROW + i + 8);
                __m256 pos281 = _mm256_load_ps(X + row_index[k + 56] * M_ROW + i + 8);
                __m256 neg281 = _mm256_load_ps(X + row_index[k + 57] * M_ROW + i + 8);
                __m256 pos291 = _mm256_load_ps(X + row_index[k + 58] * M_ROW + i + 8);
                __m256 neg291 = _mm256_load_ps(X + row_index[k + 59] * M_ROW + i + 8);
                __m256 pos301 = _mm256_load_ps(X + row_index[k + 60] * M_ROW + i + 8);
                __m256 neg301 = _mm256_load_ps(X + row_index[k + 61] * M_ROW + i + 8);
                __m256 pos311 = _mm256_load_ps(X + row_index[k + 62] * M_ROW + i + 8);
                __m256 neg311 = _mm256_load_ps(X + row_index[k + 63] * M_ROW + i + 8);
                __m256 pos02 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 16);
                __m256 neg02 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 16);
                __m256 pos12 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 16);
                __m256 neg12 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 16);
                __m256 pos22 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 16);
                __m256 neg22 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 16);
                __m256 pos32 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 16);
                __m256 neg32 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 16);
                __m256 pos42 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 16);
                __m256 neg42 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 16);
                __m256 pos52 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 16);
                __m256 neg52 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 16);
                __m256 pos62 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 16);
                __m256 neg62 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 16);
                __m256 pos72 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 16);
                __m256 neg72 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 16);
                __m256 pos82 = _mm256_load_ps(X + row_index[k + 16] * M_ROW + i + 16);
                __m256 neg82 = _mm256_load_ps(X + row_index[k + 17] * M_ROW + i + 16);
                __m256 pos92 = _mm256_load_ps(X + row_index[k + 18] * M_ROW + i + 16);
                __m256 neg92 = _mm256_load_ps(X + row_index[k + 19] * M_ROW + i + 16);
                __m256 pos102 = _mm256_load_ps(X + row_index[k + 20] * M_ROW + i + 16);
                __m256 neg102 = _mm256_load_ps(X + row_index[k + 21] * M_ROW + i + 16);
                __m256 pos112 = _mm256_load_ps(X + row_index[k + 22] * M_ROW + i + 16);
                __m256 neg112 = _mm256_load_ps(X + row_index[k + 23] * M_ROW + i + 16);
                __m256 pos122 = _mm256_load_ps(X + row_index[k + 24] * M_ROW + i + 16);
                __m256 neg122 = _mm256_load_ps(X + row_index[k + 25] * M_ROW + i + 16);
                __m256 pos132 = _mm256_load_ps(X + row_index[k + 26] * M_ROW + i + 16);
                __m256 neg132 = _mm256_load_ps(X + row_index[k + 27] * M_ROW + i + 16);
                __m256 pos142 = _mm256_load_ps(X + row_index[k + 28] * M_ROW + i + 16);
                __m256 neg142 = _mm256_load_ps(X + row_index[k + 29] * M_ROW + i + 16);
                __m256 pos152 = _mm256_load_ps(X + row_index[k + 30] * M_ROW + i + 16);
                __m256 neg152 = _mm256_load_ps(X + row_index[k + 31] * M_ROW + i + 16);
                __m256 pos162 = _mm256_load_ps(X + row_index[k + 32] * M_ROW + i + 16);
                __m256 neg162 = _mm256_load_ps(X + row_index[k + 33] * M_ROW + i + 16);
                __m256 pos172 = _mm256_load_ps(X + row_index[k + 34] * M_ROW + i + 16);
                __m256 neg172 = _mm256_load_ps(X + row_index[k + 35] * M_ROW + i + 16);
                __m256 pos182 = _mm256_load_ps(X + row_index[k + 36] * M_ROW + i + 16);
                __m256 neg182 = _mm256_load_ps(X + row_index[k + 37] * M_ROW + i + 16);
                __m256 pos192 = _mm256_load_ps(X + row_index[k + 38] * M_ROW + i + 16);
                __m256 neg192 = _mm256_load_ps(X + row_index[k + 39] * M_ROW + i + 16);
                __m256 pos202 = _mm256_load_ps(X + row_index[k + 40] * M_ROW + i + 16);
                __m256 neg202 = _mm256_load_ps(X + row_index[k + 41] * M_ROW + i + 16);
                __m256 pos212 = _mm256_load_ps(X + row_index[k + 42] * M_ROW + i + 16);
                __m256 neg212 = _mm256_load_ps(X + row_index[k + 43] * M_ROW + i + 16);
                __m256 pos222 = _mm256_load_ps(X + row_index[k + 44] * M_ROW + i + 16);
                __m256 neg222 = _mm256_load_ps(X + row_index[k + 45] * M_ROW + i + 16);
                __m256 pos232 = _mm256_load_ps(X + row_index[k + 46] * M_ROW + i + 16);
                __m256 neg232 = _mm256_load_ps(X + row_index[k + 47] * M_ROW + i + 16);
                __m256 pos242 = _mm256_load_ps(X + row_index[k + 48] * M_ROW + i + 16);
                __m256 neg242 = _mm256_load_ps(X + row_index[k + 49] * M_ROW + i + 16);
                __m256 pos252 = _mm256_load_ps(X + row_index[k + 50] * M_ROW + i + 16);
                __m256 neg252 = _mm256_load_ps(X + row_index[k + 51] * M_ROW + i + 16);
                __m256 pos262 = _mm256_load_ps(X + row_index[k + 52] * M_ROW + i + 16);
                __m256 neg262 = _mm256_load_ps(X + row_index[k + 53] * M_ROW + i + 16);
                __m256 pos272 = _mm256_load_ps(X + row_index[k + 54] * M_ROW + i + 16);
                __m256 neg272 = _mm256_load_ps(X + row_index[k + 55] * M_ROW + i + 16);
                __m256 pos282 = _mm256_load_ps(X + row_index[k + 56] * M_ROW + i + 16);
                __m256 neg282 = _mm256_load_ps(X + row_index[k + 57] * M_ROW + i + 16);
                __m256 pos292 = _mm256_load_ps(X + row_index[k + 58] * M_ROW + i + 16);
                __m256 neg292 = _mm256_load_ps(X + row_index[k + 59] * M_ROW + i + 16);
                __m256 pos302 = _mm256_load_ps(X + row_index[k + 60] * M_ROW + i + 16);
                __m256 neg302 = _mm256_load_ps(X + row_index[k + 61] * M_ROW + i + 16);
                __m256 pos312 = _mm256_load_ps(X + row_index[k + 62] * M_ROW + i + 16);
                __m256 neg312 = _mm256_load_ps(X + row_index[k + 63] * M_ROW + i + 16);
                __m256 pos03 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 24);
                __m256 neg03 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 24);
                __m256 pos13 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 24);
                __m256 neg13 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 24);
                __m256 pos23 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 24);
                __m256 neg23 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 24);
                __m256 pos33 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 24);
                __m256 neg33 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 24);
                __m256 pos43 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 24);
                __m256 neg43 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 24);
                __m256 pos53 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 24);
                __m256 neg53 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 24);
                __m256 pos63 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 24);
                __m256 neg63 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 24);
                __m256 pos73 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 24);
                __m256 neg73 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 24);
                __m256 pos83 = _mm256_load_ps(X + row_index[k + 16] * M_ROW + i + 24);
                __m256 neg83 = _mm256_load_ps(X + row_index[k + 17] * M_ROW + i + 24);
                __m256 pos93 = _mm256_load_ps(X + row_index[k + 18] * M_ROW + i + 24);
                __m256 neg93 = _mm256_load_ps(X + row_index[k + 19] * M_ROW + i + 24);
                __m256 pos103 = _mm256_load_ps(X + row_index[k + 20] * M_ROW + i + 24);
                __m256 neg103 = _mm256_load_ps(X + row_index[k + 21] * M_ROW + i + 24);
                __m256 pos113 = _mm256_load_ps(X + row_index[k + 22] * M_ROW + i + 24);
                __m256 neg113 = _mm256_load_ps(X + row_index[k + 23] * M_ROW + i + 24);
                __m256 pos123 = _mm256_load_ps(X + row_index[k + 24] * M_ROW + i + 24);
                __m256 neg123 = _mm256_load_ps(X + row_index[k + 25] * M_ROW + i + 24);
                __m256 pos133 = _mm256_load_ps(X + row_index[k + 26] * M_ROW + i + 24);
                __m256 neg133 = _mm256_load_ps(X + row_index[k + 27] * M_ROW + i + 24);
                __m256 pos143 = _mm256_load_ps(X + row_index[k + 28] * M_ROW + i + 24);
                __m256 neg143 = _mm256_load_ps(X + row_index[k + 29] * M_ROW + i + 24);
                __m256 pos153 = _mm256_load_ps(X + row_index[k + 30] * M_ROW + i + 24);
                __m256 neg153 = _mm256_load_ps(X + row_index[k + 31] * M_ROW + i + 24);
                __m256 pos163 = _mm256_load_ps(X + row_index[k + 32] * M_ROW + i + 24);
                __m256 neg163 = _mm256_load_ps(X + row_index[k + 33] * M_ROW + i + 24);
                __m256 pos173 = _mm256_load_ps(X + row_index[k + 34] * M_ROW + i + 24);
                __m256 neg173 = _mm256_load_ps(X + row_index[k + 35] * M_ROW + i + 24);
                __m256 pos183 = _mm256_load_ps(X + row_index[k + 36] * M_ROW + i + 24);
                __m256 neg183 = _mm256_load_ps(X + row_index[k + 37] * M_ROW + i + 24);
                __m256 pos193 = _mm256_load_ps(X + row_index[k + 38] * M_ROW + i + 24);
                __m256 neg193 = _mm256_load_ps(X + row_index[k + 39] * M_ROW + i + 24);
                __m256 pos203 = _mm256_load_ps(X + row_index[k + 40] * M_ROW + i + 24);
                __m256 neg203 = _mm256_load_ps(X + row_index[k + 41] * M_ROW + i + 24);
                __m256 pos213 = _mm256_load_ps(X + row_index[k + 42] * M_ROW + i + 24);
                __m256 neg213 = _mm256_load_ps(X + row_index[k + 43] * M_ROW + i + 24);
                __m256 pos223 = _mm256_load_ps(X + row_index[k + 44] * M_ROW + i + 24);
                __m256 neg223 = _mm256_load_ps(X + row_index[k + 45] * M_ROW + i + 24);
                __m256 pos233 = _mm256_load_ps(X + row_index[k + 46] * M_ROW + i + 24);
                __m256 neg233 = _mm256_load_ps(X + row_index[k + 47] * M_ROW + i + 24);
                __m256 pos243 = _mm256_load_ps(X + row_index[k + 48] * M_ROW + i + 24);
                __m256 neg243 = _mm256_load_ps(X + row_index[k + 49] * M_ROW + i + 24);
                __m256 pos253 = _mm256_load_ps(X + row_index[k + 50] * M_ROW + i + 24);
                __m256 neg253 = _mm256_load_ps(X + row_index[k + 51] * M_ROW + i + 24);
                __m256 pos263 = _mm256_load_ps(X + row_index[k + 52] * M_ROW + i + 24);
                __m256 neg263 = _mm256_load_ps(X + row_index[k + 53] * M_ROW + i + 24);
                __m256 pos273 = _mm256_load_ps(X + row_index[k + 54] * M_ROW + i + 24);
                __m256 neg273 = _mm256_load_ps(X + row_index[k + 55] * M_ROW + i + 24);
                __m256 pos283 = _mm256_load_ps(X + row_index[k + 56] * M_ROW + i + 24);
                __m256 neg283 = _mm256_load_ps(X + row_index[k + 57] * M_ROW + i + 24);
                __m256 pos293 = _mm256_load_ps(X + row_index[k + 58] * M_ROW + i + 24);
                __m256 neg293 = _mm256_load_ps(X + row_index[k + 59] * M_ROW + i + 24);
                __m256 pos303 = _mm256_load_ps(X + row_index[k + 60] * M_ROW + i + 24);
                __m256 neg303 = _mm256_load_ps(X + row_index[k + 61] * M_ROW + i + 24);
                __m256 pos313 = _mm256_load_ps(X + row_index[k + 62] * M_ROW + i + 24);
                __m256 neg313 = _mm256_load_ps(X + row_index[k + 63] * M_ROW + i + 24);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res20 = _mm256_add_ps(res20, _mm256_sub_ps(pos20, neg20));
                res30 = _mm256_add_ps(res30, _mm256_sub_ps(pos30, neg30));
                res40 = _mm256_add_ps(res40, _mm256_sub_ps(pos40, neg40));
                res50 = _mm256_add_ps(res50, _mm256_sub_ps(pos50, neg50));
                res60 = _mm256_add_ps(res60, _mm256_sub_ps(pos60, neg60));
                res70 = _mm256_add_ps(res70, _mm256_sub_ps(pos70, neg70));
                res80 = _mm256_add_ps(res80, _mm256_sub_ps(pos80, neg80));
                res90 = _mm256_add_ps(res90, _mm256_sub_ps(pos90, neg90));
                res100 = _mm256_add_ps(res100, _mm256_sub_ps(pos100, neg100));
                res110 = _mm256_add_ps(res110, _mm256_sub_ps(pos110, neg110));
                res120 = _mm256_add_ps(res120, _mm256_sub_ps(pos120, neg120));
                res130 = _mm256_add_ps(res130, _mm256_sub_ps(pos130, neg130));
                res140 = _mm256_add_ps(res140, _mm256_sub_ps(pos140, neg140));
                res150 = _mm256_add_ps(res150, _mm256_sub_ps(pos150, neg150));
                res160 = _mm256_add_ps(res160, _mm256_sub_ps(pos160, neg160));
                res170 = _mm256_add_ps(res170, _mm256_sub_ps(pos170, neg170));
                res180 = _mm256_add_ps(res180, _mm256_sub_ps(pos180, neg180));
                res190 = _mm256_add_ps(res190, _mm256_sub_ps(pos190, neg190));
                res200 = _mm256_add_ps(res200, _mm256_sub_ps(pos200, neg200));
                res210 = _mm256_add_ps(res210, _mm256_sub_ps(pos210, neg210));
                res220 = _mm256_add_ps(res220, _mm256_sub_ps(pos220, neg220));
                res230 = _mm256_add_ps(res230, _mm256_sub_ps(pos230, neg230));
                res240 = _mm256_add_ps(res240, _mm256_sub_ps(pos240, neg240));
                res250 = _mm256_add_ps(res250, _mm256_sub_ps(pos250, neg250));
                res260 = _mm256_add_ps(res260, _mm256_sub_ps(pos260, neg260));
                res270 = _mm256_add_ps(res270, _mm256_sub_ps(pos270, neg270));
                res280 = _mm256_add_ps(res280, _mm256_sub_ps(pos280, neg280));
                res290 = _mm256_add_ps(res290, _mm256_sub_ps(pos290, neg290));
                res300 = _mm256_add_ps(res300, _mm256_sub_ps(pos300, neg300));
                res310 = _mm256_add_ps(res310, _mm256_sub_ps(pos310, neg310));
                res01 = _mm256_add_ps(res01, _mm256_sub_ps(pos01, neg01));
                res11 = _mm256_add_ps(res11, _mm256_sub_ps(pos11, neg11));
                res21 = _mm256_add_ps(res21, _mm256_sub_ps(pos21, neg21));
                res31 = _mm256_add_ps(res31, _mm256_sub_ps(pos31, neg31));
                res41 = _mm256_add_ps(res41, _mm256_sub_ps(pos41, neg41));
                res51 = _mm256_add_ps(res51, _mm256_sub_ps(pos51, neg51));
                res61 = _mm256_add_ps(res61, _mm256_sub_ps(pos61, neg61));
                res71 = _mm256_add_ps(res71, _mm256_sub_ps(pos71, neg71));
                res81 = _mm256_add_ps(res81, _mm256_sub_ps(pos81, neg81));
                res91 = _mm256_add_ps(res91, _mm256_sub_ps(pos91, neg91));
                res101 = _mm256_add_ps(res101, _mm256_sub_ps(pos101, neg101));
                res111 = _mm256_add_ps(res111, _mm256_sub_ps(pos111, neg111));
                res121 = _mm256_add_ps(res121, _mm256_sub_ps(pos121, neg121));
                res131 = _mm256_add_ps(res131, _mm256_sub_ps(pos131, neg131));
                res141 = _mm256_add_ps(res141, _mm256_sub_ps(pos141, neg141));
                res151 = _mm256_add_ps(res151, _mm256_sub_ps(pos151, neg151));
                res161 = _mm256_add_ps(res161, _mm256_sub_ps(pos161, neg161));
                res171 = _mm256_add_ps(res171, _mm256_sub_ps(pos171, neg171));
                res181 = _mm256_add_ps(res181, _mm256_sub_ps(pos181, neg181));
                res191 = _mm256_add_ps(res191, _mm256_sub_ps(pos191, neg191));
                res201 = _mm256_add_ps(res201, _mm256_sub_ps(pos201, neg201));
                res211 = _mm256_add_ps(res211, _mm256_sub_ps(pos211, neg211));
                res221 = _mm256_add_ps(res221, _mm256_sub_ps(pos221, neg221));
                res231 = _mm256_add_ps(res231, _mm256_sub_ps(pos231, neg231));
                res241 = _mm256_add_ps(res241, _mm256_sub_ps(pos241, neg241));
                res251 = _mm256_add_ps(res251, _mm256_sub_ps(pos251, neg251));
                res261 = _mm256_add_ps(res261, _mm256_sub_ps(pos261, neg261));
                res271 = _mm256_add_ps(res271, _mm256_sub_ps(pos271, neg271));
                res281 = _mm256_add_ps(res281, _mm256_sub_ps(pos281, neg281));
                res291 = _mm256_add_ps(res291, _mm256_sub_ps(pos291, neg291));
                res301 = _mm256_add_ps(res301, _mm256_sub_ps(pos301, neg301));
                res311 = _mm256_add_ps(res311, _mm256_sub_ps(pos311, neg311));
                res02 = _mm256_add_ps(res02, _mm256_sub_ps(pos02, neg02));
                res12 = _mm256_add_ps(res12, _mm256_sub_ps(pos12, neg12));
                res22 = _mm256_add_ps(res22, _mm256_sub_ps(pos22, neg22));
                res32 = _mm256_add_ps(res32, _mm256_sub_ps(pos32, neg32));
                res42 = _mm256_add_ps(res42, _mm256_sub_ps(pos42, neg42));
                res52 = _mm256_add_ps(res52, _mm256_sub_ps(pos52, neg52));
                res62 = _mm256_add_ps(res62, _mm256_sub_ps(pos62, neg62));
                res72 = _mm256_add_ps(res72, _mm256_sub_ps(pos72, neg72));
                res82 = _mm256_add_ps(res82, _mm256_sub_ps(pos82, neg82));
                res92 = _mm256_add_ps(res92, _mm256_sub_ps(pos92, neg92));
                res102 = _mm256_add_ps(res102, _mm256_sub_ps(pos102, neg102));
                res112 = _mm256_add_ps(res112, _mm256_sub_ps(pos112, neg112));
                res122 = _mm256_add_ps(res122, _mm256_sub_ps(pos122, neg122));
                res132 = _mm256_add_ps(res132, _mm256_sub_ps(pos132, neg132));
                res142 = _mm256_add_ps(res142, _mm256_sub_ps(pos142, neg142));
                res152 = _mm256_add_ps(res152, _mm256_sub_ps(pos152, neg152));
                res162 = _mm256_add_ps(res162, _mm256_sub_ps(pos162, neg162));
                res172 = _mm256_add_ps(res172, _mm256_sub_ps(pos172, neg172));
                res182 = _mm256_add_ps(res182, _mm256_sub_ps(pos182, neg182));
                res192 = _mm256_add_ps(res192, _mm256_sub_ps(pos192, neg192));
                res202 = _mm256_add_ps(res202, _mm256_sub_ps(pos202, neg202));
                res212 = _mm256_add_ps(res212, _mm256_sub_ps(pos212, neg212));
                res222 = _mm256_add_ps(res222, _mm256_sub_ps(pos222, neg222));
                res232 = _mm256_add_ps(res232, _mm256_sub_ps(pos232, neg232));
                res242 = _mm256_add_ps(res242, _mm256_sub_ps(pos242, neg242));
                res252 = _mm256_add_ps(res252, _mm256_sub_ps(pos252, neg252));
                res262 = _mm256_add_ps(res262, _mm256_sub_ps(pos262, neg262));
                res272 = _mm256_add_ps(res272, _mm256_sub_ps(pos272, neg272));
                res282 = _mm256_add_ps(res282, _mm256_sub_ps(pos282, neg282));
                res292 = _mm256_add_ps(res292, _mm256_sub_ps(pos292, neg292));
                res302 = _mm256_add_ps(res302, _mm256_sub_ps(pos302, neg302));
                res312 = _mm256_add_ps(res312, _mm256_sub_ps(pos312, neg312));
                res03 = _mm256_add_ps(res03, _mm256_sub_ps(pos03, neg03));
                res13 = _mm256_add_ps(res13, _mm256_sub_ps(pos13, neg13));
                res23 = _mm256_add_ps(res23, _mm256_sub_ps(pos23, neg23));
                res33 = _mm256_add_ps(res33, _mm256_sub_ps(pos33, neg33));
                res43 = _mm256_add_ps(res43, _mm256_sub_ps(pos43, neg43));
                res53 = _mm256_add_ps(res53, _mm256_sub_ps(pos53, neg53));
                res63 = _mm256_add_ps(res63, _mm256_sub_ps(pos63, neg63));
                res73 = _mm256_add_ps(res73, _mm256_sub_ps(pos73, neg73));
                res83 = _mm256_add_ps(res83, _mm256_sub_ps(pos83, neg83));
                res93 = _mm256_add_ps(res93, _mm256_sub_ps(pos93, neg93));
                res103 = _mm256_add_ps(res103, _mm256_sub_ps(pos103, neg103));
                res113 = _mm256_add_ps(res113, _mm256_sub_ps(pos113, neg113));
                res123 = _mm256_add_ps(res123, _mm256_sub_ps(pos123, neg123));
                res133 = _mm256_add_ps(res133, _mm256_sub_ps(pos133, neg133));
                res143 = _mm256_add_ps(res143, _mm256_sub_ps(pos143, neg143));
                res153 = _mm256_add_ps(res153, _mm256_sub_ps(pos153, neg153));
                res163 = _mm256_add_ps(res163, _mm256_sub_ps(pos163, neg163));
                res173 = _mm256_add_ps(res173, _mm256_sub_ps(pos173, neg173));
                res183 = _mm256_add_ps(res183, _mm256_sub_ps(pos183, neg183));
                res193 = _mm256_add_ps(res193, _mm256_sub_ps(pos193, neg193));
                res203 = _mm256_add_ps(res203, _mm256_sub_ps(pos203, neg203));
                res213 = _mm256_add_ps(res213, _mm256_sub_ps(pos213, neg213));
                res223 = _mm256_add_ps(res223, _mm256_sub_ps(pos223, neg223));
                res233 = _mm256_add_ps(res233, _mm256_sub_ps(pos233, neg233));
                res243 = _mm256_add_ps(res243, _mm256_sub_ps(pos243, neg243));
                res253 = _mm256_add_ps(res253, _mm256_sub_ps(pos253, neg253));
                res263 = _mm256_add_ps(res263, _mm256_sub_ps(pos263, neg263));
                res273 = _mm256_add_ps(res273, _mm256_sub_ps(pos273, neg273));
                res283 = _mm256_add_ps(res283, _mm256_sub_ps(pos283, neg283));
                res293 = _mm256_add_ps(res293, _mm256_sub_ps(pos293, neg293));
                res303 = _mm256_add_ps(res303, _mm256_sub_ps(pos303, neg303));
                res313 = _mm256_add_ps(res313, _mm256_sub_ps(pos313, neg313));
            }
            for (int k = groupData[1]; k < groupData[2]; k++) {
                res00 = _mm256_add_ps(res00, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res01 = _mm256_add_ps(res01, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res02 = _mm256_add_ps(res02, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res03 = _mm256_add_ps(res03, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[2]; k < groupData[3]; k++) {
                res00 = _mm256_sub_ps(res00, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res01 = _mm256_sub_ps(res01, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res02 = _mm256_sub_ps(res02, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res03 = _mm256_sub_ps(res03, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[3]; k < groupData[4]; k++) {
                res10 = _mm256_add_ps(res10, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res11 = _mm256_add_ps(res11, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res12 = _mm256_add_ps(res12, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res13 = _mm256_add_ps(res13, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[4]; k < groupData[5]; k++) {
                res10 = _mm256_sub_ps(res10, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res11 = _mm256_sub_ps(res11, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res12 = _mm256_sub_ps(res12, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res13 = _mm256_sub_ps(res13, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[5]; k < groupData[6]; k++) {
                res20 = _mm256_add_ps(res20, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res21 = _mm256_add_ps(res21, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res22 = _mm256_add_ps(res22, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res23 = _mm256_add_ps(res23, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[6]; k < groupData[7]; k++) {
                res20 = _mm256_sub_ps(res20, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res21 = _mm256_sub_ps(res21, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res22 = _mm256_sub_ps(res22, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res23 = _mm256_sub_ps(res23, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[7]; k < groupData[8]; k++) {
                res30 = _mm256_add_ps(res30, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res31 = _mm256_add_ps(res31, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res32 = _mm256_add_ps(res32, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res33 = _mm256_add_ps(res33, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[8]; k < groupData[9]; k++) {
                res30 = _mm256_sub_ps(res30, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res31 = _mm256_sub_ps(res31, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res32 = _mm256_sub_ps(res32, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res33 = _mm256_sub_ps(res33, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[9]; k < groupData[10]; k++) {
                res40 = _mm256_add_ps(res40, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res41 = _mm256_add_ps(res41, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res42 = _mm256_add_ps(res42, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res43 = _mm256_add_ps(res43, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[10]; k < groupData[11]; k++) {
                res40 = _mm256_sub_ps(res40, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res41 = _mm256_sub_ps(res41, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res42 = _mm256_sub_ps(res42, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res43 = _mm256_sub_ps(res43, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[11]; k < groupData[12]; k++) {
                res50 = _mm256_add_ps(res50, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res51 = _mm256_add_ps(res51, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res52 = _mm256_add_ps(res52, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res53 = _mm256_add_ps(res53, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[12]; k < groupData[13]; k++) {
                res50 = _mm256_sub_ps(res50, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res51 = _mm256_sub_ps(res51, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res52 = _mm256_sub_ps(res52, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res53 = _mm256_sub_ps(res53, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[13]; k < groupData[14]; k++) {
                res60 = _mm256_add_ps(res60, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res61 = _mm256_add_ps(res61, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res62 = _mm256_add_ps(res62, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res63 = _mm256_add_ps(res63, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[14]; k < groupData[15]; k++) {
                res60 = _mm256_sub_ps(res60, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res61 = _mm256_sub_ps(res61, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res62 = _mm256_sub_ps(res62, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res63 = _mm256_sub_ps(res63, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[15]; k < groupData[16]; k++) {
                res70 = _mm256_add_ps(res70, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res71 = _mm256_add_ps(res71, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res72 = _mm256_add_ps(res72, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res73 = _mm256_add_ps(res73, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[16]; k < groupData[17]; k++) {
                res70 = _mm256_sub_ps(res70, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res71 = _mm256_sub_ps(res71, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res72 = _mm256_sub_ps(res72, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res73 = _mm256_sub_ps(res73, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[17]; k < groupData[18]; k++) {
                res80 = _mm256_add_ps(res80, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res81 = _mm256_add_ps(res81, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res82 = _mm256_add_ps(res82, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res83 = _mm256_add_ps(res83, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[18]; k < groupData[19]; k++) {
                res80 = _mm256_sub_ps(res80, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res81 = _mm256_sub_ps(res81, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res82 = _mm256_sub_ps(res82, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res83 = _mm256_sub_ps(res83, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[19]; k < groupData[20]; k++) {
                res90 = _mm256_add_ps(res90, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res91 = _mm256_add_ps(res91, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res92 = _mm256_add_ps(res92, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res93 = _mm256_add_ps(res93, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[20]; k < groupData[21]; k++) {
                res90 = _mm256_sub_ps(res90, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res91 = _mm256_sub_ps(res91, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res92 = _mm256_sub_ps(res92, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res93 = _mm256_sub_ps(res93, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[21]; k < groupData[22]; k++) {
                res100 = _mm256_add_ps(res100, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res101 = _mm256_add_ps(res101, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res102 = _mm256_add_ps(res102, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res103 = _mm256_add_ps(res103, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[22]; k < groupData[23]; k++) {
                res100 = _mm256_sub_ps(res100, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res101 = _mm256_sub_ps(res101, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res102 = _mm256_sub_ps(res102, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res103 = _mm256_sub_ps(res103, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[23]; k < groupData[24]; k++) {
                res110 = _mm256_add_ps(res110, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res111 = _mm256_add_ps(res111, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res112 = _mm256_add_ps(res112, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res113 = _mm256_add_ps(res113, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[24]; k < groupData[25]; k++) {
                res110 = _mm256_sub_ps(res110, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res111 = _mm256_sub_ps(res111, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res112 = _mm256_sub_ps(res112, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res113 = _mm256_sub_ps(res113, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[25]; k < groupData[26]; k++) {
                res120 = _mm256_add_ps(res120, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res121 = _mm256_add_ps(res121, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res122 = _mm256_add_ps(res122, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res123 = _mm256_add_ps(res123, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[26]; k < groupData[27]; k++) {
                res120 = _mm256_sub_ps(res120, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res121 = _mm256_sub_ps(res121, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res122 = _mm256_sub_ps(res122, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res123 = _mm256_sub_ps(res123, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[27]; k < groupData[28]; k++) {
                res130 = _mm256_add_ps(res130, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res131 = _mm256_add_ps(res131, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res132 = _mm256_add_ps(res132, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res133 = _mm256_add_ps(res133, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[28]; k < groupData[29]; k++) {
                res130 = _mm256_sub_ps(res130, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res131 = _mm256_sub_ps(res131, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res132 = _mm256_sub_ps(res132, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res133 = _mm256_sub_ps(res133, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[29]; k < groupData[30]; k++) {
                res140 = _mm256_add_ps(res140, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res141 = _mm256_add_ps(res141, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res142 = _mm256_add_ps(res142, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res143 = _mm256_add_ps(res143, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[30]; k < groupData[31]; k++) {
                res140 = _mm256_sub_ps(res140, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res141 = _mm256_sub_ps(res141, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res142 = _mm256_sub_ps(res142, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res143 = _mm256_sub_ps(res143, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[31]; k < groupData[32]; k++) {
                res150 = _mm256_add_ps(res150, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res151 = _mm256_add_ps(res151, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res152 = _mm256_add_ps(res152, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res153 = _mm256_add_ps(res153, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[32]; k < groupData[33]; k++) {
                res150 = _mm256_sub_ps(res150, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res151 = _mm256_sub_ps(res151, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res152 = _mm256_sub_ps(res152, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res153 = _mm256_sub_ps(res153, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[33]; k < groupData[34]; k++) {
                res160 = _mm256_add_ps(res160, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res161 = _mm256_add_ps(res161, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res162 = _mm256_add_ps(res162, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res163 = _mm256_add_ps(res163, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[34]; k < groupData[35]; k++) {
                res160 = _mm256_sub_ps(res160, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res161 = _mm256_sub_ps(res161, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res162 = _mm256_sub_ps(res162, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res163 = _mm256_sub_ps(res163, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[35]; k < groupData[36]; k++) {
                res170 = _mm256_add_ps(res170, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res171 = _mm256_add_ps(res171, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res172 = _mm256_add_ps(res172, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res173 = _mm256_add_ps(res173, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[36]; k < groupData[37]; k++) {
                res170 = _mm256_sub_ps(res170, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res171 = _mm256_sub_ps(res171, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res172 = _mm256_sub_ps(res172, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res173 = _mm256_sub_ps(res173, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[37]; k < groupData[38]; k++) {
                res180 = _mm256_add_ps(res180, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res181 = _mm256_add_ps(res181, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res182 = _mm256_add_ps(res182, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res183 = _mm256_add_ps(res183, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[38]; k < groupData[39]; k++) {
                res180 = _mm256_sub_ps(res180, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res181 = _mm256_sub_ps(res181, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res182 = _mm256_sub_ps(res182, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res183 = _mm256_sub_ps(res183, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[39]; k < groupData[40]; k++) {
                res190 = _mm256_add_ps(res190, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res191 = _mm256_add_ps(res191, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res192 = _mm256_add_ps(res192, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res193 = _mm256_add_ps(res193, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[40]; k < groupData[41]; k++) {
                res190 = _mm256_sub_ps(res190, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res191 = _mm256_sub_ps(res191, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res192 = _mm256_sub_ps(res192, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res193 = _mm256_sub_ps(res193, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[41]; k < groupData[42]; k++) {
                res200 = _mm256_add_ps(res200, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res201 = _mm256_add_ps(res201, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res202 = _mm256_add_ps(res202, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res203 = _mm256_add_ps(res203, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[42]; k < groupData[43]; k++) {
                res200 = _mm256_sub_ps(res200, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res201 = _mm256_sub_ps(res201, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res202 = _mm256_sub_ps(res202, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res203 = _mm256_sub_ps(res203, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[43]; k < groupData[44]; k++) {
                res210 = _mm256_add_ps(res210, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res211 = _mm256_add_ps(res211, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res212 = _mm256_add_ps(res212, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res213 = _mm256_add_ps(res213, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[44]; k < groupData[45]; k++) {
                res210 = _mm256_sub_ps(res210, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res211 = _mm256_sub_ps(res211, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res212 = _mm256_sub_ps(res212, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res213 = _mm256_sub_ps(res213, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[45]; k < groupData[46]; k++) {
                res220 = _mm256_add_ps(res220, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res221 = _mm256_add_ps(res221, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res222 = _mm256_add_ps(res222, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res223 = _mm256_add_ps(res223, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[46]; k < groupData[47]; k++) {
                res220 = _mm256_sub_ps(res220, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res221 = _mm256_sub_ps(res221, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res222 = _mm256_sub_ps(res222, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res223 = _mm256_sub_ps(res223, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[47]; k < groupData[48]; k++) {
                res230 = _mm256_add_ps(res230, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res231 = _mm256_add_ps(res231, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res232 = _mm256_add_ps(res232, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res233 = _mm256_add_ps(res233, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[48]; k < groupData[49]; k++) {
                res230 = _mm256_sub_ps(res230, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res231 = _mm256_sub_ps(res231, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res232 = _mm256_sub_ps(res232, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res233 = _mm256_sub_ps(res233, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[49]; k < groupData[50]; k++) {
                res240 = _mm256_add_ps(res240, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res241 = _mm256_add_ps(res241, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res242 = _mm256_add_ps(res242, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res243 = _mm256_add_ps(res243, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[50]; k < groupData[51]; k++) {
                res240 = _mm256_sub_ps(res240, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res241 = _mm256_sub_ps(res241, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res242 = _mm256_sub_ps(res242, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res243 = _mm256_sub_ps(res243, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[51]; k < groupData[52]; k++) {
                res250 = _mm256_add_ps(res250, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res251 = _mm256_add_ps(res251, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res252 = _mm256_add_ps(res252, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res253 = _mm256_add_ps(res253, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[52]; k < groupData[53]; k++) {
                res250 = _mm256_sub_ps(res250, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res251 = _mm256_sub_ps(res251, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res252 = _mm256_sub_ps(res252, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res253 = _mm256_sub_ps(res253, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[53]; k < groupData[54]; k++) {
                res260 = _mm256_add_ps(res260, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res261 = _mm256_add_ps(res261, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res262 = _mm256_add_ps(res262, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res263 = _mm256_add_ps(res263, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[54]; k < groupData[55]; k++) {
                res260 = _mm256_sub_ps(res260, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res261 = _mm256_sub_ps(res261, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res262 = _mm256_sub_ps(res262, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res263 = _mm256_sub_ps(res263, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[55]; k < groupData[56]; k++) {
                res270 = _mm256_add_ps(res270, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res271 = _mm256_add_ps(res271, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res272 = _mm256_add_ps(res272, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res273 = _mm256_add_ps(res273, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[56]; k < groupData[57]; k++) {
                res270 = _mm256_sub_ps(res270, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res271 = _mm256_sub_ps(res271, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res272 = _mm256_sub_ps(res272, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res273 = _mm256_sub_ps(res273, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[57]; k < groupData[58]; k++) {
                res280 = _mm256_add_ps(res280, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res281 = _mm256_add_ps(res281, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res282 = _mm256_add_ps(res282, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res283 = _mm256_add_ps(res283, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[58]; k < groupData[59]; k++) {
                res280 = _mm256_sub_ps(res280, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res281 = _mm256_sub_ps(res281, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res282 = _mm256_sub_ps(res282, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res283 = _mm256_sub_ps(res283, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[59]; k < groupData[60]; k++) {
                res290 = _mm256_add_ps(res290, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res291 = _mm256_add_ps(res291, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res292 = _mm256_add_ps(res292, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res293 = _mm256_add_ps(res293, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[60]; k < groupData[61]; k++) {
                res290 = _mm256_sub_ps(res290, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res291 = _mm256_sub_ps(res291, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res292 = _mm256_sub_ps(res292, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res293 = _mm256_sub_ps(res293, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[61]; k < groupData[62]; k++) {
                res300 = _mm256_add_ps(res300, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res301 = _mm256_add_ps(res301, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res302 = _mm256_add_ps(res302, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res303 = _mm256_add_ps(res303, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[62]; k < groupData[63]; k++) {
                res300 = _mm256_sub_ps(res300, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res301 = _mm256_sub_ps(res301, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res302 = _mm256_sub_ps(res302, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res303 = _mm256_sub_ps(res303, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[63]; k < groupData[64]; k++) {
                res310 = _mm256_add_ps(res310, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res311 = _mm256_add_ps(res311, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res312 = _mm256_add_ps(res312, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res313 = _mm256_add_ps(res313, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            for (int k = groupData[64]; k < groupData[65]; k++) {
                res310 = _mm256_sub_ps(res310, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res311 = _mm256_sub_ps(res311, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res312 = _mm256_sub_ps(res312, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res313 = _mm256_sub_ps(res313, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
            }
            _mm256_store_ps(result + (j * 32 + 0) * M_ROW + i + 0, res00);
            _mm256_store_ps(result + (j * 32 + 1) * M_ROW + i + 0, res10);
            _mm256_store_ps(result + (j * 32 + 2) * M_ROW + i + 0, res20);
            _mm256_store_ps(result + (j * 32 + 3) * M_ROW + i + 0, res30);
            _mm256_store_ps(result + (j * 32 + 4) * M_ROW + i + 0, res40);
            _mm256_store_ps(result + (j * 32 + 5) * M_ROW + i + 0, res50);
            _mm256_store_ps(result + (j * 32 + 6) * M_ROW + i + 0, res60);
            _mm256_store_ps(result + (j * 32 + 7) * M_ROW + i + 0, res70);
            _mm256_store_ps(result + (j * 32 + 8) * M_ROW + i + 0, res80);
            _mm256_store_ps(result + (j * 32 + 9) * M_ROW + i + 0, res90);
            _mm256_store_ps(result + (j * 32 + 10) * M_ROW + i + 0, res100);
            _mm256_store_ps(result + (j * 32 + 11) * M_ROW + i + 0, res110);
            _mm256_store_ps(result + (j * 32 + 12) * M_ROW + i + 0, res120);
            _mm256_store_ps(result + (j * 32 + 13) * M_ROW + i + 0, res130);
            _mm256_store_ps(result + (j * 32 + 14) * M_ROW + i + 0, res140);
            _mm256_store_ps(result + (j * 32 + 15) * M_ROW + i + 0, res150);
            _mm256_store_ps(result + (j * 32 + 16) * M_ROW + i + 0, res160);
            _mm256_store_ps(result + (j * 32 + 17) * M_ROW + i + 0, res170);
            _mm256_store_ps(result + (j * 32 + 18) * M_ROW + i + 0, res180);
            _mm256_store_ps(result + (j * 32 + 19) * M_ROW + i + 0, res190);
            _mm256_store_ps(result + (j * 32 + 20) * M_ROW + i + 0, res200);
            _mm256_store_ps(result + (j * 32 + 21) * M_ROW + i + 0, res210);
            _mm256_store_ps(result + (j * 32 + 22) * M_ROW + i + 0, res220);
            _mm256_store_ps(result + (j * 32 + 23) * M_ROW + i + 0, res230);
            _mm256_store_ps(result + (j * 32 + 24) * M_ROW + i + 0, res240);
            _mm256_store_ps(result + (j * 32 + 25) * M_ROW + i + 0, res250);
            _mm256_store_ps(result + (j * 32 + 26) * M_ROW + i + 0, res260);
            _mm256_store_ps(result + (j * 32 + 27) * M_ROW + i + 0, res270);
            _mm256_store_ps(result + (j * 32 + 28) * M_ROW + i + 0, res280);
            _mm256_store_ps(result + (j * 32 + 29) * M_ROW + i + 0, res290);
            _mm256_store_ps(result + (j * 32 + 30) * M_ROW + i + 0, res300);
            _mm256_store_ps(result + (j * 32 + 31) * M_ROW + i + 0, res310);
            _mm256_store_ps(result + (j * 32 + 0) * M_ROW + i + 8, res01);
            _mm256_store_ps(result + (j * 32 + 1) * M_ROW + i + 8, res11);
            _mm256_store_ps(result + (j * 32 + 2) * M_ROW + i + 8, res21);
            _mm256_store_ps(result + (j * 32 + 3) * M_ROW + i + 8, res31);
            _mm256_store_ps(result + (j * 32 + 4) * M_ROW + i + 8, res41);
            _mm256_store_ps(result + (j * 32 + 5) * M_ROW + i + 8, res51);
            _mm256_store_ps(result + (j * 32 + 6) * M_ROW + i + 8, res61);
            _mm256_store_ps(result + (j * 32 + 7) * M_ROW + i + 8, res71);
            _mm256_store_ps(result + (j * 32 + 8) * M_ROW + i + 8, res81);
            _mm256_store_ps(result + (j * 32 + 9) * M_ROW + i + 8, res91);
            _mm256_store_ps(result + (j * 32 + 10) * M_ROW + i + 8, res101);
            _mm256_store_ps(result + (j * 32 + 11) * M_ROW + i + 8, res111);
            _mm256_store_ps(result + (j * 32 + 12) * M_ROW + i + 8, res121);
            _mm256_store_ps(result + (j * 32 + 13) * M_ROW + i + 8, res131);
            _mm256_store_ps(result + (j * 32 + 14) * M_ROW + i + 8, res141);
            _mm256_store_ps(result + (j * 32 + 15) * M_ROW + i + 8, res151);
            _mm256_store_ps(result + (j * 32 + 16) * M_ROW + i + 8, res161);
            _mm256_store_ps(result + (j * 32 + 17) * M_ROW + i + 8, res171);
            _mm256_store_ps(result + (j * 32 + 18) * M_ROW + i + 8, res181);
            _mm256_store_ps(result + (j * 32 + 19) * M_ROW + i + 8, res191);
            _mm256_store_ps(result + (j * 32 + 20) * M_ROW + i + 8, res201);
            _mm256_store_ps(result + (j * 32 + 21) * M_ROW + i + 8, res211);
            _mm256_store_ps(result + (j * 32 + 22) * M_ROW + i + 8, res221);
            _mm256_store_ps(result + (j * 32 + 23) * M_ROW + i + 8, res231);
            _mm256_store_ps(result + (j * 32 + 24) * M_ROW + i + 8, res241);
            _mm256_store_ps(result + (j * 32 + 25) * M_ROW + i + 8, res251);
            _mm256_store_ps(result + (j * 32 + 26) * M_ROW + i + 8, res261);
            _mm256_store_ps(result + (j * 32 + 27) * M_ROW + i + 8, res271);
            _mm256_store_ps(result + (j * 32 + 28) * M_ROW + i + 8, res281);
            _mm256_store_ps(result + (j * 32 + 29) * M_ROW + i + 8, res291);
            _mm256_store_ps(result + (j * 32 + 30) * M_ROW + i + 8, res301);
            _mm256_store_ps(result + (j * 32 + 31) * M_ROW + i + 8, res311);
            _mm256_store_ps(result + (j * 32 + 0) * M_ROW + i + 16, res02);
            _mm256_store_ps(result + (j * 32 + 1) * M_ROW + i + 16, res12);
            _mm256_store_ps(result + (j * 32 + 2) * M_ROW + i + 16, res22);
            _mm256_store_ps(result + (j * 32 + 3) * M_ROW + i + 16, res32);
            _mm256_store_ps(result + (j * 32 + 4) * M_ROW + i + 16, res42);
            _mm256_store_ps(result + (j * 32 + 5) * M_ROW + i + 16, res52);
            _mm256_store_ps(result + (j * 32 + 6) * M_ROW + i + 16, res62);
            _mm256_store_ps(result + (j * 32 + 7) * M_ROW + i + 16, res72);
            _mm256_store_ps(result + (j * 32 + 8) * M_ROW + i + 16, res82);
            _mm256_store_ps(result + (j * 32 + 9) * M_ROW + i + 16, res92);
            _mm256_store_ps(result + (j * 32 + 10) * M_ROW + i + 16, res102);
            _mm256_store_ps(result + (j * 32 + 11) * M_ROW + i + 16, res112);
            _mm256_store_ps(result + (j * 32 + 12) * M_ROW + i + 16, res122);
            _mm256_store_ps(result + (j * 32 + 13) * M_ROW + i + 16, res132);
            _mm256_store_ps(result + (j * 32 + 14) * M_ROW + i + 16, res142);
            _mm256_store_ps(result + (j * 32 + 15) * M_ROW + i + 16, res152);
            _mm256_store_ps(result + (j * 32 + 16) * M_ROW + i + 16, res162);
            _mm256_store_ps(result + (j * 32 + 17) * M_ROW + i + 16, res172);
            _mm256_store_ps(result + (j * 32 + 18) * M_ROW + i + 16, res182);
            _mm256_store_ps(result + (j * 32 + 19) * M_ROW + i + 16, res192);
            _mm256_store_ps(result + (j * 32 + 20) * M_ROW + i + 16, res202);
            _mm256_store_ps(result + (j * 32 + 21) * M_ROW + i + 16, res212);
            _mm256_store_ps(result + (j * 32 + 22) * M_ROW + i + 16, res222);
            _mm256_store_ps(result + (j * 32 + 23) * M_ROW + i + 16, res232);
            _mm256_store_ps(result + (j * 32 + 24) * M_ROW + i + 16, res242);
            _mm256_store_ps(result + (j * 32 + 25) * M_ROW + i + 16, res252);
            _mm256_store_ps(result + (j * 32 + 26) * M_ROW + i + 16, res262);
            _mm256_store_ps(result + (j * 32 + 27) * M_ROW + i + 16, res272);
            _mm256_store_ps(result + (j * 32 + 28) * M_ROW + i + 16, res282);
            _mm256_store_ps(result + (j * 32 + 29) * M_ROW + i + 16, res292);
            _mm256_store_ps(result + (j * 32 + 30) * M_ROW + i + 16, res302);
            _mm256_store_ps(result + (j * 32 + 31) * M_ROW + i + 16, res312);
            _mm256_store_ps(result + (j * 32 + 0) * M_ROW + i + 24, res03);
            _mm256_store_ps(result + (j * 32 + 1) * M_ROW + i + 24, res13);
            _mm256_store_ps(result + (j * 32 + 2) * M_ROW + i + 24, res23);
            _mm256_store_ps(result + (j * 32 + 3) * M_ROW + i + 24, res33);
            _mm256_store_ps(result + (j * 32 + 4) * M_ROW + i + 24, res43);
            _mm256_store_ps(result + (j * 32 + 5) * M_ROW + i + 24, res53);
            _mm256_store_ps(result + (j * 32 + 6) * M_ROW + i + 24, res63);
            _mm256_store_ps(result + (j * 32 + 7) * M_ROW + i + 24, res73);
            _mm256_store_ps(result + (j * 32 + 8) * M_ROW + i + 24, res83);
            _mm256_store_ps(result + (j * 32 + 9) * M_ROW + i + 24, res93);
            _mm256_store_ps(result + (j * 32 + 10) * M_ROW + i + 24, res103);
            _mm256_store_ps(result + (j * 32 + 11) * M_ROW + i + 24, res113);
            _mm256_store_ps(result + (j * 32 + 12) * M_ROW + i + 24, res123);
            _mm256_store_ps(result + (j * 32 + 13) * M_ROW + i + 24, res133);
            _mm256_store_ps(result + (j * 32 + 14) * M_ROW + i + 24, res143);
            _mm256_store_ps(result + (j * 32 + 15) * M_ROW + i + 24, res153);
            _mm256_store_ps(result + (j * 32 + 16) * M_ROW + i + 24, res163);
            _mm256_store_ps(result + (j * 32 + 17) * M_ROW + i + 24, res173);
            _mm256_store_ps(result + (j * 32 + 18) * M_ROW + i + 24, res183);
            _mm256_store_ps(result + (j * 32 + 19) * M_ROW + i + 24, res193);
            _mm256_store_ps(result + (j * 32 + 20) * M_ROW + i + 24, res203);
            _mm256_store_ps(result + (j * 32 + 21) * M_ROW + i + 24, res213);
            _mm256_store_ps(result + (j * 32 + 22) * M_ROW + i + 24, res223);
            _mm256_store_ps(result + (j * 32 + 23) * M_ROW + i + 24, res233);
            _mm256_store_ps(result + (j * 32 + 24) * M_ROW + i + 24, res243);
            _mm256_store_ps(result + (j * 32 + 25) * M_ROW + i + 24, res253);
            _mm256_store_ps(result + (j * 32 + 26) * M_ROW + i + 24, res263);
            _mm256_store_ps(result + (j * 32 + 27) * M_ROW + i + 24, res273);
            _mm256_store_ps(result + (j * 32 + 28) * M_ROW + i + 24, res283);
            _mm256_store_ps(result + (j * 32 + 29) * M_ROW + i + 24, res293);
            _mm256_store_ps(result + (j * 32 + 30) * M_ROW + i + 24, res303);
            _mm256_store_ps(result + (j * 32 + 31) * M_ROW + i + 24, res313);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Merged_GroupMin_64xG4_AVX2_OpenMP(const float* X, const int32_t* metadata, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 4; j++) {
        const int* groupData = &metadata[j * 10];
        for (int i = 0; i < M_ROW; i += 64) {
            __m256 res00 = _mm256_setzero_ps();
            __m256 res01 = _mm256_setzero_ps();
            __m256 res02 = _mm256_setzero_ps();
            __m256 res03 = _mm256_setzero_ps();
            __m256 res04 = _mm256_setzero_ps();
            __m256 res05 = _mm256_setzero_ps();
            __m256 res06 = _mm256_setzero_ps();
            __m256 res07 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res11 = _mm256_setzero_ps();
            __m256 res12 = _mm256_setzero_ps();
            __m256 res13 = _mm256_setzero_ps();
            __m256 res14 = _mm256_setzero_ps();
            __m256 res15 = _mm256_setzero_ps();
            __m256 res16 = _mm256_setzero_ps();
            __m256 res17 = _mm256_setzero_ps();
            __m256 res20 = _mm256_setzero_ps();
            __m256 res21 = _mm256_setzero_ps();
            __m256 res22 = _mm256_setzero_ps();
            __m256 res23 = _mm256_setzero_ps();
            __m256 res24 = _mm256_setzero_ps();
            __m256 res25 = _mm256_setzero_ps();
            __m256 res26 = _mm256_setzero_ps();
            __m256 res27 = _mm256_setzero_ps();
            __m256 res30 = _mm256_setzero_ps();
            __m256 res31 = _mm256_setzero_ps();
            __m256 res32 = _mm256_setzero_ps();
            __m256 res33 = _mm256_setzero_ps();
            __m256 res34 = _mm256_setzero_ps();
            __m256 res35 = _mm256_setzero_ps();
            __m256 res36 = _mm256_setzero_ps();
            __m256 res37 = _mm256_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 8) {
                __m256 pos00 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m256 neg00 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m256 pos10 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m256 neg10 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m256 pos20 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m256 neg20 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m256 pos30 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m256 neg30 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m256 pos01 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 8);
                __m256 neg01 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 8);
                __m256 pos11 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 8);
                __m256 neg11 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 8);
                __m256 pos21 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 8);
                __m256 neg21 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 8);
                __m256 pos31 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 8);
                __m256 neg31 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 8);
                __m256 pos02 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 16);
                __m256 neg02 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 16);
                __m256 pos12 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 16);
                __m256 neg12 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 16);
                __m256 pos22 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 16);
                __m256 neg22 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 16);
                __m256 pos32 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 16);
                __m256 neg32 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 16);
                __m256 pos03 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 24);
                __m256 neg03 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 24);
                __m256 pos13 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 24);
                __m256 neg13 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 24);
                __m256 pos23 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 24);
                __m256 neg23 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 24);
                __m256 pos33 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 24);
                __m256 neg33 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 24);
                __m256 pos04 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 32);
                __m256 neg04 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 32);
                __m256 pos14 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 32);
                __m256 neg14 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 32);
                __m256 pos24 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 32);
                __m256 neg24 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 32);
                __m256 pos34 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 32);
                __m256 neg34 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 32);
                __m256 pos05 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 40);
                __m256 neg05 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 40);
                __m256 pos15 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 40);
                __m256 neg15 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 40);
                __m256 pos25 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 40);
                __m256 neg25 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 40);
                __m256 pos35 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 40);
                __m256 neg35 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 40);
                __m256 pos06 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 48);
                __m256 neg06 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 48);
                __m256 pos16 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 48);
                __m256 neg16 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 48);
                __m256 pos26 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 48);
                __m256 neg26 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 48);
                __m256 pos36 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 48);
                __m256 neg36 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 48);
                __m256 pos07 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 56);
                __m256 neg07 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 56);
                __m256 pos17 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 56);
                __m256 neg17 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 56);
                __m256 pos27 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 56);
                __m256 neg27 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 56);
                __m256 pos37 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 56);
                __m256 neg37 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 56);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res20 = _mm256_add_ps(res20, _mm256_sub_ps(pos20, neg20));
                res30 = _mm256_add_ps(res30, _mm256_sub_ps(pos30, neg30));
                res01 = _mm256_add_ps(res01, _mm256_sub_ps(pos01, neg01));
                res11 = _mm256_add_ps(res11, _mm256_sub_ps(pos11, neg11));
                res21 = _mm256_add_ps(res21, _mm256_sub_ps(pos21, neg21));
                res31 = _mm256_add_ps(res31, _mm256_sub_ps(pos31, neg31));
                res02 = _mm256_add_ps(res02, _mm256_sub_ps(pos02, neg02));
                res12 = _mm256_add_ps(res12, _mm256_sub_ps(pos12, neg12));
                res22 = _mm256_add_ps(res22, _mm256_sub_ps(pos22, neg22));
                res32 = _mm256_add_ps(res32, _mm256_sub_ps(pos32, neg32));
                res03 = _mm256_add_ps(res03, _mm256_sub_ps(pos03, neg03));
                res13 = _mm256_add_ps(res13, _mm256_sub_ps(pos13, neg13));
                res23 = _mm256_add_ps(res23, _mm256_sub_ps(pos23, neg23));
                res33 = _mm256_add_ps(res33, _mm256_sub_ps(pos33, neg33));
                res04 = _mm256_add_ps(res04, _mm256_sub_ps(pos04, neg04));
                res14 = _mm256_add_ps(res14, _mm256_sub_ps(pos14, neg14));
                res24 = _mm256_add_ps(res24, _mm256_sub_ps(pos24, neg24));
                res34 = _mm256_add_ps(res34, _mm256_sub_ps(pos34, neg34));
                res05 = _mm256_add_ps(res05, _mm256_sub_ps(pos05, neg05));
                res15 = _mm256_add_ps(res15, _mm256_sub_ps(pos15, neg15));
                res25 = _mm256_add_ps(res25, _mm256_sub_ps(pos25, neg25));
                res35 = _mm256_add_ps(res35, _mm256_sub_ps(pos35, neg35));
                res06 = _mm256_add_ps(res06, _mm256_sub_ps(pos06, neg06));
                res16 = _mm256_add_ps(res16, _mm256_sub_ps(pos16, neg16));
                res26 = _mm256_add_ps(res26, _mm256_sub_ps(pos26, neg26));
                res36 = _mm256_add_ps(res36, _mm256_sub_ps(pos36, neg36));
                res07 = _mm256_add_ps(res07, _mm256_sub_ps(pos07, neg07));
                res17 = _mm256_add_ps(res17, _mm256_sub_ps(pos17, neg17));
                res27 = _mm256_add_ps(res27, _mm256_sub_ps(pos27, neg27));
                res37 = _mm256_add_ps(res37, _mm256_sub_ps(pos37, neg37));
            }
            for (int k = groupData[1]; k < groupData[2]; k++) {
                res00 = _mm256_add_ps(res00, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res01 = _mm256_add_ps(res01, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res02 = _mm256_add_ps(res02, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res03 = _mm256_add_ps(res03, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
                res04 = _mm256_add_ps(res04, _mm256_load_ps(X + row_index[k] * M_ROW + i + 32));
                res05 = _mm256_add_ps(res05, _mm256_load_ps(X + row_index[k] * M_ROW + i + 40));
                res06 = _mm256_add_ps(res06, _mm256_load_ps(X + row_index[k] * M_ROW + i + 48));
                res07 = _mm256_add_ps(res07, _mm256_load_ps(X + row_index[k] * M_ROW + i + 56));
            }
            for (int k = groupData[2]; k < groupData[3]; k++) {
                res00 = _mm256_sub_ps(res00, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res01 = _mm256_sub_ps(res01, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res02 = _mm256_sub_ps(res02, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res03 = _mm256_sub_ps(res03, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
                res04 = _mm256_sub_ps(res04, _mm256_load_ps(X + row_index[k] * M_ROW + i + 32));
                res05 = _mm256_sub_ps(res05, _mm256_load_ps(X + row_index[k] * M_ROW + i + 40));
                res06 = _mm256_sub_ps(res06, _mm256_load_ps(X + row_index[k] * M_ROW + i + 48));
                res07 = _mm256_sub_ps(res07, _mm256_load_ps(X + row_index[k] * M_ROW + i + 56));
            }
            for (int k = groupData[3]; k < groupData[4]; k++) {
                res10 = _mm256_add_ps(res10, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res11 = _mm256_add_ps(res11, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res12 = _mm256_add_ps(res12, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res13 = _mm256_add_ps(res13, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
                res14 = _mm256_add_ps(res14, _mm256_load_ps(X + row_index[k] * M_ROW + i + 32));
                res15 = _mm256_add_ps(res15, _mm256_load_ps(X + row_index[k] * M_ROW + i + 40));
                res16 = _mm256_add_ps(res16, _mm256_load_ps(X + row_index[k] * M_ROW + i + 48));
                res17 = _mm256_add_ps(res17, _mm256_load_ps(X + row_index[k] * M_ROW + i + 56));
            }
            for (int k = groupData[4]; k < groupData[5]; k++) {
                res10 = _mm256_sub_ps(res10, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res11 = _mm256_sub_ps(res11, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res12 = _mm256_sub_ps(res12, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res13 = _mm256_sub_ps(res13, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
                res14 = _mm256_sub_ps(res14, _mm256_load_ps(X + row_index[k] * M_ROW + i + 32));
                res15 = _mm256_sub_ps(res15, _mm256_load_ps(X + row_index[k] * M_ROW + i + 40));
                res16 = _mm256_sub_ps(res16, _mm256_load_ps(X + row_index[k] * M_ROW + i + 48));
                res17 = _mm256_sub_ps(res17, _mm256_load_ps(X + row_index[k] * M_ROW + i + 56));
            }
            for (int k = groupData[5]; k < groupData[6]; k++) {
                res20 = _mm256_add_ps(res20, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res21 = _mm256_add_ps(res21, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res22 = _mm256_add_ps(res22, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res23 = _mm256_add_ps(res23, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
                res24 = _mm256_add_ps(res24, _mm256_load_ps(X + row_index[k] * M_ROW + i + 32));
                res25 = _mm256_add_ps(res25, _mm256_load_ps(X + row_index[k] * M_ROW + i + 40));
                res26 = _mm256_add_ps(res26, _mm256_load_ps(X + row_index[k] * M_ROW + i + 48));
                res27 = _mm256_add_ps(res27, _mm256_load_ps(X + row_index[k] * M_ROW + i + 56));
            }
            for (int k = groupData[6]; k < groupData[7]; k++) {
                res20 = _mm256_sub_ps(res20, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res21 = _mm256_sub_ps(res21, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res22 = _mm256_sub_ps(res22, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res23 = _mm256_sub_ps(res23, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
                res24 = _mm256_sub_ps(res24, _mm256_load_ps(X + row_index[k] * M_ROW + i + 32));
                res25 = _mm256_sub_ps(res25, _mm256_load_ps(X + row_index[k] * M_ROW + i + 40));
                res26 = _mm256_sub_ps(res26, _mm256_load_ps(X + row_index[k] * M_ROW + i + 48));
                res27 = _mm256_sub_ps(res27, _mm256_load_ps(X + row_index[k] * M_ROW + i + 56));
            }
            for (int k = groupData[7]; k < groupData[8]; k++) {
                res30 = _mm256_add_ps(res30, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res31 = _mm256_add_ps(res31, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res32 = _mm256_add_ps(res32, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res33 = _mm256_add_ps(res33, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
                res34 = _mm256_add_ps(res34, _mm256_load_ps(X + row_index[k] * M_ROW + i + 32));
                res35 = _mm256_add_ps(res35, _mm256_load_ps(X + row_index[k] * M_ROW + i + 40));
                res36 = _mm256_add_ps(res36, _mm256_load_ps(X + row_index[k] * M_ROW + i + 48));
                res37 = _mm256_add_ps(res37, _mm256_load_ps(X + row_index[k] * M_ROW + i + 56));
            }
            for (int k = groupData[8]; k < groupData[9]; k++) {
                res30 = _mm256_sub_ps(res30, _mm256_load_ps(X + row_index[k] * M_ROW + i + 0));
                res31 = _mm256_sub_ps(res31, _mm256_load_ps(X + row_index[k] * M_ROW + i + 8));
                res32 = _mm256_sub_ps(res32, _mm256_load_ps(X + row_index[k] * M_ROW + i + 16));
                res33 = _mm256_sub_ps(res33, _mm256_load_ps(X + row_index[k] * M_ROW + i + 24));
                res34 = _mm256_sub_ps(res34, _mm256_load_ps(X + row_index[k] * M_ROW + i + 32));
                res35 = _mm256_sub_ps(res35, _mm256_load_ps(X + row_index[k] * M_ROW + i + 40));
                res36 = _mm256_sub_ps(res36, _mm256_load_ps(X + row_index[k] * M_ROW + i + 48));
                res37 = _mm256_sub_ps(res37, _mm256_load_ps(X + row_index[k] * M_ROW + i + 56));
            }
            _mm256_store_ps(result + (j * 4 + 0) * M_ROW + i + 0, res00);
            _mm256_store_ps(result + (j * 4 + 1) * M_ROW + i + 0, res10);
            _mm256_store_ps(result + (j * 4 + 2) * M_ROW + i + 0, res20);
            _mm256_store_ps(result + (j * 4 + 3) * M_ROW + i + 0, res30);
            _mm256_store_ps(result + (j * 4 + 0) * M_ROW + i + 8, res01);
            _mm256_store_ps(result + (j * 4 + 1) * M_ROW + i + 8, res11);
            _mm256_store_ps(result + (j * 4 + 2) * M_ROW + i + 8, res21);
            _mm256_store_ps(result + (j * 4 + 3) * M_ROW + i + 8, res31);
            _mm256_store_ps(result + (j * 4 + 0) * M_ROW + i + 16, res02);
            _mm256_store_ps(result + (j * 4 + 1) * M_ROW + i + 16, res12);
            _mm256_store_ps(result + (j * 4 + 2) * M_ROW + i + 16, res22);
            _mm256_store_ps(result + (j * 4 + 3) * M_ROW + i + 16, res32);
            _mm256_store_ps(result + (j * 4 + 0) * M_ROW + i + 24, res03);
            _mm256_store_ps(result + (j * 4 + 1) * M_ROW + i + 24, res13);
            _mm256_store_ps(result + (j * 4 + 2) * M_ROW + i + 24, res23);
            _mm256_store_ps(result + (j * 4 + 3) * M_ROW + i + 24, res33);
            _mm256_store_ps(result + (j * 4 + 0) * M_ROW + i + 32, res04);
            _mm256_store_ps(result + (j * 4 + 1) * M_ROW + i + 32, res14);
            _mm256_store_ps(result + (j * 4 + 2) * M_ROW + i + 32, res24);
            _mm256_store_ps(result + (j * 4 + 3) * M_ROW + i + 32, res34);
            _mm256_store_ps(result + (j * 4 + 0) * M_ROW + i + 40, res05);
            _mm256_store_ps(result + (j * 4 + 1) * M_ROW + i + 40, res15);
            _mm256_store_ps(result + (j * 4 + 2) * M_ROW + i + 40, res25);
            _mm256_store_ps(result + (j * 4 + 3) * M_ROW + i + 40, res35);
            _mm256_store_ps(result + (j * 4 + 0) * M_ROW + i + 48, res06);
            _mm256_store_ps(result + (j * 4 + 1) * M_ROW + i + 48, res16);
            _mm256_store_ps(result + (j * 4 + 2) * M_ROW + i + 48, res26);
            _mm256_store_ps(result + (j * 4 + 3) * M_ROW + i + 48, res36);
            _mm256_store_ps(result + (j * 4 + 0) * M_ROW + i + 56, res07);
            _mm256_store_ps(result + (j * 4 + 1) * M_ROW + i + 56, res17);
            _mm256_store_ps(result + (j * 4 + 2) * M_ROW + i + 56, res27);
            _mm256_store_ps(result + (j * 4 + 3) * M_ROW + i + 56, res37);
        }
    }
}


void GEMM_CPU_FP32_colMajor_TCSC_Merged_GroupMid_8xG4_AVX2_OpenMP(const float* X, const int32_t* metadata, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL/4; j ++) {
        /* Pointer to where a column starts and ends
        int align_start  = metadata[j * 10 + 0];
        int align_end    = metadata[j * 10 + 1];
        int remain_end0 = metadata[j * 10 + 2];
        int remain_val0 = metadata[j * 10 + 3];
        int remain_end1 = metadata[j * 10 + 4];
        int remain_val1 = metadata[j * 10 + 5];
        ...
        */
        // Group # = j, metadata per group = 10
        const int* groupData  = &metadata[j * 10];

        for (int i = 0; i < M_ROW; i += 8) {
            __m256 res0 = _mm256_set1_ps(0.0f);
            __m256 res1 = _mm256_set1_ps(0.0f);
            __m256 res2 = _mm256_set1_ps(0.0f);
            __m256 res3 = _mm256_set1_ps(0.0f);

            for (int k = groupData[0]; k < groupData[1]; k += 8) {
                __m256 pos0 = _mm256_load_ps(&X[row_index[k + 0] * M_ROW + i]);
                __m256 neg0 = _mm256_load_ps(&X[row_index[k + 1] * M_ROW + i]);
                __m256 pos1 = _mm256_load_ps(&X[row_index[k + 2] * M_ROW + i]);
                __m256 neg1 = _mm256_load_ps(&X[row_index[k + 3] * M_ROW + i]);
                __m256 pos2 = _mm256_load_ps(&X[row_index[k + 4] * M_ROW + i]);
                __m256 neg2 = _mm256_load_ps(&X[row_index[k + 5] * M_ROW + i]);
                __m256 pos3 = _mm256_load_ps(&X[row_index[k + 6] * M_ROW + i]);
                __m256 neg3 = _mm256_load_ps(&X[row_index[k + 7] * M_ROW + i]);
                res0 = _mm256_add_ps(res0, _mm256_sub_ps(pos0, neg0));
                res1 = _mm256_add_ps(res1, _mm256_sub_ps(pos1, neg1));
                res2 = _mm256_add_ps(res2, _mm256_sub_ps(pos2, neg2));
                res3 = _mm256_add_ps(res3, _mm256_sub_ps(pos3, neg3));
            }

            __m256 pos0 = _mm256_set1_ps(0.0f);
            for (int k = groupData[1]; k < groupData[2]; k++) {
                pos0 = _mm256_add_ps(pos0, _mm256_load_ps(&X[row_index[k] * M_ROW + i]));
            }
            res0 = _mm256_add_ps(res0, _mm256_mul_ps(pos0, _mm256_set1_ps(groupData[3])));

            __m256 pos1 = _mm256_set1_ps(0.0f);
            for (int k = groupData[2]; k < groupData[4]; k++) {
                pos1 = _mm256_add_ps(pos1, _mm256_load_ps(&X[row_index[k] * M_ROW + i]));
            }
            res1 = _mm256_add_ps(res1, _mm256_mul_ps(pos1, _mm256_set1_ps(groupData[5])));

            __m256 pos2 = _mm256_set1_ps(0.0f);
            for (int k = groupData[4]; k < groupData[6]; k++) {
                pos2 = _mm256_add_ps(pos2, _mm256_load_ps(&X[row_index[k] * M_ROW + i]));
            }
            res2 = _mm256_add_ps(res2, _mm256_mul_ps(pos2, _mm256_set1_ps(groupData[7])));

            __m256 pos3 = _mm256_set1_ps(0.0f);
            for (int k = groupData[6]; k < groupData[8]; k++) {
                pos3 = _mm256_add_ps(pos3, _mm256_load_ps(&X[row_index[k] * M_ROW + i]));
            }
            res3 = _mm256_add_ps(res3, _mm256_mul_ps(pos3, _mm256_set1_ps(groupData[9])));

            _mm256_store_ps(&result[(j * 4 + 0) * M_ROW + i], res0);
            _mm256_store_ps(&result[(j * 4 + 1) * M_ROW + i], res1);
            _mm256_store_ps(&result[(j * 4 + 2) * M_ROW + i], res2);
            _mm256_store_ps(&result[(j * 4 + 3) * M_ROW + i], res3);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Aligned_GroupMax_8xG4_AVX2_OpenMP(const float* X, const int32_t* metadata, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL/4; j ++) {
        /* Pointer to where a column starts and ends
        int align_start  = metadata[j + 0];
        int align_end    = metadata[j + 1];
        */
        // Group # = j, metadata per group = 2
        for (int i = 0; i < M_ROW; i += 8) {
            __m256 res0 = _mm256_set1_ps(0.0f);
            __m256 res1 = _mm256_set1_ps(0.0f);
            __m256 res2 = _mm256_set1_ps(0.0f);
            __m256 res3 = _mm256_set1_ps(0.0f);
            for (int k = metadata[j*2]; k < metadata[j*2+1]; k += 8) {
                __m256 pos0 = _mm256_load_ps(&X[row_index[k + 0] * M_ROW + i]);
                __m256 neg0 = _mm256_load_ps(&X[row_index[k + 1] * M_ROW + i]);
                __m256 pos1 = _mm256_load_ps(&X[row_index[k + 2] * M_ROW + i]);
                __m256 neg1 = _mm256_load_ps(&X[row_index[k + 3] * M_ROW + i]);
                __m256 pos2 = _mm256_load_ps(&X[row_index[k + 4] * M_ROW + i]);
                __m256 neg2 = _mm256_load_ps(&X[row_index[k + 5] * M_ROW + i]);
                __m256 pos3 = _mm256_load_ps(&X[row_index[k + 6] * M_ROW + i]);
                __m256 neg3 = _mm256_load_ps(&X[row_index[k + 7] * M_ROW + i]);
                res0 = _mm256_add_ps(res0, _mm256_sub_ps(pos0, neg0));
                res1 = _mm256_add_ps(res1, _mm256_sub_ps(pos1, neg1));
                res2 = _mm256_add_ps(res2, _mm256_sub_ps(pos2, neg2));
                res3 = _mm256_add_ps(res3, _mm256_sub_ps(pos3, neg3));
            }
            _mm256_store_ps(&result[(j * 4 + 0) * M_ROW + i], res0);
            _mm256_store_ps(&result[(j * 4 + 1) * M_ROW + i], res1);
            _mm256_store_ps(&result[(j * 4 + 2) * M_ROW + i], res2);
            _mm256_store_ps(&result[(j * 4 + 3) * M_ROW + i], res3);
        }
    }
}


void GEMM_CPU_FP32_colMajor_TCSC_Uniform_8xG4_AVX2_OpenMP(const float* X, const int32_t NonZeroPerCol, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL/4; j ++) {
        /* Pointer to where a column starts and ends
        int align_start  = metadata[j + 0];
        int align_end    = metadata[j + 1];
        */
        // Group # = j, metadata per group = 2
        for (int i = 0; i < M_ROW; i += 8) {
            __m256 res0 = _mm256_set1_ps(0.0f);
            __m256 res1 = _mm256_set1_ps(0.0f);
            __m256 res2 = _mm256_set1_ps(0.0f);
            __m256 res3 = _mm256_set1_ps(0.0f);
            for (int k = j * 4 * NonZeroPerCol; k < (j * 4 + 4)*NonZeroPerCol; k += 8) {
                __m256 pos0 = _mm256_load_ps(&X[row_index[k + 0] * M_ROW + i]);
                __m256 neg0 = _mm256_load_ps(&X[row_index[k + 1] * M_ROW + i]);
                __m256 pos1 = _mm256_load_ps(&X[row_index[k + 2] * M_ROW + i]);
                __m256 neg1 = _mm256_load_ps(&X[row_index[k + 3] * M_ROW + i]);
                __m256 pos2 = _mm256_load_ps(&X[row_index[k + 4] * M_ROW + i]);
                __m256 neg2 = _mm256_load_ps(&X[row_index[k + 5] * M_ROW + i]);
                __m256 pos3 = _mm256_load_ps(&X[row_index[k + 6] * M_ROW + i]);
                __m256 neg3 = _mm256_load_ps(&X[row_index[k + 7] * M_ROW + i]);
                res0 = _mm256_add_ps(res0, _mm256_sub_ps(pos0, neg0));
                res1 = _mm256_add_ps(res1, _mm256_sub_ps(pos1, neg1));
                res2 = _mm256_add_ps(res2, _mm256_sub_ps(pos2, neg2));
                res3 = _mm256_add_ps(res3, _mm256_sub_ps(pos3, neg3));
            }
            _mm256_store_ps(&result[(j * 4 + 0) * M_ROW + i], res0);
            _mm256_store_ps(&result[(j * 4 + 1) * M_ROW + i], res1);
            _mm256_store_ps(&result[(j * 4 + 2) * M_ROW + i], res2);
            _mm256_store_ps(&result[(j * 4 + 3) * M_ROW + i], res3);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Uniform_8xG8_AVX2_OpenMP(const float* X, const int32_t NonZeroPerCol, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 8; j++) {
        for (int i = 0; i < M_ROW; i += 8) {
            __m256 res00 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res20 = _mm256_setzero_ps();
            __m256 res30 = _mm256_setzero_ps();
            __m256 res40 = _mm256_setzero_ps();
            __m256 res50 = _mm256_setzero_ps();
            __m256 res60 = _mm256_setzero_ps();
            __m256 res70 = _mm256_setzero_ps();
            for (int k = j * 8 * NonZeroPerCol; k < (j + 1) * 8 * NonZeroPerCol; k += 16) {
                __m256 pos00 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m256 neg00 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m256 pos10 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m256 neg10 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m256 pos20 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m256 neg20 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m256 pos30 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m256 neg30 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m256 pos40 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 0);
                __m256 neg40 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 0);
                __m256 pos50 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 0);
                __m256 neg50 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 0);
                __m256 pos60 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 0);
                __m256 neg60 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 0);
                __m256 pos70 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 0);
                __m256 neg70 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 0);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res20 = _mm256_add_ps(res20, _mm256_sub_ps(pos20, neg20));
                res30 = _mm256_add_ps(res30, _mm256_sub_ps(pos30, neg30));
                res40 = _mm256_add_ps(res40, _mm256_sub_ps(pos40, neg40));
                res50 = _mm256_add_ps(res50, _mm256_sub_ps(pos50, neg50));
                res60 = _mm256_add_ps(res60, _mm256_sub_ps(pos60, neg60));
                res70 = _mm256_add_ps(res70, _mm256_sub_ps(pos70, neg70));
            }
            _mm256_store_ps(result + (j * 8 + 0) * M_ROW + i + 0, res00);
            _mm256_store_ps(result + (j * 8 + 1) * M_ROW + i + 0, res10);
            _mm256_store_ps(result + (j * 8 + 2) * M_ROW + i + 0, res20);
            _mm256_store_ps(result + (j * 8 + 3) * M_ROW + i + 0, res30);
            _mm256_store_ps(result + (j * 8 + 4) * M_ROW + i + 0, res40);
            _mm256_store_ps(result + (j * 8 + 5) * M_ROW + i + 0, res50);
            _mm256_store_ps(result + (j * 8 + 6) * M_ROW + i + 0, res60);
            _mm256_store_ps(result + (j * 8 + 7) * M_ROW + i + 0, res70);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Uniform_8xG16_AVX2_OpenMP(const float* X, const int32_t NonZeroPerCol, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 16; j++) {
        for (int i = 0; i < M_ROW; i += 8) {
            __m256 res00 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res20 = _mm256_setzero_ps();
            __m256 res30 = _mm256_setzero_ps();
            __m256 res40 = _mm256_setzero_ps();
            __m256 res50 = _mm256_setzero_ps();
            __m256 res60 = _mm256_setzero_ps();
            __m256 res70 = _mm256_setzero_ps();
            __m256 res80 = _mm256_setzero_ps();
            __m256 res90 = _mm256_setzero_ps();
            __m256 res100 = _mm256_setzero_ps();
            __m256 res110 = _mm256_setzero_ps();
            __m256 res120 = _mm256_setzero_ps();
            __m256 res130 = _mm256_setzero_ps();
            __m256 res140 = _mm256_setzero_ps();
            __m256 res150 = _mm256_setzero_ps();
            for (int k = j * 16 * NonZeroPerCol; k < (j + 1) * 16 * NonZeroPerCol; k += 32) {
                __m256 pos00 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m256 neg00 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m256 pos10 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m256 neg10 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m256 pos20 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m256 neg20 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m256 pos30 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m256 neg30 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m256 pos40 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 0);
                __m256 neg40 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 0);
                __m256 pos50 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 0);
                __m256 neg50 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 0);
                __m256 pos60 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 0);
                __m256 neg60 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 0);
                __m256 pos70 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 0);
                __m256 neg70 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 0);
                __m256 pos80 = _mm256_load_ps(X + row_index[k + 16] * M_ROW + i + 0);
                __m256 neg80 = _mm256_load_ps(X + row_index[k + 17] * M_ROW + i + 0);
                __m256 pos90 = _mm256_load_ps(X + row_index[k + 18] * M_ROW + i + 0);
                __m256 neg90 = _mm256_load_ps(X + row_index[k + 19] * M_ROW + i + 0);
                __m256 pos100 = _mm256_load_ps(X + row_index[k + 20] * M_ROW + i + 0);
                __m256 neg100 = _mm256_load_ps(X + row_index[k + 21] * M_ROW + i + 0);
                __m256 pos110 = _mm256_load_ps(X + row_index[k + 22] * M_ROW + i + 0);
                __m256 neg110 = _mm256_load_ps(X + row_index[k + 23] * M_ROW + i + 0);
                __m256 pos120 = _mm256_load_ps(X + row_index[k + 24] * M_ROW + i + 0);
                __m256 neg120 = _mm256_load_ps(X + row_index[k + 25] * M_ROW + i + 0);
                __m256 pos130 = _mm256_load_ps(X + row_index[k + 26] * M_ROW + i + 0);
                __m256 neg130 = _mm256_load_ps(X + row_index[k + 27] * M_ROW + i + 0);
                __m256 pos140 = _mm256_load_ps(X + row_index[k + 28] * M_ROW + i + 0);
                __m256 neg140 = _mm256_load_ps(X + row_index[k + 29] * M_ROW + i + 0);
                __m256 pos150 = _mm256_load_ps(X + row_index[k + 30] * M_ROW + i + 0);
                __m256 neg150 = _mm256_load_ps(X + row_index[k + 31] * M_ROW + i + 0);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res20 = _mm256_add_ps(res20, _mm256_sub_ps(pos20, neg20));
                res30 = _mm256_add_ps(res30, _mm256_sub_ps(pos30, neg30));
                res40 = _mm256_add_ps(res40, _mm256_sub_ps(pos40, neg40));
                res50 = _mm256_add_ps(res50, _mm256_sub_ps(pos50, neg50));
                res60 = _mm256_add_ps(res60, _mm256_sub_ps(pos60, neg60));
                res70 = _mm256_add_ps(res70, _mm256_sub_ps(pos70, neg70));
                res80 = _mm256_add_ps(res80, _mm256_sub_ps(pos80, neg80));
                res90 = _mm256_add_ps(res90, _mm256_sub_ps(pos90, neg90));
                res100 = _mm256_add_ps(res100, _mm256_sub_ps(pos100, neg100));
                res110 = _mm256_add_ps(res110, _mm256_sub_ps(pos110, neg110));
                res120 = _mm256_add_ps(res120, _mm256_sub_ps(pos120, neg120));
                res130 = _mm256_add_ps(res130, _mm256_sub_ps(pos130, neg130));
                res140 = _mm256_add_ps(res140, _mm256_sub_ps(pos140, neg140));
                res150 = _mm256_add_ps(res150, _mm256_sub_ps(pos150, neg150));
            }
            _mm256_store_ps(result + (j * 16 + 0) * M_ROW + i + 0, res00);
            _mm256_store_ps(result + (j * 16 + 1) * M_ROW + i + 0, res10);
            _mm256_store_ps(result + (j * 16 + 2) * M_ROW + i + 0, res20);
            _mm256_store_ps(result + (j * 16 + 3) * M_ROW + i + 0, res30);
            _mm256_store_ps(result + (j * 16 + 4) * M_ROW + i + 0, res40);
            _mm256_store_ps(result + (j * 16 + 5) * M_ROW + i + 0, res50);
            _mm256_store_ps(result + (j * 16 + 6) * M_ROW + i + 0, res60);
            _mm256_store_ps(result + (j * 16 + 7) * M_ROW + i + 0, res70);
            _mm256_store_ps(result + (j * 16 + 8) * M_ROW + i + 0, res80);
            _mm256_store_ps(result + (j * 16 + 9) * M_ROW + i + 0, res90);
            _mm256_store_ps(result + (j * 16 + 10) * M_ROW + i + 0, res100);
            _mm256_store_ps(result + (j * 16 + 11) * M_ROW + i + 0, res110);
            _mm256_store_ps(result + (j * 16 + 12) * M_ROW + i + 0, res120);
            _mm256_store_ps(result + (j * 16 + 13) * M_ROW + i + 0, res130);
            _mm256_store_ps(result + (j * 16 + 14) * M_ROW + i + 0, res140);
            _mm256_store_ps(result + (j * 16 + 15) * M_ROW + i + 0, res150);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Uniform_8xG32_AVX2_OpenMP(const float* X, const int32_t NonZeroPerCol, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 32; j++) {
        for (int i = 0; i < M_ROW; i += 8) {
            __m256 res00 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res20 = _mm256_setzero_ps();
            __m256 res30 = _mm256_setzero_ps();
            __m256 res40 = _mm256_setzero_ps();
            __m256 res50 = _mm256_setzero_ps();
            __m256 res60 = _mm256_setzero_ps();
            __m256 res70 = _mm256_setzero_ps();
            __m256 res80 = _mm256_setzero_ps();
            __m256 res90 = _mm256_setzero_ps();
            __m256 res100 = _mm256_setzero_ps();
            __m256 res110 = _mm256_setzero_ps();
            __m256 res120 = _mm256_setzero_ps();
            __m256 res130 = _mm256_setzero_ps();
            __m256 res140 = _mm256_setzero_ps();
            __m256 res150 = _mm256_setzero_ps();
            __m256 res160 = _mm256_setzero_ps();
            __m256 res170 = _mm256_setzero_ps();
            __m256 res180 = _mm256_setzero_ps();
            __m256 res190 = _mm256_setzero_ps();
            __m256 res200 = _mm256_setzero_ps();
            __m256 res210 = _mm256_setzero_ps();
            __m256 res220 = _mm256_setzero_ps();
            __m256 res230 = _mm256_setzero_ps();
            __m256 res240 = _mm256_setzero_ps();
            __m256 res250 = _mm256_setzero_ps();
            __m256 res260 = _mm256_setzero_ps();
            __m256 res270 = _mm256_setzero_ps();
            __m256 res280 = _mm256_setzero_ps();
            __m256 res290 = _mm256_setzero_ps();
            __m256 res300 = _mm256_setzero_ps();
            __m256 res310 = _mm256_setzero_ps();
            for (int k = j * 32 * NonZeroPerCol; k < (j + 1) * 32 * NonZeroPerCol; k += 64) {
                __m256 pos00 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m256 neg00 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m256 pos10 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m256 neg10 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m256 pos20 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m256 neg20 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m256 pos30 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m256 neg30 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m256 pos40 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 0);
                __m256 neg40 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 0);
                __m256 pos50 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 0);
                __m256 neg50 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 0);
                __m256 pos60 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 0);
                __m256 neg60 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 0);
                __m256 pos70 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 0);
                __m256 neg70 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 0);
                __m256 pos80 = _mm256_load_ps(X + row_index[k + 16] * M_ROW + i + 0);
                __m256 neg80 = _mm256_load_ps(X + row_index[k + 17] * M_ROW + i + 0);
                __m256 pos90 = _mm256_load_ps(X + row_index[k + 18] * M_ROW + i + 0);
                __m256 neg90 = _mm256_load_ps(X + row_index[k + 19] * M_ROW + i + 0);
                __m256 pos100 = _mm256_load_ps(X + row_index[k + 20] * M_ROW + i + 0);
                __m256 neg100 = _mm256_load_ps(X + row_index[k + 21] * M_ROW + i + 0);
                __m256 pos110 = _mm256_load_ps(X + row_index[k + 22] * M_ROW + i + 0);
                __m256 neg110 = _mm256_load_ps(X + row_index[k + 23] * M_ROW + i + 0);
                __m256 pos120 = _mm256_load_ps(X + row_index[k + 24] * M_ROW + i + 0);
                __m256 neg120 = _mm256_load_ps(X + row_index[k + 25] * M_ROW + i + 0);
                __m256 pos130 = _mm256_load_ps(X + row_index[k + 26] * M_ROW + i + 0);
                __m256 neg130 = _mm256_load_ps(X + row_index[k + 27] * M_ROW + i + 0);
                __m256 pos140 = _mm256_load_ps(X + row_index[k + 28] * M_ROW + i + 0);
                __m256 neg140 = _mm256_load_ps(X + row_index[k + 29] * M_ROW + i + 0);
                __m256 pos150 = _mm256_load_ps(X + row_index[k + 30] * M_ROW + i + 0);
                __m256 neg150 = _mm256_load_ps(X + row_index[k + 31] * M_ROW + i + 0);
                __m256 pos160 = _mm256_load_ps(X + row_index[k + 32] * M_ROW + i + 0);
                __m256 neg160 = _mm256_load_ps(X + row_index[k + 33] * M_ROW + i + 0);
                __m256 pos170 = _mm256_load_ps(X + row_index[k + 34] * M_ROW + i + 0);
                __m256 neg170 = _mm256_load_ps(X + row_index[k + 35] * M_ROW + i + 0);
                __m256 pos180 = _mm256_load_ps(X + row_index[k + 36] * M_ROW + i + 0);
                __m256 neg180 = _mm256_load_ps(X + row_index[k + 37] * M_ROW + i + 0);
                __m256 pos190 = _mm256_load_ps(X + row_index[k + 38] * M_ROW + i + 0);
                __m256 neg190 = _mm256_load_ps(X + row_index[k + 39] * M_ROW + i + 0);
                __m256 pos200 = _mm256_load_ps(X + row_index[k + 40] * M_ROW + i + 0);
                __m256 neg200 = _mm256_load_ps(X + row_index[k + 41] * M_ROW + i + 0);
                __m256 pos210 = _mm256_load_ps(X + row_index[k + 42] * M_ROW + i + 0);
                __m256 neg210 = _mm256_load_ps(X + row_index[k + 43] * M_ROW + i + 0);
                __m256 pos220 = _mm256_load_ps(X + row_index[k + 44] * M_ROW + i + 0);
                __m256 neg220 = _mm256_load_ps(X + row_index[k + 45] * M_ROW + i + 0);
                __m256 pos230 = _mm256_load_ps(X + row_index[k + 46] * M_ROW + i + 0);
                __m256 neg230 = _mm256_load_ps(X + row_index[k + 47] * M_ROW + i + 0);
                __m256 pos240 = _mm256_load_ps(X + row_index[k + 48] * M_ROW + i + 0);
                __m256 neg240 = _mm256_load_ps(X + row_index[k + 49] * M_ROW + i + 0);
                __m256 pos250 = _mm256_load_ps(X + row_index[k + 50] * M_ROW + i + 0);
                __m256 neg250 = _mm256_load_ps(X + row_index[k + 51] * M_ROW + i + 0);
                __m256 pos260 = _mm256_load_ps(X + row_index[k + 52] * M_ROW + i + 0);
                __m256 neg260 = _mm256_load_ps(X + row_index[k + 53] * M_ROW + i + 0);
                __m256 pos270 = _mm256_load_ps(X + row_index[k + 54] * M_ROW + i + 0);
                __m256 neg270 = _mm256_load_ps(X + row_index[k + 55] * M_ROW + i + 0);
                __m256 pos280 = _mm256_load_ps(X + row_index[k + 56] * M_ROW + i + 0);
                __m256 neg280 = _mm256_load_ps(X + row_index[k + 57] * M_ROW + i + 0);
                __m256 pos290 = _mm256_load_ps(X + row_index[k + 58] * M_ROW + i + 0);
                __m256 neg290 = _mm256_load_ps(X + row_index[k + 59] * M_ROW + i + 0);
                __m256 pos300 = _mm256_load_ps(X + row_index[k + 60] * M_ROW + i + 0);
                __m256 neg300 = _mm256_load_ps(X + row_index[k + 61] * M_ROW + i + 0);
                __m256 pos310 = _mm256_load_ps(X + row_index[k + 62] * M_ROW + i + 0);
                __m256 neg310 = _mm256_load_ps(X + row_index[k + 63] * M_ROW + i + 0);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res20 = _mm256_add_ps(res20, _mm256_sub_ps(pos20, neg20));
                res30 = _mm256_add_ps(res30, _mm256_sub_ps(pos30, neg30));
                res40 = _mm256_add_ps(res40, _mm256_sub_ps(pos40, neg40));
                res50 = _mm256_add_ps(res50, _mm256_sub_ps(pos50, neg50));
                res60 = _mm256_add_ps(res60, _mm256_sub_ps(pos60, neg60));
                res70 = _mm256_add_ps(res70, _mm256_sub_ps(pos70, neg70));
                res80 = _mm256_add_ps(res80, _mm256_sub_ps(pos80, neg80));
                res90 = _mm256_add_ps(res90, _mm256_sub_ps(pos90, neg90));
                res100 = _mm256_add_ps(res100, _mm256_sub_ps(pos100, neg100));
                res110 = _mm256_add_ps(res110, _mm256_sub_ps(pos110, neg110));
                res120 = _mm256_add_ps(res120, _mm256_sub_ps(pos120, neg120));
                res130 = _mm256_add_ps(res130, _mm256_sub_ps(pos130, neg130));
                res140 = _mm256_add_ps(res140, _mm256_sub_ps(pos140, neg140));
                res150 = _mm256_add_ps(res150, _mm256_sub_ps(pos150, neg150));
                res160 = _mm256_add_ps(res160, _mm256_sub_ps(pos160, neg160));
                res170 = _mm256_add_ps(res170, _mm256_sub_ps(pos170, neg170));
                res180 = _mm256_add_ps(res180, _mm256_sub_ps(pos180, neg180));
                res190 = _mm256_add_ps(res190, _mm256_sub_ps(pos190, neg190));
                res200 = _mm256_add_ps(res200, _mm256_sub_ps(pos200, neg200));
                res210 = _mm256_add_ps(res210, _mm256_sub_ps(pos210, neg210));
                res220 = _mm256_add_ps(res220, _mm256_sub_ps(pos220, neg220));
                res230 = _mm256_add_ps(res230, _mm256_sub_ps(pos230, neg230));
                res240 = _mm256_add_ps(res240, _mm256_sub_ps(pos240, neg240));
                res250 = _mm256_add_ps(res250, _mm256_sub_ps(pos250, neg250));
                res260 = _mm256_add_ps(res260, _mm256_sub_ps(pos260, neg260));
                res270 = _mm256_add_ps(res270, _mm256_sub_ps(pos270, neg270));
                res280 = _mm256_add_ps(res280, _mm256_sub_ps(pos280, neg280));
                res290 = _mm256_add_ps(res290, _mm256_sub_ps(pos290, neg290));
                res300 = _mm256_add_ps(res300, _mm256_sub_ps(pos300, neg300));
                res310 = _mm256_add_ps(res310, _mm256_sub_ps(pos310, neg310));
            }
            _mm256_store_ps(result + (j * 32 + 0) * M_ROW + i + 0, res00);
            _mm256_store_ps(result + (j * 32 + 1) * M_ROW + i + 0, res10);
            _mm256_store_ps(result + (j * 32 + 2) * M_ROW + i + 0, res20);
            _mm256_store_ps(result + (j * 32 + 3) * M_ROW + i + 0, res30);
            _mm256_store_ps(result + (j * 32 + 4) * M_ROW + i + 0, res40);
            _mm256_store_ps(result + (j * 32 + 5) * M_ROW + i + 0, res50);
            _mm256_store_ps(result + (j * 32 + 6) * M_ROW + i + 0, res60);
            _mm256_store_ps(result + (j * 32 + 7) * M_ROW + i + 0, res70);
            _mm256_store_ps(result + (j * 32 + 8) * M_ROW + i + 0, res80);
            _mm256_store_ps(result + (j * 32 + 9) * M_ROW + i + 0, res90);
            _mm256_store_ps(result + (j * 32 + 10) * M_ROW + i + 0, res100);
            _mm256_store_ps(result + (j * 32 + 11) * M_ROW + i + 0, res110);
            _mm256_store_ps(result + (j * 32 + 12) * M_ROW + i + 0, res120);
            _mm256_store_ps(result + (j * 32 + 13) * M_ROW + i + 0, res130);
            _mm256_store_ps(result + (j * 32 + 14) * M_ROW + i + 0, res140);
            _mm256_store_ps(result + (j * 32 + 15) * M_ROW + i + 0, res150);
            _mm256_store_ps(result + (j * 32 + 16) * M_ROW + i + 0, res160);
            _mm256_store_ps(result + (j * 32 + 17) * M_ROW + i + 0, res170);
            _mm256_store_ps(result + (j * 32 + 18) * M_ROW + i + 0, res180);
            _mm256_store_ps(result + (j * 32 + 19) * M_ROW + i + 0, res190);
            _mm256_store_ps(result + (j * 32 + 20) * M_ROW + i + 0, res200);
            _mm256_store_ps(result + (j * 32 + 21) * M_ROW + i + 0, res210);
            _mm256_store_ps(result + (j * 32 + 22) * M_ROW + i + 0, res220);
            _mm256_store_ps(result + (j * 32 + 23) * M_ROW + i + 0, res230);
            _mm256_store_ps(result + (j * 32 + 24) * M_ROW + i + 0, res240);
            _mm256_store_ps(result + (j * 32 + 25) * M_ROW + i + 0, res250);
            _mm256_store_ps(result + (j * 32 + 26) * M_ROW + i + 0, res260);
            _mm256_store_ps(result + (j * 32 + 27) * M_ROW + i + 0, res270);
            _mm256_store_ps(result + (j * 32 + 28) * M_ROW + i + 0, res280);
            _mm256_store_ps(result + (j * 32 + 29) * M_ROW + i + 0, res290);
            _mm256_store_ps(result + (j * 32 + 30) * M_ROW + i + 0, res300);
            _mm256_store_ps(result + (j * 32 + 31) * M_ROW + i + 0, res310);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Uniform_16xG4_AVX2_OpenMP(const float* X, const int32_t NonZeroPerCol, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 4; j++) {
        for (int i = 0; i < M_ROW; i += 16) {
            __m256 res00 = _mm256_setzero_ps();
            __m256 res01 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res11 = _mm256_setzero_ps();
            __m256 res20 = _mm256_setzero_ps();
            __m256 res21 = _mm256_setzero_ps();
            __m256 res30 = _mm256_setzero_ps();
            __m256 res31 = _mm256_setzero_ps();
            for (int k = j * 4 * NonZeroPerCol; k < (j + 1) * 4 * NonZeroPerCol; k += 8) {
                __m256 pos00 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m256 neg00 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m256 pos10 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m256 neg10 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m256 pos20 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m256 neg20 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m256 pos30 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m256 neg30 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m256 pos01 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 8);
                __m256 neg01 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 8);
                __m256 pos11 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 8);
                __m256 neg11 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 8);
                __m256 pos21 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 8);
                __m256 neg21 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 8);
                __m256 pos31 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 8);
                __m256 neg31 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 8);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res20 = _mm256_add_ps(res20, _mm256_sub_ps(pos20, neg20));
                res30 = _mm256_add_ps(res30, _mm256_sub_ps(pos30, neg30));
                res01 = _mm256_add_ps(res01, _mm256_sub_ps(pos01, neg01));
                res11 = _mm256_add_ps(res11, _mm256_sub_ps(pos11, neg11));
                res21 = _mm256_add_ps(res21, _mm256_sub_ps(pos21, neg21));
                res31 = _mm256_add_ps(res31, _mm256_sub_ps(pos31, neg31));
            }
            _mm256_store_ps(result + (j * 4 + 0) * M_ROW + i + 0, res00);
            _mm256_store_ps(result + (j * 4 + 1) * M_ROW + i + 0, res10);
            _mm256_store_ps(result + (j * 4 + 2) * M_ROW + i + 0, res20);
            _mm256_store_ps(result + (j * 4 + 3) * M_ROW + i + 0, res30);
            _mm256_store_ps(result + (j * 4 + 0) * M_ROW + i + 8, res01);
            _mm256_store_ps(result + (j * 4 + 1) * M_ROW + i + 8, res11);
            _mm256_store_ps(result + (j * 4 + 2) * M_ROW + i + 8, res21);
            _mm256_store_ps(result + (j * 4 + 3) * M_ROW + i + 8, res31);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Uniform_16xG8_AVX2_OpenMP(const float* X, const int32_t NonZeroPerCol, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 8; j++) {
        for (int i = 0; i < M_ROW; i += 16) {
            __m256 res00 = _mm256_setzero_ps();
            __m256 res01 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res11 = _mm256_setzero_ps();
            __m256 res20 = _mm256_setzero_ps();
            __m256 res21 = _mm256_setzero_ps();
            __m256 res30 = _mm256_setzero_ps();
            __m256 res31 = _mm256_setzero_ps();
            __m256 res40 = _mm256_setzero_ps();
            __m256 res41 = _mm256_setzero_ps();
            __m256 res50 = _mm256_setzero_ps();
            __m256 res51 = _mm256_setzero_ps();
            __m256 res60 = _mm256_setzero_ps();
            __m256 res61 = _mm256_setzero_ps();
            __m256 res70 = _mm256_setzero_ps();
            __m256 res71 = _mm256_setzero_ps();
            for (int k = j * 8 * NonZeroPerCol; k < (j + 1) * 8 * NonZeroPerCol; k += 16) {
                __m256 pos00 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m256 neg00 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m256 pos10 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m256 neg10 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m256 pos20 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m256 neg20 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m256 pos30 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m256 neg30 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m256 pos40 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 0);
                __m256 neg40 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 0);
                __m256 pos50 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 0);
                __m256 neg50 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 0);
                __m256 pos60 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 0);
                __m256 neg60 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 0);
                __m256 pos70 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 0);
                __m256 neg70 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 0);
                __m256 pos01 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 8);
                __m256 neg01 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 8);
                __m256 pos11 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 8);
                __m256 neg11 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 8);
                __m256 pos21 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 8);
                __m256 neg21 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 8);
                __m256 pos31 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 8);
                __m256 neg31 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 8);
                __m256 pos41 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 8);
                __m256 neg41 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 8);
                __m256 pos51 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 8);
                __m256 neg51 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 8);
                __m256 pos61 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 8);
                __m256 neg61 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 8);
                __m256 pos71 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 8);
                __m256 neg71 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 8);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res20 = _mm256_add_ps(res20, _mm256_sub_ps(pos20, neg20));
                res30 = _mm256_add_ps(res30, _mm256_sub_ps(pos30, neg30));
                res40 = _mm256_add_ps(res40, _mm256_sub_ps(pos40, neg40));
                res50 = _mm256_add_ps(res50, _mm256_sub_ps(pos50, neg50));
                res60 = _mm256_add_ps(res60, _mm256_sub_ps(pos60, neg60));
                res70 = _mm256_add_ps(res70, _mm256_sub_ps(pos70, neg70));
                res01 = _mm256_add_ps(res01, _mm256_sub_ps(pos01, neg01));
                res11 = _mm256_add_ps(res11, _mm256_sub_ps(pos11, neg11));
                res21 = _mm256_add_ps(res21, _mm256_sub_ps(pos21, neg21));
                res31 = _mm256_add_ps(res31, _mm256_sub_ps(pos31, neg31));
                res41 = _mm256_add_ps(res41, _mm256_sub_ps(pos41, neg41));
                res51 = _mm256_add_ps(res51, _mm256_sub_ps(pos51, neg51));
                res61 = _mm256_add_ps(res61, _mm256_sub_ps(pos61, neg61));
                res71 = _mm256_add_ps(res71, _mm256_sub_ps(pos71, neg71));
            }
            _mm256_store_ps(result + (j * 8 + 0) * M_ROW + i + 0, res00);
            _mm256_store_ps(result + (j * 8 + 1) * M_ROW + i + 0, res10);
            _mm256_store_ps(result + (j * 8 + 2) * M_ROW + i + 0, res20);
            _mm256_store_ps(result + (j * 8 + 3) * M_ROW + i + 0, res30);
            _mm256_store_ps(result + (j * 8 + 4) * M_ROW + i + 0, res40);
            _mm256_store_ps(result + (j * 8 + 5) * M_ROW + i + 0, res50);
            _mm256_store_ps(result + (j * 8 + 6) * M_ROW + i + 0, res60);
            _mm256_store_ps(result + (j * 8 + 7) * M_ROW + i + 0, res70);
            _mm256_store_ps(result + (j * 8 + 0) * M_ROW + i + 8, res01);
            _mm256_store_ps(result + (j * 8 + 1) * M_ROW + i + 8, res11);
            _mm256_store_ps(result + (j * 8 + 2) * M_ROW + i + 8, res21);
            _mm256_store_ps(result + (j * 8 + 3) * M_ROW + i + 8, res31);
            _mm256_store_ps(result + (j * 8 + 4) * M_ROW + i + 8, res41);
            _mm256_store_ps(result + (j * 8 + 5) * M_ROW + i + 8, res51);
            _mm256_store_ps(result + (j * 8 + 6) * M_ROW + i + 8, res61);
            _mm256_store_ps(result + (j * 8 + 7) * M_ROW + i + 8, res71);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Uniform_16xG16_AVX2_OpenMP(const float* X, const int32_t NonZeroPerCol, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 16; j++) {
        for (int i = 0; i < M_ROW; i += 16) {
            __m256 res00 = _mm256_setzero_ps();
            __m256 res01 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res11 = _mm256_setzero_ps();
            __m256 res20 = _mm256_setzero_ps();
            __m256 res21 = _mm256_setzero_ps();
            __m256 res30 = _mm256_setzero_ps();
            __m256 res31 = _mm256_setzero_ps();
            __m256 res40 = _mm256_setzero_ps();
            __m256 res41 = _mm256_setzero_ps();
            __m256 res50 = _mm256_setzero_ps();
            __m256 res51 = _mm256_setzero_ps();
            __m256 res60 = _mm256_setzero_ps();
            __m256 res61 = _mm256_setzero_ps();
            __m256 res70 = _mm256_setzero_ps();
            __m256 res71 = _mm256_setzero_ps();
            __m256 res80 = _mm256_setzero_ps();
            __m256 res81 = _mm256_setzero_ps();
            __m256 res90 = _mm256_setzero_ps();
            __m256 res91 = _mm256_setzero_ps();
            __m256 res100 = _mm256_setzero_ps();
            __m256 res101 = _mm256_setzero_ps();
            __m256 res110 = _mm256_setzero_ps();
            __m256 res111 = _mm256_setzero_ps();
            __m256 res120 = _mm256_setzero_ps();
            __m256 res121 = _mm256_setzero_ps();
            __m256 res130 = _mm256_setzero_ps();
            __m256 res131 = _mm256_setzero_ps();
            __m256 res140 = _mm256_setzero_ps();
            __m256 res141 = _mm256_setzero_ps();
            __m256 res150 = _mm256_setzero_ps();
            __m256 res151 = _mm256_setzero_ps();
            for (int k = j * 16 * NonZeroPerCol; k < (j + 1) * 16 * NonZeroPerCol; k += 32) {
                __m256 pos00 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m256 neg00 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m256 pos10 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m256 neg10 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m256 pos20 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m256 neg20 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m256 pos30 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m256 neg30 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m256 pos40 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 0);
                __m256 neg40 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 0);
                __m256 pos50 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 0);
                __m256 neg50 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 0);
                __m256 pos60 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 0);
                __m256 neg60 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 0);
                __m256 pos70 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 0);
                __m256 neg70 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 0);
                __m256 pos80 = _mm256_load_ps(X + row_index[k + 16] * M_ROW + i + 0);
                __m256 neg80 = _mm256_load_ps(X + row_index[k + 17] * M_ROW + i + 0);
                __m256 pos90 = _mm256_load_ps(X + row_index[k + 18] * M_ROW + i + 0);
                __m256 neg90 = _mm256_load_ps(X + row_index[k + 19] * M_ROW + i + 0);
                __m256 pos100 = _mm256_load_ps(X + row_index[k + 20] * M_ROW + i + 0);
                __m256 neg100 = _mm256_load_ps(X + row_index[k + 21] * M_ROW + i + 0);
                __m256 pos110 = _mm256_load_ps(X + row_index[k + 22] * M_ROW + i + 0);
                __m256 neg110 = _mm256_load_ps(X + row_index[k + 23] * M_ROW + i + 0);
                __m256 pos120 = _mm256_load_ps(X + row_index[k + 24] * M_ROW + i + 0);
                __m256 neg120 = _mm256_load_ps(X + row_index[k + 25] * M_ROW + i + 0);
                __m256 pos130 = _mm256_load_ps(X + row_index[k + 26] * M_ROW + i + 0);
                __m256 neg130 = _mm256_load_ps(X + row_index[k + 27] * M_ROW + i + 0);
                __m256 pos140 = _mm256_load_ps(X + row_index[k + 28] * M_ROW + i + 0);
                __m256 neg140 = _mm256_load_ps(X + row_index[k + 29] * M_ROW + i + 0);
                __m256 pos150 = _mm256_load_ps(X + row_index[k + 30] * M_ROW + i + 0);
                __m256 neg150 = _mm256_load_ps(X + row_index[k + 31] * M_ROW + i + 0);
                __m256 pos01 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 8);
                __m256 neg01 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 8);
                __m256 pos11 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 8);
                __m256 neg11 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 8);
                __m256 pos21 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 8);
                __m256 neg21 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 8);
                __m256 pos31 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 8);
                __m256 neg31 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 8);
                __m256 pos41 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 8);
                __m256 neg41 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 8);
                __m256 pos51 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 8);
                __m256 neg51 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 8);
                __m256 pos61 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 8);
                __m256 neg61 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 8);
                __m256 pos71 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 8);
                __m256 neg71 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 8);
                __m256 pos81 = _mm256_load_ps(X + row_index[k + 16] * M_ROW + i + 8);
                __m256 neg81 = _mm256_load_ps(X + row_index[k + 17] * M_ROW + i + 8);
                __m256 pos91 = _mm256_load_ps(X + row_index[k + 18] * M_ROW + i + 8);
                __m256 neg91 = _mm256_load_ps(X + row_index[k + 19] * M_ROW + i + 8);
                __m256 pos101 = _mm256_load_ps(X + row_index[k + 20] * M_ROW + i + 8);
                __m256 neg101 = _mm256_load_ps(X + row_index[k + 21] * M_ROW + i + 8);
                __m256 pos111 = _mm256_load_ps(X + row_index[k + 22] * M_ROW + i + 8);
                __m256 neg111 = _mm256_load_ps(X + row_index[k + 23] * M_ROW + i + 8);
                __m256 pos121 = _mm256_load_ps(X + row_index[k + 24] * M_ROW + i + 8);
                __m256 neg121 = _mm256_load_ps(X + row_index[k + 25] * M_ROW + i + 8);
                __m256 pos131 = _mm256_load_ps(X + row_index[k + 26] * M_ROW + i + 8);
                __m256 neg131 = _mm256_load_ps(X + row_index[k + 27] * M_ROW + i + 8);
                __m256 pos141 = _mm256_load_ps(X + row_index[k + 28] * M_ROW + i + 8);
                __m256 neg141 = _mm256_load_ps(X + row_index[k + 29] * M_ROW + i + 8);
                __m256 pos151 = _mm256_load_ps(X + row_index[k + 30] * M_ROW + i + 8);
                __m256 neg151 = _mm256_load_ps(X + row_index[k + 31] * M_ROW + i + 8);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res20 = _mm256_add_ps(res20, _mm256_sub_ps(pos20, neg20));
                res30 = _mm256_add_ps(res30, _mm256_sub_ps(pos30, neg30));
                res40 = _mm256_add_ps(res40, _mm256_sub_ps(pos40, neg40));
                res50 = _mm256_add_ps(res50, _mm256_sub_ps(pos50, neg50));
                res60 = _mm256_add_ps(res60, _mm256_sub_ps(pos60, neg60));
                res70 = _mm256_add_ps(res70, _mm256_sub_ps(pos70, neg70));
                res80 = _mm256_add_ps(res80, _mm256_sub_ps(pos80, neg80));
                res90 = _mm256_add_ps(res90, _mm256_sub_ps(pos90, neg90));
                res100 = _mm256_add_ps(res100, _mm256_sub_ps(pos100, neg100));
                res110 = _mm256_add_ps(res110, _mm256_sub_ps(pos110, neg110));
                res120 = _mm256_add_ps(res120, _mm256_sub_ps(pos120, neg120));
                res130 = _mm256_add_ps(res130, _mm256_sub_ps(pos130, neg130));
                res140 = _mm256_add_ps(res140, _mm256_sub_ps(pos140, neg140));
                res150 = _mm256_add_ps(res150, _mm256_sub_ps(pos150, neg150));
                res01 = _mm256_add_ps(res01, _mm256_sub_ps(pos01, neg01));
                res11 = _mm256_add_ps(res11, _mm256_sub_ps(pos11, neg11));
                res21 = _mm256_add_ps(res21, _mm256_sub_ps(pos21, neg21));
                res31 = _mm256_add_ps(res31, _mm256_sub_ps(pos31, neg31));
                res41 = _mm256_add_ps(res41, _mm256_sub_ps(pos41, neg41));
                res51 = _mm256_add_ps(res51, _mm256_sub_ps(pos51, neg51));
                res61 = _mm256_add_ps(res61, _mm256_sub_ps(pos61, neg61));
                res71 = _mm256_add_ps(res71, _mm256_sub_ps(pos71, neg71));
                res81 = _mm256_add_ps(res81, _mm256_sub_ps(pos81, neg81));
                res91 = _mm256_add_ps(res91, _mm256_sub_ps(pos91, neg91));
                res101 = _mm256_add_ps(res101, _mm256_sub_ps(pos101, neg101));
                res111 = _mm256_add_ps(res111, _mm256_sub_ps(pos111, neg111));
                res121 = _mm256_add_ps(res121, _mm256_sub_ps(pos121, neg121));
                res131 = _mm256_add_ps(res131, _mm256_sub_ps(pos131, neg131));
                res141 = _mm256_add_ps(res141, _mm256_sub_ps(pos141, neg141));
                res151 = _mm256_add_ps(res151, _mm256_sub_ps(pos151, neg151));
            }
            _mm256_store_ps(result + (j * 16 + 0) * M_ROW + i + 0, res00);
            _mm256_store_ps(result + (j * 16 + 1) * M_ROW + i + 0, res10);
            _mm256_store_ps(result + (j * 16 + 2) * M_ROW + i + 0, res20);
            _mm256_store_ps(result + (j * 16 + 3) * M_ROW + i + 0, res30);
            _mm256_store_ps(result + (j * 16 + 4) * M_ROW + i + 0, res40);
            _mm256_store_ps(result + (j * 16 + 5) * M_ROW + i + 0, res50);
            _mm256_store_ps(result + (j * 16 + 6) * M_ROW + i + 0, res60);
            _mm256_store_ps(result + (j * 16 + 7) * M_ROW + i + 0, res70);
            _mm256_store_ps(result + (j * 16 + 8) * M_ROW + i + 0, res80);
            _mm256_store_ps(result + (j * 16 + 9) * M_ROW + i + 0, res90);
            _mm256_store_ps(result + (j * 16 + 10) * M_ROW + i + 0, res100);
            _mm256_store_ps(result + (j * 16 + 11) * M_ROW + i + 0, res110);
            _mm256_store_ps(result + (j * 16 + 12) * M_ROW + i + 0, res120);
            _mm256_store_ps(result + (j * 16 + 13) * M_ROW + i + 0, res130);
            _mm256_store_ps(result + (j * 16 + 14) * M_ROW + i + 0, res140);
            _mm256_store_ps(result + (j * 16 + 15) * M_ROW + i + 0, res150);
            _mm256_store_ps(result + (j * 16 + 0) * M_ROW + i + 8, res01);
            _mm256_store_ps(result + (j * 16 + 1) * M_ROW + i + 8, res11);
            _mm256_store_ps(result + (j * 16 + 2) * M_ROW + i + 8, res21);
            _mm256_store_ps(result + (j * 16 + 3) * M_ROW + i + 8, res31);
            _mm256_store_ps(result + (j * 16 + 4) * M_ROW + i + 8, res41);
            _mm256_store_ps(result + (j * 16 + 5) * M_ROW + i + 8, res51);
            _mm256_store_ps(result + (j * 16 + 6) * M_ROW + i + 8, res61);
            _mm256_store_ps(result + (j * 16 + 7) * M_ROW + i + 8, res71);
            _mm256_store_ps(result + (j * 16 + 8) * M_ROW + i + 8, res81);
            _mm256_store_ps(result + (j * 16 + 9) * M_ROW + i + 8, res91);
            _mm256_store_ps(result + (j * 16 + 10) * M_ROW + i + 8, res101);
            _mm256_store_ps(result + (j * 16 + 11) * M_ROW + i + 8, res111);
            _mm256_store_ps(result + (j * 16 + 12) * M_ROW + i + 8, res121);
            _mm256_store_ps(result + (j * 16 + 13) * M_ROW + i + 8, res131);
            _mm256_store_ps(result + (j * 16 + 14) * M_ROW + i + 8, res141);
            _mm256_store_ps(result + (j * 16 + 15) * M_ROW + i + 8, res151);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Uniform_16xG32_AVX2_OpenMP(const float* X, const int32_t NonZeroPerCol, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 32; j++) {
        for (int i = 0; i < M_ROW; i += 16) {
            __m256 res00 = _mm256_setzero_ps();
            __m256 res01 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res11 = _mm256_setzero_ps();
            __m256 res20 = _mm256_setzero_ps();
            __m256 res21 = _mm256_setzero_ps();
            __m256 res30 = _mm256_setzero_ps();
            __m256 res31 = _mm256_setzero_ps();
            __m256 res40 = _mm256_setzero_ps();
            __m256 res41 = _mm256_setzero_ps();
            __m256 res50 = _mm256_setzero_ps();
            __m256 res51 = _mm256_setzero_ps();
            __m256 res60 = _mm256_setzero_ps();
            __m256 res61 = _mm256_setzero_ps();
            __m256 res70 = _mm256_setzero_ps();
            __m256 res71 = _mm256_setzero_ps();
            __m256 res80 = _mm256_setzero_ps();
            __m256 res81 = _mm256_setzero_ps();
            __m256 res90 = _mm256_setzero_ps();
            __m256 res91 = _mm256_setzero_ps();
            __m256 res100 = _mm256_setzero_ps();
            __m256 res101 = _mm256_setzero_ps();
            __m256 res110 = _mm256_setzero_ps();
            __m256 res111 = _mm256_setzero_ps();
            __m256 res120 = _mm256_setzero_ps();
            __m256 res121 = _mm256_setzero_ps();
            __m256 res130 = _mm256_setzero_ps();
            __m256 res131 = _mm256_setzero_ps();
            __m256 res140 = _mm256_setzero_ps();
            __m256 res141 = _mm256_setzero_ps();
            __m256 res150 = _mm256_setzero_ps();
            __m256 res151 = _mm256_setzero_ps();
            __m256 res160 = _mm256_setzero_ps();
            __m256 res161 = _mm256_setzero_ps();
            __m256 res170 = _mm256_setzero_ps();
            __m256 res171 = _mm256_setzero_ps();
            __m256 res180 = _mm256_setzero_ps();
            __m256 res181 = _mm256_setzero_ps();
            __m256 res190 = _mm256_setzero_ps();
            __m256 res191 = _mm256_setzero_ps();
            __m256 res200 = _mm256_setzero_ps();
            __m256 res201 = _mm256_setzero_ps();
            __m256 res210 = _mm256_setzero_ps();
            __m256 res211 = _mm256_setzero_ps();
            __m256 res220 = _mm256_setzero_ps();
            __m256 res221 = _mm256_setzero_ps();
            __m256 res230 = _mm256_setzero_ps();
            __m256 res231 = _mm256_setzero_ps();
            __m256 res240 = _mm256_setzero_ps();
            __m256 res241 = _mm256_setzero_ps();
            __m256 res250 = _mm256_setzero_ps();
            __m256 res251 = _mm256_setzero_ps();
            __m256 res260 = _mm256_setzero_ps();
            __m256 res261 = _mm256_setzero_ps();
            __m256 res270 = _mm256_setzero_ps();
            __m256 res271 = _mm256_setzero_ps();
            __m256 res280 = _mm256_setzero_ps();
            __m256 res281 = _mm256_setzero_ps();
            __m256 res290 = _mm256_setzero_ps();
            __m256 res291 = _mm256_setzero_ps();
            __m256 res300 = _mm256_setzero_ps();
            __m256 res301 = _mm256_setzero_ps();
            __m256 res310 = _mm256_setzero_ps();
            __m256 res311 = _mm256_setzero_ps();
            for (int k = j * 32 * NonZeroPerCol; k < (j + 1) * 32 * NonZeroPerCol; k += 64) {
                __m256 pos00 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m256 neg00 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m256 pos10 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m256 neg10 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m256 pos20 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m256 neg20 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m256 pos30 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m256 neg30 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m256 pos40 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 0);
                __m256 neg40 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 0);
                __m256 pos50 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 0);
                __m256 neg50 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 0);
                __m256 pos60 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 0);
                __m256 neg60 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 0);
                __m256 pos70 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 0);
                __m256 neg70 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 0);
                __m256 pos80 = _mm256_load_ps(X + row_index[k + 16] * M_ROW + i + 0);
                __m256 neg80 = _mm256_load_ps(X + row_index[k + 17] * M_ROW + i + 0);
                __m256 pos90 = _mm256_load_ps(X + row_index[k + 18] * M_ROW + i + 0);
                __m256 neg90 = _mm256_load_ps(X + row_index[k + 19] * M_ROW + i + 0);
                __m256 pos100 = _mm256_load_ps(X + row_index[k + 20] * M_ROW + i + 0);
                __m256 neg100 = _mm256_load_ps(X + row_index[k + 21] * M_ROW + i + 0);
                __m256 pos110 = _mm256_load_ps(X + row_index[k + 22] * M_ROW + i + 0);
                __m256 neg110 = _mm256_load_ps(X + row_index[k + 23] * M_ROW + i + 0);
                __m256 pos120 = _mm256_load_ps(X + row_index[k + 24] * M_ROW + i + 0);
                __m256 neg120 = _mm256_load_ps(X + row_index[k + 25] * M_ROW + i + 0);
                __m256 pos130 = _mm256_load_ps(X + row_index[k + 26] * M_ROW + i + 0);
                __m256 neg130 = _mm256_load_ps(X + row_index[k + 27] * M_ROW + i + 0);
                __m256 pos140 = _mm256_load_ps(X + row_index[k + 28] * M_ROW + i + 0);
                __m256 neg140 = _mm256_load_ps(X + row_index[k + 29] * M_ROW + i + 0);
                __m256 pos150 = _mm256_load_ps(X + row_index[k + 30] * M_ROW + i + 0);
                __m256 neg150 = _mm256_load_ps(X + row_index[k + 31] * M_ROW + i + 0);
                __m256 pos160 = _mm256_load_ps(X + row_index[k + 32] * M_ROW + i + 0);
                __m256 neg160 = _mm256_load_ps(X + row_index[k + 33] * M_ROW + i + 0);
                __m256 pos170 = _mm256_load_ps(X + row_index[k + 34] * M_ROW + i + 0);
                __m256 neg170 = _mm256_load_ps(X + row_index[k + 35] * M_ROW + i + 0);
                __m256 pos180 = _mm256_load_ps(X + row_index[k + 36] * M_ROW + i + 0);
                __m256 neg180 = _mm256_load_ps(X + row_index[k + 37] * M_ROW + i + 0);
                __m256 pos190 = _mm256_load_ps(X + row_index[k + 38] * M_ROW + i + 0);
                __m256 neg190 = _mm256_load_ps(X + row_index[k + 39] * M_ROW + i + 0);
                __m256 pos200 = _mm256_load_ps(X + row_index[k + 40] * M_ROW + i + 0);
                __m256 neg200 = _mm256_load_ps(X + row_index[k + 41] * M_ROW + i + 0);
                __m256 pos210 = _mm256_load_ps(X + row_index[k + 42] * M_ROW + i + 0);
                __m256 neg210 = _mm256_load_ps(X + row_index[k + 43] * M_ROW + i + 0);
                __m256 pos220 = _mm256_load_ps(X + row_index[k + 44] * M_ROW + i + 0);
                __m256 neg220 = _mm256_load_ps(X + row_index[k + 45] * M_ROW + i + 0);
                __m256 pos230 = _mm256_load_ps(X + row_index[k + 46] * M_ROW + i + 0);
                __m256 neg230 = _mm256_load_ps(X + row_index[k + 47] * M_ROW + i + 0);
                __m256 pos240 = _mm256_load_ps(X + row_index[k + 48] * M_ROW + i + 0);
                __m256 neg240 = _mm256_load_ps(X + row_index[k + 49] * M_ROW + i + 0);
                __m256 pos250 = _mm256_load_ps(X + row_index[k + 50] * M_ROW + i + 0);
                __m256 neg250 = _mm256_load_ps(X + row_index[k + 51] * M_ROW + i + 0);
                __m256 pos260 = _mm256_load_ps(X + row_index[k + 52] * M_ROW + i + 0);
                __m256 neg260 = _mm256_load_ps(X + row_index[k + 53] * M_ROW + i + 0);
                __m256 pos270 = _mm256_load_ps(X + row_index[k + 54] * M_ROW + i + 0);
                __m256 neg270 = _mm256_load_ps(X + row_index[k + 55] * M_ROW + i + 0);
                __m256 pos280 = _mm256_load_ps(X + row_index[k + 56] * M_ROW + i + 0);
                __m256 neg280 = _mm256_load_ps(X + row_index[k + 57] * M_ROW + i + 0);
                __m256 pos290 = _mm256_load_ps(X + row_index[k + 58] * M_ROW + i + 0);
                __m256 neg290 = _mm256_load_ps(X + row_index[k + 59] * M_ROW + i + 0);
                __m256 pos300 = _mm256_load_ps(X + row_index[k + 60] * M_ROW + i + 0);
                __m256 neg300 = _mm256_load_ps(X + row_index[k + 61] * M_ROW + i + 0);
                __m256 pos310 = _mm256_load_ps(X + row_index[k + 62] * M_ROW + i + 0);
                __m256 neg310 = _mm256_load_ps(X + row_index[k + 63] * M_ROW + i + 0);
                __m256 pos01 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 8);
                __m256 neg01 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 8);
                __m256 pos11 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 8);
                __m256 neg11 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 8);
                __m256 pos21 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 8);
                __m256 neg21 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 8);
                __m256 pos31 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 8);
                __m256 neg31 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 8);
                __m256 pos41 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 8);
                __m256 neg41 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 8);
                __m256 pos51 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 8);
                __m256 neg51 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 8);
                __m256 pos61 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 8);
                __m256 neg61 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 8);
                __m256 pos71 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 8);
                __m256 neg71 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 8);
                __m256 pos81 = _mm256_load_ps(X + row_index[k + 16] * M_ROW + i + 8);
                __m256 neg81 = _mm256_load_ps(X + row_index[k + 17] * M_ROW + i + 8);
                __m256 pos91 = _mm256_load_ps(X + row_index[k + 18] * M_ROW + i + 8);
                __m256 neg91 = _mm256_load_ps(X + row_index[k + 19] * M_ROW + i + 8);
                __m256 pos101 = _mm256_load_ps(X + row_index[k + 20] * M_ROW + i + 8);
                __m256 neg101 = _mm256_load_ps(X + row_index[k + 21] * M_ROW + i + 8);
                __m256 pos111 = _mm256_load_ps(X + row_index[k + 22] * M_ROW + i + 8);
                __m256 neg111 = _mm256_load_ps(X + row_index[k + 23] * M_ROW + i + 8);
                __m256 pos121 = _mm256_load_ps(X + row_index[k + 24] * M_ROW + i + 8);
                __m256 neg121 = _mm256_load_ps(X + row_index[k + 25] * M_ROW + i + 8);
                __m256 pos131 = _mm256_load_ps(X + row_index[k + 26] * M_ROW + i + 8);
                __m256 neg131 = _mm256_load_ps(X + row_index[k + 27] * M_ROW + i + 8);
                __m256 pos141 = _mm256_load_ps(X + row_index[k + 28] * M_ROW + i + 8);
                __m256 neg141 = _mm256_load_ps(X + row_index[k + 29] * M_ROW + i + 8);
                __m256 pos151 = _mm256_load_ps(X + row_index[k + 30] * M_ROW + i + 8);
                __m256 neg151 = _mm256_load_ps(X + row_index[k + 31] * M_ROW + i + 8);
                __m256 pos161 = _mm256_load_ps(X + row_index[k + 32] * M_ROW + i + 8);
                __m256 neg161 = _mm256_load_ps(X + row_index[k + 33] * M_ROW + i + 8);
                __m256 pos171 = _mm256_load_ps(X + row_index[k + 34] * M_ROW + i + 8);
                __m256 neg171 = _mm256_load_ps(X + row_index[k + 35] * M_ROW + i + 8);
                __m256 pos181 = _mm256_load_ps(X + row_index[k + 36] * M_ROW + i + 8);
                __m256 neg181 = _mm256_load_ps(X + row_index[k + 37] * M_ROW + i + 8);
                __m256 pos191 = _mm256_load_ps(X + row_index[k + 38] * M_ROW + i + 8);
                __m256 neg191 = _mm256_load_ps(X + row_index[k + 39] * M_ROW + i + 8);
                __m256 pos201 = _mm256_load_ps(X + row_index[k + 40] * M_ROW + i + 8);
                __m256 neg201 = _mm256_load_ps(X + row_index[k + 41] * M_ROW + i + 8);
                __m256 pos211 = _mm256_load_ps(X + row_index[k + 42] * M_ROW + i + 8);
                __m256 neg211 = _mm256_load_ps(X + row_index[k + 43] * M_ROW + i + 8);
                __m256 pos221 = _mm256_load_ps(X + row_index[k + 44] * M_ROW + i + 8);
                __m256 neg221 = _mm256_load_ps(X + row_index[k + 45] * M_ROW + i + 8);
                __m256 pos231 = _mm256_load_ps(X + row_index[k + 46] * M_ROW + i + 8);
                __m256 neg231 = _mm256_load_ps(X + row_index[k + 47] * M_ROW + i + 8);
                __m256 pos241 = _mm256_load_ps(X + row_index[k + 48] * M_ROW + i + 8);
                __m256 neg241 = _mm256_load_ps(X + row_index[k + 49] * M_ROW + i + 8);
                __m256 pos251 = _mm256_load_ps(X + row_index[k + 50] * M_ROW + i + 8);
                __m256 neg251 = _mm256_load_ps(X + row_index[k + 51] * M_ROW + i + 8);
                __m256 pos261 = _mm256_load_ps(X + row_index[k + 52] * M_ROW + i + 8);
                __m256 neg261 = _mm256_load_ps(X + row_index[k + 53] * M_ROW + i + 8);
                __m256 pos271 = _mm256_load_ps(X + row_index[k + 54] * M_ROW + i + 8);
                __m256 neg271 = _mm256_load_ps(X + row_index[k + 55] * M_ROW + i + 8);
                __m256 pos281 = _mm256_load_ps(X + row_index[k + 56] * M_ROW + i + 8);
                __m256 neg281 = _mm256_load_ps(X + row_index[k + 57] * M_ROW + i + 8);
                __m256 pos291 = _mm256_load_ps(X + row_index[k + 58] * M_ROW + i + 8);
                __m256 neg291 = _mm256_load_ps(X + row_index[k + 59] * M_ROW + i + 8);
                __m256 pos301 = _mm256_load_ps(X + row_index[k + 60] * M_ROW + i + 8);
                __m256 neg301 = _mm256_load_ps(X + row_index[k + 61] * M_ROW + i + 8);
                __m256 pos311 = _mm256_load_ps(X + row_index[k + 62] * M_ROW + i + 8);
                __m256 neg311 = _mm256_load_ps(X + row_index[k + 63] * M_ROW + i + 8);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res20 = _mm256_add_ps(res20, _mm256_sub_ps(pos20, neg20));
                res30 = _mm256_add_ps(res30, _mm256_sub_ps(pos30, neg30));
                res40 = _mm256_add_ps(res40, _mm256_sub_ps(pos40, neg40));
                res50 = _mm256_add_ps(res50, _mm256_sub_ps(pos50, neg50));
                res60 = _mm256_add_ps(res60, _mm256_sub_ps(pos60, neg60));
                res70 = _mm256_add_ps(res70, _mm256_sub_ps(pos70, neg70));
                res80 = _mm256_add_ps(res80, _mm256_sub_ps(pos80, neg80));
                res90 = _mm256_add_ps(res90, _mm256_sub_ps(pos90, neg90));
                res100 = _mm256_add_ps(res100, _mm256_sub_ps(pos100, neg100));
                res110 = _mm256_add_ps(res110, _mm256_sub_ps(pos110, neg110));
                res120 = _mm256_add_ps(res120, _mm256_sub_ps(pos120, neg120));
                res130 = _mm256_add_ps(res130, _mm256_sub_ps(pos130, neg130));
                res140 = _mm256_add_ps(res140, _mm256_sub_ps(pos140, neg140));
                res150 = _mm256_add_ps(res150, _mm256_sub_ps(pos150, neg150));
                res160 = _mm256_add_ps(res160, _mm256_sub_ps(pos160, neg160));
                res170 = _mm256_add_ps(res170, _mm256_sub_ps(pos170, neg170));
                res180 = _mm256_add_ps(res180, _mm256_sub_ps(pos180, neg180));
                res190 = _mm256_add_ps(res190, _mm256_sub_ps(pos190, neg190));
                res200 = _mm256_add_ps(res200, _mm256_sub_ps(pos200, neg200));
                res210 = _mm256_add_ps(res210, _mm256_sub_ps(pos210, neg210));
                res220 = _mm256_add_ps(res220, _mm256_sub_ps(pos220, neg220));
                res230 = _mm256_add_ps(res230, _mm256_sub_ps(pos230, neg230));
                res240 = _mm256_add_ps(res240, _mm256_sub_ps(pos240, neg240));
                res250 = _mm256_add_ps(res250, _mm256_sub_ps(pos250, neg250));
                res260 = _mm256_add_ps(res260, _mm256_sub_ps(pos260, neg260));
                res270 = _mm256_add_ps(res270, _mm256_sub_ps(pos270, neg270));
                res280 = _mm256_add_ps(res280, _mm256_sub_ps(pos280, neg280));
                res290 = _mm256_add_ps(res290, _mm256_sub_ps(pos290, neg290));
                res300 = _mm256_add_ps(res300, _mm256_sub_ps(pos300, neg300));
                res310 = _mm256_add_ps(res310, _mm256_sub_ps(pos310, neg310));
                res01 = _mm256_add_ps(res01, _mm256_sub_ps(pos01, neg01));
                res11 = _mm256_add_ps(res11, _mm256_sub_ps(pos11, neg11));
                res21 = _mm256_add_ps(res21, _mm256_sub_ps(pos21, neg21));
                res31 = _mm256_add_ps(res31, _mm256_sub_ps(pos31, neg31));
                res41 = _mm256_add_ps(res41, _mm256_sub_ps(pos41, neg41));
                res51 = _mm256_add_ps(res51, _mm256_sub_ps(pos51, neg51));
                res61 = _mm256_add_ps(res61, _mm256_sub_ps(pos61, neg61));
                res71 = _mm256_add_ps(res71, _mm256_sub_ps(pos71, neg71));
                res81 = _mm256_add_ps(res81, _mm256_sub_ps(pos81, neg81));
                res91 = _mm256_add_ps(res91, _mm256_sub_ps(pos91, neg91));
                res101 = _mm256_add_ps(res101, _mm256_sub_ps(pos101, neg101));
                res111 = _mm256_add_ps(res111, _mm256_sub_ps(pos111, neg111));
                res121 = _mm256_add_ps(res121, _mm256_sub_ps(pos121, neg121));
                res131 = _mm256_add_ps(res131, _mm256_sub_ps(pos131, neg131));
                res141 = _mm256_add_ps(res141, _mm256_sub_ps(pos141, neg141));
                res151 = _mm256_add_ps(res151, _mm256_sub_ps(pos151, neg151));
                res161 = _mm256_add_ps(res161, _mm256_sub_ps(pos161, neg161));
                res171 = _mm256_add_ps(res171, _mm256_sub_ps(pos171, neg171));
                res181 = _mm256_add_ps(res181, _mm256_sub_ps(pos181, neg181));
                res191 = _mm256_add_ps(res191, _mm256_sub_ps(pos191, neg191));
                res201 = _mm256_add_ps(res201, _mm256_sub_ps(pos201, neg201));
                res211 = _mm256_add_ps(res211, _mm256_sub_ps(pos211, neg211));
                res221 = _mm256_add_ps(res221, _mm256_sub_ps(pos221, neg221));
                res231 = _mm256_add_ps(res231, _mm256_sub_ps(pos231, neg231));
                res241 = _mm256_add_ps(res241, _mm256_sub_ps(pos241, neg241));
                res251 = _mm256_add_ps(res251, _mm256_sub_ps(pos251, neg251));
                res261 = _mm256_add_ps(res261, _mm256_sub_ps(pos261, neg261));
                res271 = _mm256_add_ps(res271, _mm256_sub_ps(pos271, neg271));
                res281 = _mm256_add_ps(res281, _mm256_sub_ps(pos281, neg281));
                res291 = _mm256_add_ps(res291, _mm256_sub_ps(pos291, neg291));
                res301 = _mm256_add_ps(res301, _mm256_sub_ps(pos301, neg301));
                res311 = _mm256_add_ps(res311, _mm256_sub_ps(pos311, neg311));
            }
            _mm256_store_ps(result + (j * 32 + 0) * M_ROW + i + 0, res00);
            _mm256_store_ps(result + (j * 32 + 1) * M_ROW + i + 0, res10);
            _mm256_store_ps(result + (j * 32 + 2) * M_ROW + i + 0, res20);
            _mm256_store_ps(result + (j * 32 + 3) * M_ROW + i + 0, res30);
            _mm256_store_ps(result + (j * 32 + 4) * M_ROW + i + 0, res40);
            _mm256_store_ps(result + (j * 32 + 5) * M_ROW + i + 0, res50);
            _mm256_store_ps(result + (j * 32 + 6) * M_ROW + i + 0, res60);
            _mm256_store_ps(result + (j * 32 + 7) * M_ROW + i + 0, res70);
            _mm256_store_ps(result + (j * 32 + 8) * M_ROW + i + 0, res80);
            _mm256_store_ps(result + (j * 32 + 9) * M_ROW + i + 0, res90);
            _mm256_store_ps(result + (j * 32 + 10) * M_ROW + i + 0, res100);
            _mm256_store_ps(result + (j * 32 + 11) * M_ROW + i + 0, res110);
            _mm256_store_ps(result + (j * 32 + 12) * M_ROW + i + 0, res120);
            _mm256_store_ps(result + (j * 32 + 13) * M_ROW + i + 0, res130);
            _mm256_store_ps(result + (j * 32 + 14) * M_ROW + i + 0, res140);
            _mm256_store_ps(result + (j * 32 + 15) * M_ROW + i + 0, res150);
            _mm256_store_ps(result + (j * 32 + 16) * M_ROW + i + 0, res160);
            _mm256_store_ps(result + (j * 32 + 17) * M_ROW + i + 0, res170);
            _mm256_store_ps(result + (j * 32 + 18) * M_ROW + i + 0, res180);
            _mm256_store_ps(result + (j * 32 + 19) * M_ROW + i + 0, res190);
            _mm256_store_ps(result + (j * 32 + 20) * M_ROW + i + 0, res200);
            _mm256_store_ps(result + (j * 32 + 21) * M_ROW + i + 0, res210);
            _mm256_store_ps(result + (j * 32 + 22) * M_ROW + i + 0, res220);
            _mm256_store_ps(result + (j * 32 + 23) * M_ROW + i + 0, res230);
            _mm256_store_ps(result + (j * 32 + 24) * M_ROW + i + 0, res240);
            _mm256_store_ps(result + (j * 32 + 25) * M_ROW + i + 0, res250);
            _mm256_store_ps(result + (j * 32 + 26) * M_ROW + i + 0, res260);
            _mm256_store_ps(result + (j * 32 + 27) * M_ROW + i + 0, res270);
            _mm256_store_ps(result + (j * 32 + 28) * M_ROW + i + 0, res280);
            _mm256_store_ps(result + (j * 32 + 29) * M_ROW + i + 0, res290);
            _mm256_store_ps(result + (j * 32 + 30) * M_ROW + i + 0, res300);
            _mm256_store_ps(result + (j * 32 + 31) * M_ROW + i + 0, res310);
            _mm256_store_ps(result + (j * 32 + 0) * M_ROW + i + 8, res01);
            _mm256_store_ps(result + (j * 32 + 1) * M_ROW + i + 8, res11);
            _mm256_store_ps(result + (j * 32 + 2) * M_ROW + i + 8, res21);
            _mm256_store_ps(result + (j * 32 + 3) * M_ROW + i + 8, res31);
            _mm256_store_ps(result + (j * 32 + 4) * M_ROW + i + 8, res41);
            _mm256_store_ps(result + (j * 32 + 5) * M_ROW + i + 8, res51);
            _mm256_store_ps(result + (j * 32 + 6) * M_ROW + i + 8, res61);
            _mm256_store_ps(result + (j * 32 + 7) * M_ROW + i + 8, res71);
            _mm256_store_ps(result + (j * 32 + 8) * M_ROW + i + 8, res81);
            _mm256_store_ps(result + (j * 32 + 9) * M_ROW + i + 8, res91);
            _mm256_store_ps(result + (j * 32 + 10) * M_ROW + i + 8, res101);
            _mm256_store_ps(result + (j * 32 + 11) * M_ROW + i + 8, res111);
            _mm256_store_ps(result + (j * 32 + 12) * M_ROW + i + 8, res121);
            _mm256_store_ps(result + (j * 32 + 13) * M_ROW + i + 8, res131);
            _mm256_store_ps(result + (j * 32 + 14) * M_ROW + i + 8, res141);
            _mm256_store_ps(result + (j * 32 + 15) * M_ROW + i + 8, res151);
            _mm256_store_ps(result + (j * 32 + 16) * M_ROW + i + 8, res161);
            _mm256_store_ps(result + (j * 32 + 17) * M_ROW + i + 8, res171);
            _mm256_store_ps(result + (j * 32 + 18) * M_ROW + i + 8, res181);
            _mm256_store_ps(result + (j * 32 + 19) * M_ROW + i + 8, res191);
            _mm256_store_ps(result + (j * 32 + 20) * M_ROW + i + 8, res201);
            _mm256_store_ps(result + (j * 32 + 21) * M_ROW + i + 8, res211);
            _mm256_store_ps(result + (j * 32 + 22) * M_ROW + i + 8, res221);
            _mm256_store_ps(result + (j * 32 + 23) * M_ROW + i + 8, res231);
            _mm256_store_ps(result + (j * 32 + 24) * M_ROW + i + 8, res241);
            _mm256_store_ps(result + (j * 32 + 25) * M_ROW + i + 8, res251);
            _mm256_store_ps(result + (j * 32 + 26) * M_ROW + i + 8, res261);
            _mm256_store_ps(result + (j * 32 + 27) * M_ROW + i + 8, res271);
            _mm256_store_ps(result + (j * 32 + 28) * M_ROW + i + 8, res281);
            _mm256_store_ps(result + (j * 32 + 29) * M_ROW + i + 8, res291);
            _mm256_store_ps(result + (j * 32 + 30) * M_ROW + i + 8, res301);
            _mm256_store_ps(result + (j * 32 + 31) * M_ROW + i + 8, res311);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Uniform_32xG4_AVX2_OpenMP(const float* X, const int32_t NonZeroPerCol, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 4; j++) {
        for (int i = 0; i < M_ROW; i += 32) {
            __m256 res00 = _mm256_setzero_ps();
            __m256 res01 = _mm256_setzero_ps();
            __m256 res02 = _mm256_setzero_ps();
            __m256 res03 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res11 = _mm256_setzero_ps();
            __m256 res12 = _mm256_setzero_ps();
            __m256 res13 = _mm256_setzero_ps();
            __m256 res20 = _mm256_setzero_ps();
            __m256 res21 = _mm256_setzero_ps();
            __m256 res22 = _mm256_setzero_ps();
            __m256 res23 = _mm256_setzero_ps();
            __m256 res30 = _mm256_setzero_ps();
            __m256 res31 = _mm256_setzero_ps();
            __m256 res32 = _mm256_setzero_ps();
            __m256 res33 = _mm256_setzero_ps();
            for (int k = j * 4 * NonZeroPerCol; k < (j + 1) * 4 * NonZeroPerCol; k += 8) {
                __m256 pos00 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m256 neg00 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m256 pos10 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m256 neg10 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m256 pos20 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m256 neg20 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m256 pos30 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m256 neg30 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m256 pos01 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 8);
                __m256 neg01 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 8);
                __m256 pos11 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 8);
                __m256 neg11 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 8);
                __m256 pos21 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 8);
                __m256 neg21 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 8);
                __m256 pos31 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 8);
                __m256 neg31 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 8);
                __m256 pos02 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 16);
                __m256 neg02 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 16);
                __m256 pos12 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 16);
                __m256 neg12 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 16);
                __m256 pos22 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 16);
                __m256 neg22 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 16);
                __m256 pos32 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 16);
                __m256 neg32 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 16);
                __m256 pos03 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 24);
                __m256 neg03 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 24);
                __m256 pos13 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 24);
                __m256 neg13 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 24);
                __m256 pos23 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 24);
                __m256 neg23 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 24);
                __m256 pos33 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 24);
                __m256 neg33 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 24);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res20 = _mm256_add_ps(res20, _mm256_sub_ps(pos20, neg20));
                res30 = _mm256_add_ps(res30, _mm256_sub_ps(pos30, neg30));
                res01 = _mm256_add_ps(res01, _mm256_sub_ps(pos01, neg01));
                res11 = _mm256_add_ps(res11, _mm256_sub_ps(pos11, neg11));
                res21 = _mm256_add_ps(res21, _mm256_sub_ps(pos21, neg21));
                res31 = _mm256_add_ps(res31, _mm256_sub_ps(pos31, neg31));
                res02 = _mm256_add_ps(res02, _mm256_sub_ps(pos02, neg02));
                res12 = _mm256_add_ps(res12, _mm256_sub_ps(pos12, neg12));
                res22 = _mm256_add_ps(res22, _mm256_sub_ps(pos22, neg22));
                res32 = _mm256_add_ps(res32, _mm256_sub_ps(pos32, neg32));
                res03 = _mm256_add_ps(res03, _mm256_sub_ps(pos03, neg03));
                res13 = _mm256_add_ps(res13, _mm256_sub_ps(pos13, neg13));
                res23 = _mm256_add_ps(res23, _mm256_sub_ps(pos23, neg23));
                res33 = _mm256_add_ps(res33, _mm256_sub_ps(pos33, neg33));
            }
            _mm256_store_ps(result + (j * 4 + 0) * M_ROW + i + 0, res00);
            _mm256_store_ps(result + (j * 4 + 1) * M_ROW + i + 0, res10);
            _mm256_store_ps(result + (j * 4 + 2) * M_ROW + i + 0, res20);
            _mm256_store_ps(result + (j * 4 + 3) * M_ROW + i + 0, res30);
            _mm256_store_ps(result + (j * 4 + 0) * M_ROW + i + 8, res01);
            _mm256_store_ps(result + (j * 4 + 1) * M_ROW + i + 8, res11);
            _mm256_store_ps(result + (j * 4 + 2) * M_ROW + i + 8, res21);
            _mm256_store_ps(result + (j * 4 + 3) * M_ROW + i + 8, res31);
            _mm256_store_ps(result + (j * 4 + 0) * M_ROW + i + 16, res02);
            _mm256_store_ps(result + (j * 4 + 1) * M_ROW + i + 16, res12);
            _mm256_store_ps(result + (j * 4 + 2) * M_ROW + i + 16, res22);
            _mm256_store_ps(result + (j * 4 + 3) * M_ROW + i + 16, res32);
            _mm256_store_ps(result + (j * 4 + 0) * M_ROW + i + 24, res03);
            _mm256_store_ps(result + (j * 4 + 1) * M_ROW + i + 24, res13);
            _mm256_store_ps(result + (j * 4 + 2) * M_ROW + i + 24, res23);
            _mm256_store_ps(result + (j * 4 + 3) * M_ROW + i + 24, res33);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Uniform_32xG8_AVX2_OpenMP(const float* X, const int32_t NonZeroPerCol, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 8; j++) {
        for (int i = 0; i < M_ROW; i += 32) {
            __m256 res00 = _mm256_setzero_ps();
            __m256 res01 = _mm256_setzero_ps();
            __m256 res02 = _mm256_setzero_ps();
            __m256 res03 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res11 = _mm256_setzero_ps();
            __m256 res12 = _mm256_setzero_ps();
            __m256 res13 = _mm256_setzero_ps();
            __m256 res20 = _mm256_setzero_ps();
            __m256 res21 = _mm256_setzero_ps();
            __m256 res22 = _mm256_setzero_ps();
            __m256 res23 = _mm256_setzero_ps();
            __m256 res30 = _mm256_setzero_ps();
            __m256 res31 = _mm256_setzero_ps();
            __m256 res32 = _mm256_setzero_ps();
            __m256 res33 = _mm256_setzero_ps();
            __m256 res40 = _mm256_setzero_ps();
            __m256 res41 = _mm256_setzero_ps();
            __m256 res42 = _mm256_setzero_ps();
            __m256 res43 = _mm256_setzero_ps();
            __m256 res50 = _mm256_setzero_ps();
            __m256 res51 = _mm256_setzero_ps();
            __m256 res52 = _mm256_setzero_ps();
            __m256 res53 = _mm256_setzero_ps();
            __m256 res60 = _mm256_setzero_ps();
            __m256 res61 = _mm256_setzero_ps();
            __m256 res62 = _mm256_setzero_ps();
            __m256 res63 = _mm256_setzero_ps();
            __m256 res70 = _mm256_setzero_ps();
            __m256 res71 = _mm256_setzero_ps();
            __m256 res72 = _mm256_setzero_ps();
            __m256 res73 = _mm256_setzero_ps();
            for (int k = j * 8 * NonZeroPerCol; k < (j + 1) * 8 * NonZeroPerCol; k += 16) {
                __m256 pos00 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m256 neg00 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m256 pos10 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m256 neg10 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m256 pos20 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m256 neg20 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m256 pos30 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m256 neg30 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m256 pos40 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 0);
                __m256 neg40 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 0);
                __m256 pos50 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 0);
                __m256 neg50 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 0);
                __m256 pos60 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 0);
                __m256 neg60 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 0);
                __m256 pos70 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 0);
                __m256 neg70 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 0);
                __m256 pos01 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 8);
                __m256 neg01 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 8);
                __m256 pos11 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 8);
                __m256 neg11 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 8);
                __m256 pos21 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 8);
                __m256 neg21 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 8);
                __m256 pos31 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 8);
                __m256 neg31 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 8);
                __m256 pos41 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 8);
                __m256 neg41 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 8);
                __m256 pos51 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 8);
                __m256 neg51 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 8);
                __m256 pos61 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 8);
                __m256 neg61 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 8);
                __m256 pos71 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 8);
                __m256 neg71 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 8);
                __m256 pos02 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 16);
                __m256 neg02 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 16);
                __m256 pos12 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 16);
                __m256 neg12 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 16);
                __m256 pos22 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 16);
                __m256 neg22 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 16);
                __m256 pos32 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 16);
                __m256 neg32 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 16);
                __m256 pos42 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 16);
                __m256 neg42 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 16);
                __m256 pos52 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 16);
                __m256 neg52 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 16);
                __m256 pos62 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 16);
                __m256 neg62 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 16);
                __m256 pos72 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 16);
                __m256 neg72 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 16);
                __m256 pos03 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 24);
                __m256 neg03 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 24);
                __m256 pos13 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 24);
                __m256 neg13 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 24);
                __m256 pos23 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 24);
                __m256 neg23 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 24);
                __m256 pos33 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 24);
                __m256 neg33 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 24);
                __m256 pos43 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 24);
                __m256 neg43 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 24);
                __m256 pos53 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 24);
                __m256 neg53 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 24);
                __m256 pos63 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 24);
                __m256 neg63 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 24);
                __m256 pos73 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 24);
                __m256 neg73 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 24);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res20 = _mm256_add_ps(res20, _mm256_sub_ps(pos20, neg20));
                res30 = _mm256_add_ps(res30, _mm256_sub_ps(pos30, neg30));
                res40 = _mm256_add_ps(res40, _mm256_sub_ps(pos40, neg40));
                res50 = _mm256_add_ps(res50, _mm256_sub_ps(pos50, neg50));
                res60 = _mm256_add_ps(res60, _mm256_sub_ps(pos60, neg60));
                res70 = _mm256_add_ps(res70, _mm256_sub_ps(pos70, neg70));
                res01 = _mm256_add_ps(res01, _mm256_sub_ps(pos01, neg01));
                res11 = _mm256_add_ps(res11, _mm256_sub_ps(pos11, neg11));
                res21 = _mm256_add_ps(res21, _mm256_sub_ps(pos21, neg21));
                res31 = _mm256_add_ps(res31, _mm256_sub_ps(pos31, neg31));
                res41 = _mm256_add_ps(res41, _mm256_sub_ps(pos41, neg41));
                res51 = _mm256_add_ps(res51, _mm256_sub_ps(pos51, neg51));
                res61 = _mm256_add_ps(res61, _mm256_sub_ps(pos61, neg61));
                res71 = _mm256_add_ps(res71, _mm256_sub_ps(pos71, neg71));
                res02 = _mm256_add_ps(res02, _mm256_sub_ps(pos02, neg02));
                res12 = _mm256_add_ps(res12, _mm256_sub_ps(pos12, neg12));
                res22 = _mm256_add_ps(res22, _mm256_sub_ps(pos22, neg22));
                res32 = _mm256_add_ps(res32, _mm256_sub_ps(pos32, neg32));
                res42 = _mm256_add_ps(res42, _mm256_sub_ps(pos42, neg42));
                res52 = _mm256_add_ps(res52, _mm256_sub_ps(pos52, neg52));
                res62 = _mm256_add_ps(res62, _mm256_sub_ps(pos62, neg62));
                res72 = _mm256_add_ps(res72, _mm256_sub_ps(pos72, neg72));
                res03 = _mm256_add_ps(res03, _mm256_sub_ps(pos03, neg03));
                res13 = _mm256_add_ps(res13, _mm256_sub_ps(pos13, neg13));
                res23 = _mm256_add_ps(res23, _mm256_sub_ps(pos23, neg23));
                res33 = _mm256_add_ps(res33, _mm256_sub_ps(pos33, neg33));
                res43 = _mm256_add_ps(res43, _mm256_sub_ps(pos43, neg43));
                res53 = _mm256_add_ps(res53, _mm256_sub_ps(pos53, neg53));
                res63 = _mm256_add_ps(res63, _mm256_sub_ps(pos63, neg63));
                res73 = _mm256_add_ps(res73, _mm256_sub_ps(pos73, neg73));
            }
            _mm256_store_ps(result + (j * 8 + 0) * M_ROW + i + 0, res00);
            _mm256_store_ps(result + (j * 8 + 1) * M_ROW + i + 0, res10);
            _mm256_store_ps(result + (j * 8 + 2) * M_ROW + i + 0, res20);
            _mm256_store_ps(result + (j * 8 + 3) * M_ROW + i + 0, res30);
            _mm256_store_ps(result + (j * 8 + 4) * M_ROW + i + 0, res40);
            _mm256_store_ps(result + (j * 8 + 5) * M_ROW + i + 0, res50);
            _mm256_store_ps(result + (j * 8 + 6) * M_ROW + i + 0, res60);
            _mm256_store_ps(result + (j * 8 + 7) * M_ROW + i + 0, res70);
            _mm256_store_ps(result + (j * 8 + 0) * M_ROW + i + 8, res01);
            _mm256_store_ps(result + (j * 8 + 1) * M_ROW + i + 8, res11);
            _mm256_store_ps(result + (j * 8 + 2) * M_ROW + i + 8, res21);
            _mm256_store_ps(result + (j * 8 + 3) * M_ROW + i + 8, res31);
            _mm256_store_ps(result + (j * 8 + 4) * M_ROW + i + 8, res41);
            _mm256_store_ps(result + (j * 8 + 5) * M_ROW + i + 8, res51);
            _mm256_store_ps(result + (j * 8 + 6) * M_ROW + i + 8, res61);
            _mm256_store_ps(result + (j * 8 + 7) * M_ROW + i + 8, res71);
            _mm256_store_ps(result + (j * 8 + 0) * M_ROW + i + 16, res02);
            _mm256_store_ps(result + (j * 8 + 1) * M_ROW + i + 16, res12);
            _mm256_store_ps(result + (j * 8 + 2) * M_ROW + i + 16, res22);
            _mm256_store_ps(result + (j * 8 + 3) * M_ROW + i + 16, res32);
            _mm256_store_ps(result + (j * 8 + 4) * M_ROW + i + 16, res42);
            _mm256_store_ps(result + (j * 8 + 5) * M_ROW + i + 16, res52);
            _mm256_store_ps(result + (j * 8 + 6) * M_ROW + i + 16, res62);
            _mm256_store_ps(result + (j * 8 + 7) * M_ROW + i + 16, res72);
            _mm256_store_ps(result + (j * 8 + 0) * M_ROW + i + 24, res03);
            _mm256_store_ps(result + (j * 8 + 1) * M_ROW + i + 24, res13);
            _mm256_store_ps(result + (j * 8 + 2) * M_ROW + i + 24, res23);
            _mm256_store_ps(result + (j * 8 + 3) * M_ROW + i + 24, res33);
            _mm256_store_ps(result + (j * 8 + 4) * M_ROW + i + 24, res43);
            _mm256_store_ps(result + (j * 8 + 5) * M_ROW + i + 24, res53);
            _mm256_store_ps(result + (j * 8 + 6) * M_ROW + i + 24, res63);
            _mm256_store_ps(result + (j * 8 + 7) * M_ROW + i + 24, res73);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Uniform_32xG16_AVX2_OpenMP(const float* X, const int32_t NonZeroPerCol, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 16; j++) {
        for (int i = 0; i < M_ROW; i += 32) {
            __m256 res00 = _mm256_setzero_ps();
            __m256 res01 = _mm256_setzero_ps();
            __m256 res02 = _mm256_setzero_ps();
            __m256 res03 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res11 = _mm256_setzero_ps();
            __m256 res12 = _mm256_setzero_ps();
            __m256 res13 = _mm256_setzero_ps();
            __m256 res20 = _mm256_setzero_ps();
            __m256 res21 = _mm256_setzero_ps();
            __m256 res22 = _mm256_setzero_ps();
            __m256 res23 = _mm256_setzero_ps();
            __m256 res30 = _mm256_setzero_ps();
            __m256 res31 = _mm256_setzero_ps();
            __m256 res32 = _mm256_setzero_ps();
            __m256 res33 = _mm256_setzero_ps();
            __m256 res40 = _mm256_setzero_ps();
            __m256 res41 = _mm256_setzero_ps();
            __m256 res42 = _mm256_setzero_ps();
            __m256 res43 = _mm256_setzero_ps();
            __m256 res50 = _mm256_setzero_ps();
            __m256 res51 = _mm256_setzero_ps();
            __m256 res52 = _mm256_setzero_ps();
            __m256 res53 = _mm256_setzero_ps();
            __m256 res60 = _mm256_setzero_ps();
            __m256 res61 = _mm256_setzero_ps();
            __m256 res62 = _mm256_setzero_ps();
            __m256 res63 = _mm256_setzero_ps();
            __m256 res70 = _mm256_setzero_ps();
            __m256 res71 = _mm256_setzero_ps();
            __m256 res72 = _mm256_setzero_ps();
            __m256 res73 = _mm256_setzero_ps();
            __m256 res80 = _mm256_setzero_ps();
            __m256 res81 = _mm256_setzero_ps();
            __m256 res82 = _mm256_setzero_ps();
            __m256 res83 = _mm256_setzero_ps();
            __m256 res90 = _mm256_setzero_ps();
            __m256 res91 = _mm256_setzero_ps();
            __m256 res92 = _mm256_setzero_ps();
            __m256 res93 = _mm256_setzero_ps();
            __m256 res100 = _mm256_setzero_ps();
            __m256 res101 = _mm256_setzero_ps();
            __m256 res102 = _mm256_setzero_ps();
            __m256 res103 = _mm256_setzero_ps();
            __m256 res110 = _mm256_setzero_ps();
            __m256 res111 = _mm256_setzero_ps();
            __m256 res112 = _mm256_setzero_ps();
            __m256 res113 = _mm256_setzero_ps();
            __m256 res120 = _mm256_setzero_ps();
            __m256 res121 = _mm256_setzero_ps();
            __m256 res122 = _mm256_setzero_ps();
            __m256 res123 = _mm256_setzero_ps();
            __m256 res130 = _mm256_setzero_ps();
            __m256 res131 = _mm256_setzero_ps();
            __m256 res132 = _mm256_setzero_ps();
            __m256 res133 = _mm256_setzero_ps();
            __m256 res140 = _mm256_setzero_ps();
            __m256 res141 = _mm256_setzero_ps();
            __m256 res142 = _mm256_setzero_ps();
            __m256 res143 = _mm256_setzero_ps();
            __m256 res150 = _mm256_setzero_ps();
            __m256 res151 = _mm256_setzero_ps();
            __m256 res152 = _mm256_setzero_ps();
            __m256 res153 = _mm256_setzero_ps();
            for (int k = j * 16 * NonZeroPerCol; k < (j + 1) * 16 * NonZeroPerCol; k += 32) {
                __m256 pos00 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m256 neg00 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m256 pos10 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m256 neg10 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m256 pos20 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m256 neg20 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m256 pos30 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m256 neg30 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m256 pos40 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 0);
                __m256 neg40 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 0);
                __m256 pos50 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 0);
                __m256 neg50 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 0);
                __m256 pos60 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 0);
                __m256 neg60 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 0);
                __m256 pos70 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 0);
                __m256 neg70 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 0);
                __m256 pos80 = _mm256_load_ps(X + row_index[k + 16] * M_ROW + i + 0);
                __m256 neg80 = _mm256_load_ps(X + row_index[k + 17] * M_ROW + i + 0);
                __m256 pos90 = _mm256_load_ps(X + row_index[k + 18] * M_ROW + i + 0);
                __m256 neg90 = _mm256_load_ps(X + row_index[k + 19] * M_ROW + i + 0);
                __m256 pos100 = _mm256_load_ps(X + row_index[k + 20] * M_ROW + i + 0);
                __m256 neg100 = _mm256_load_ps(X + row_index[k + 21] * M_ROW + i + 0);
                __m256 pos110 = _mm256_load_ps(X + row_index[k + 22] * M_ROW + i + 0);
                __m256 neg110 = _mm256_load_ps(X + row_index[k + 23] * M_ROW + i + 0);
                __m256 pos120 = _mm256_load_ps(X + row_index[k + 24] * M_ROW + i + 0);
                __m256 neg120 = _mm256_load_ps(X + row_index[k + 25] * M_ROW + i + 0);
                __m256 pos130 = _mm256_load_ps(X + row_index[k + 26] * M_ROW + i + 0);
                __m256 neg130 = _mm256_load_ps(X + row_index[k + 27] * M_ROW + i + 0);
                __m256 pos140 = _mm256_load_ps(X + row_index[k + 28] * M_ROW + i + 0);
                __m256 neg140 = _mm256_load_ps(X + row_index[k + 29] * M_ROW + i + 0);
                __m256 pos150 = _mm256_load_ps(X + row_index[k + 30] * M_ROW + i + 0);
                __m256 neg150 = _mm256_load_ps(X + row_index[k + 31] * M_ROW + i + 0);
                __m256 pos01 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 8);
                __m256 neg01 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 8);
                __m256 pos11 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 8);
                __m256 neg11 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 8);
                __m256 pos21 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 8);
                __m256 neg21 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 8);
                __m256 pos31 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 8);
                __m256 neg31 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 8);
                __m256 pos41 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 8);
                __m256 neg41 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 8);
                __m256 pos51 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 8);
                __m256 neg51 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 8);
                __m256 pos61 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 8);
                __m256 neg61 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 8);
                __m256 pos71 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 8);
                __m256 neg71 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 8);
                __m256 pos81 = _mm256_load_ps(X + row_index[k + 16] * M_ROW + i + 8);
                __m256 neg81 = _mm256_load_ps(X + row_index[k + 17] * M_ROW + i + 8);
                __m256 pos91 = _mm256_load_ps(X + row_index[k + 18] * M_ROW + i + 8);
                __m256 neg91 = _mm256_load_ps(X + row_index[k + 19] * M_ROW + i + 8);
                __m256 pos101 = _mm256_load_ps(X + row_index[k + 20] * M_ROW + i + 8);
                __m256 neg101 = _mm256_load_ps(X + row_index[k + 21] * M_ROW + i + 8);
                __m256 pos111 = _mm256_load_ps(X + row_index[k + 22] * M_ROW + i + 8);
                __m256 neg111 = _mm256_load_ps(X + row_index[k + 23] * M_ROW + i + 8);
                __m256 pos121 = _mm256_load_ps(X + row_index[k + 24] * M_ROW + i + 8);
                __m256 neg121 = _mm256_load_ps(X + row_index[k + 25] * M_ROW + i + 8);
                __m256 pos131 = _mm256_load_ps(X + row_index[k + 26] * M_ROW + i + 8);
                __m256 neg131 = _mm256_load_ps(X + row_index[k + 27] * M_ROW + i + 8);
                __m256 pos141 = _mm256_load_ps(X + row_index[k + 28] * M_ROW + i + 8);
                __m256 neg141 = _mm256_load_ps(X + row_index[k + 29] * M_ROW + i + 8);
                __m256 pos151 = _mm256_load_ps(X + row_index[k + 30] * M_ROW + i + 8);
                __m256 neg151 = _mm256_load_ps(X + row_index[k + 31] * M_ROW + i + 8);
                __m256 pos02 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 16);
                __m256 neg02 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 16);
                __m256 pos12 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 16);
                __m256 neg12 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 16);
                __m256 pos22 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 16);
                __m256 neg22 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 16);
                __m256 pos32 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 16);
                __m256 neg32 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 16);
                __m256 pos42 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 16);
                __m256 neg42 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 16);
                __m256 pos52 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 16);
                __m256 neg52 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 16);
                __m256 pos62 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 16);
                __m256 neg62 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 16);
                __m256 pos72 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 16);
                __m256 neg72 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 16);
                __m256 pos82 = _mm256_load_ps(X + row_index[k + 16] * M_ROW + i + 16);
                __m256 neg82 = _mm256_load_ps(X + row_index[k + 17] * M_ROW + i + 16);
                __m256 pos92 = _mm256_load_ps(X + row_index[k + 18] * M_ROW + i + 16);
                __m256 neg92 = _mm256_load_ps(X + row_index[k + 19] * M_ROW + i + 16);
                __m256 pos102 = _mm256_load_ps(X + row_index[k + 20] * M_ROW + i + 16);
                __m256 neg102 = _mm256_load_ps(X + row_index[k + 21] * M_ROW + i + 16);
                __m256 pos112 = _mm256_load_ps(X + row_index[k + 22] * M_ROW + i + 16);
                __m256 neg112 = _mm256_load_ps(X + row_index[k + 23] * M_ROW + i + 16);
                __m256 pos122 = _mm256_load_ps(X + row_index[k + 24] * M_ROW + i + 16);
                __m256 neg122 = _mm256_load_ps(X + row_index[k + 25] * M_ROW + i + 16);
                __m256 pos132 = _mm256_load_ps(X + row_index[k + 26] * M_ROW + i + 16);
                __m256 neg132 = _mm256_load_ps(X + row_index[k + 27] * M_ROW + i + 16);
                __m256 pos142 = _mm256_load_ps(X + row_index[k + 28] * M_ROW + i + 16);
                __m256 neg142 = _mm256_load_ps(X + row_index[k + 29] * M_ROW + i + 16);
                __m256 pos152 = _mm256_load_ps(X + row_index[k + 30] * M_ROW + i + 16);
                __m256 neg152 = _mm256_load_ps(X + row_index[k + 31] * M_ROW + i + 16);
                __m256 pos03 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 24);
                __m256 neg03 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 24);
                __m256 pos13 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 24);
                __m256 neg13 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 24);
                __m256 pos23 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 24);
                __m256 neg23 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 24);
                __m256 pos33 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 24);
                __m256 neg33 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 24);
                __m256 pos43 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 24);
                __m256 neg43 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 24);
                __m256 pos53 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 24);
                __m256 neg53 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 24);
                __m256 pos63 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 24);
                __m256 neg63 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 24);
                __m256 pos73 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 24);
                __m256 neg73 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 24);
                __m256 pos83 = _mm256_load_ps(X + row_index[k + 16] * M_ROW + i + 24);
                __m256 neg83 = _mm256_load_ps(X + row_index[k + 17] * M_ROW + i + 24);
                __m256 pos93 = _mm256_load_ps(X + row_index[k + 18] * M_ROW + i + 24);
                __m256 neg93 = _mm256_load_ps(X + row_index[k + 19] * M_ROW + i + 24);
                __m256 pos103 = _mm256_load_ps(X + row_index[k + 20] * M_ROW + i + 24);
                __m256 neg103 = _mm256_load_ps(X + row_index[k + 21] * M_ROW + i + 24);
                __m256 pos113 = _mm256_load_ps(X + row_index[k + 22] * M_ROW + i + 24);
                __m256 neg113 = _mm256_load_ps(X + row_index[k + 23] * M_ROW + i + 24);
                __m256 pos123 = _mm256_load_ps(X + row_index[k + 24] * M_ROW + i + 24);
                __m256 neg123 = _mm256_load_ps(X + row_index[k + 25] * M_ROW + i + 24);
                __m256 pos133 = _mm256_load_ps(X + row_index[k + 26] * M_ROW + i + 24);
                __m256 neg133 = _mm256_load_ps(X + row_index[k + 27] * M_ROW + i + 24);
                __m256 pos143 = _mm256_load_ps(X + row_index[k + 28] * M_ROW + i + 24);
                __m256 neg143 = _mm256_load_ps(X + row_index[k + 29] * M_ROW + i + 24);
                __m256 pos153 = _mm256_load_ps(X + row_index[k + 30] * M_ROW + i + 24);
                __m256 neg153 = _mm256_load_ps(X + row_index[k + 31] * M_ROW + i + 24);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res20 = _mm256_add_ps(res20, _mm256_sub_ps(pos20, neg20));
                res30 = _mm256_add_ps(res30, _mm256_sub_ps(pos30, neg30));
                res40 = _mm256_add_ps(res40, _mm256_sub_ps(pos40, neg40));
                res50 = _mm256_add_ps(res50, _mm256_sub_ps(pos50, neg50));
                res60 = _mm256_add_ps(res60, _mm256_sub_ps(pos60, neg60));
                res70 = _mm256_add_ps(res70, _mm256_sub_ps(pos70, neg70));
                res80 = _mm256_add_ps(res80, _mm256_sub_ps(pos80, neg80));
                res90 = _mm256_add_ps(res90, _mm256_sub_ps(pos90, neg90));
                res100 = _mm256_add_ps(res100, _mm256_sub_ps(pos100, neg100));
                res110 = _mm256_add_ps(res110, _mm256_sub_ps(pos110, neg110));
                res120 = _mm256_add_ps(res120, _mm256_sub_ps(pos120, neg120));
                res130 = _mm256_add_ps(res130, _mm256_sub_ps(pos130, neg130));
                res140 = _mm256_add_ps(res140, _mm256_sub_ps(pos140, neg140));
                res150 = _mm256_add_ps(res150, _mm256_sub_ps(pos150, neg150));
                res01 = _mm256_add_ps(res01, _mm256_sub_ps(pos01, neg01));
                res11 = _mm256_add_ps(res11, _mm256_sub_ps(pos11, neg11));
                res21 = _mm256_add_ps(res21, _mm256_sub_ps(pos21, neg21));
                res31 = _mm256_add_ps(res31, _mm256_sub_ps(pos31, neg31));
                res41 = _mm256_add_ps(res41, _mm256_sub_ps(pos41, neg41));
                res51 = _mm256_add_ps(res51, _mm256_sub_ps(pos51, neg51));
                res61 = _mm256_add_ps(res61, _mm256_sub_ps(pos61, neg61));
                res71 = _mm256_add_ps(res71, _mm256_sub_ps(pos71, neg71));
                res81 = _mm256_add_ps(res81, _mm256_sub_ps(pos81, neg81));
                res91 = _mm256_add_ps(res91, _mm256_sub_ps(pos91, neg91));
                res101 = _mm256_add_ps(res101, _mm256_sub_ps(pos101, neg101));
                res111 = _mm256_add_ps(res111, _mm256_sub_ps(pos111, neg111));
                res121 = _mm256_add_ps(res121, _mm256_sub_ps(pos121, neg121));
                res131 = _mm256_add_ps(res131, _mm256_sub_ps(pos131, neg131));
                res141 = _mm256_add_ps(res141, _mm256_sub_ps(pos141, neg141));
                res151 = _mm256_add_ps(res151, _mm256_sub_ps(pos151, neg151));
                res02 = _mm256_add_ps(res02, _mm256_sub_ps(pos02, neg02));
                res12 = _mm256_add_ps(res12, _mm256_sub_ps(pos12, neg12));
                res22 = _mm256_add_ps(res22, _mm256_sub_ps(pos22, neg22));
                res32 = _mm256_add_ps(res32, _mm256_sub_ps(pos32, neg32));
                res42 = _mm256_add_ps(res42, _mm256_sub_ps(pos42, neg42));
                res52 = _mm256_add_ps(res52, _mm256_sub_ps(pos52, neg52));
                res62 = _mm256_add_ps(res62, _mm256_sub_ps(pos62, neg62));
                res72 = _mm256_add_ps(res72, _mm256_sub_ps(pos72, neg72));
                res82 = _mm256_add_ps(res82, _mm256_sub_ps(pos82, neg82));
                res92 = _mm256_add_ps(res92, _mm256_sub_ps(pos92, neg92));
                res102 = _mm256_add_ps(res102, _mm256_sub_ps(pos102, neg102));
                res112 = _mm256_add_ps(res112, _mm256_sub_ps(pos112, neg112));
                res122 = _mm256_add_ps(res122, _mm256_sub_ps(pos122, neg122));
                res132 = _mm256_add_ps(res132, _mm256_sub_ps(pos132, neg132));
                res142 = _mm256_add_ps(res142, _mm256_sub_ps(pos142, neg142));
                res152 = _mm256_add_ps(res152, _mm256_sub_ps(pos152, neg152));
                res03 = _mm256_add_ps(res03, _mm256_sub_ps(pos03, neg03));
                res13 = _mm256_add_ps(res13, _mm256_sub_ps(pos13, neg13));
                res23 = _mm256_add_ps(res23, _mm256_sub_ps(pos23, neg23));
                res33 = _mm256_add_ps(res33, _mm256_sub_ps(pos33, neg33));
                res43 = _mm256_add_ps(res43, _mm256_sub_ps(pos43, neg43));
                res53 = _mm256_add_ps(res53, _mm256_sub_ps(pos53, neg53));
                res63 = _mm256_add_ps(res63, _mm256_sub_ps(pos63, neg63));
                res73 = _mm256_add_ps(res73, _mm256_sub_ps(pos73, neg73));
                res83 = _mm256_add_ps(res83, _mm256_sub_ps(pos83, neg83));
                res93 = _mm256_add_ps(res93, _mm256_sub_ps(pos93, neg93));
                res103 = _mm256_add_ps(res103, _mm256_sub_ps(pos103, neg103));
                res113 = _mm256_add_ps(res113, _mm256_sub_ps(pos113, neg113));
                res123 = _mm256_add_ps(res123, _mm256_sub_ps(pos123, neg123));
                res133 = _mm256_add_ps(res133, _mm256_sub_ps(pos133, neg133));
                res143 = _mm256_add_ps(res143, _mm256_sub_ps(pos143, neg143));
                res153 = _mm256_add_ps(res153, _mm256_sub_ps(pos153, neg153));
            }
            _mm256_store_ps(result + (j * 16 + 0) * M_ROW + i + 0, res00);
            _mm256_store_ps(result + (j * 16 + 1) * M_ROW + i + 0, res10);
            _mm256_store_ps(result + (j * 16 + 2) * M_ROW + i + 0, res20);
            _mm256_store_ps(result + (j * 16 + 3) * M_ROW + i + 0, res30);
            _mm256_store_ps(result + (j * 16 + 4) * M_ROW + i + 0, res40);
            _mm256_store_ps(result + (j * 16 + 5) * M_ROW + i + 0, res50);
            _mm256_store_ps(result + (j * 16 + 6) * M_ROW + i + 0, res60);
            _mm256_store_ps(result + (j * 16 + 7) * M_ROW + i + 0, res70);
            _mm256_store_ps(result + (j * 16 + 8) * M_ROW + i + 0, res80);
            _mm256_store_ps(result + (j * 16 + 9) * M_ROW + i + 0, res90);
            _mm256_store_ps(result + (j * 16 + 10) * M_ROW + i + 0, res100);
            _mm256_store_ps(result + (j * 16 + 11) * M_ROW + i + 0, res110);
            _mm256_store_ps(result + (j * 16 + 12) * M_ROW + i + 0, res120);
            _mm256_store_ps(result + (j * 16 + 13) * M_ROW + i + 0, res130);
            _mm256_store_ps(result + (j * 16 + 14) * M_ROW + i + 0, res140);
            _mm256_store_ps(result + (j * 16 + 15) * M_ROW + i + 0, res150);
            _mm256_store_ps(result + (j * 16 + 0) * M_ROW + i + 8, res01);
            _mm256_store_ps(result + (j * 16 + 1) * M_ROW + i + 8, res11);
            _mm256_store_ps(result + (j * 16 + 2) * M_ROW + i + 8, res21);
            _mm256_store_ps(result + (j * 16 + 3) * M_ROW + i + 8, res31);
            _mm256_store_ps(result + (j * 16 + 4) * M_ROW + i + 8, res41);
            _mm256_store_ps(result + (j * 16 + 5) * M_ROW + i + 8, res51);
            _mm256_store_ps(result + (j * 16 + 6) * M_ROW + i + 8, res61);
            _mm256_store_ps(result + (j * 16 + 7) * M_ROW + i + 8, res71);
            _mm256_store_ps(result + (j * 16 + 8) * M_ROW + i + 8, res81);
            _mm256_store_ps(result + (j * 16 + 9) * M_ROW + i + 8, res91);
            _mm256_store_ps(result + (j * 16 + 10) * M_ROW + i + 8, res101);
            _mm256_store_ps(result + (j * 16 + 11) * M_ROW + i + 8, res111);
            _mm256_store_ps(result + (j * 16 + 12) * M_ROW + i + 8, res121);
            _mm256_store_ps(result + (j * 16 + 13) * M_ROW + i + 8, res131);
            _mm256_store_ps(result + (j * 16 + 14) * M_ROW + i + 8, res141);
            _mm256_store_ps(result + (j * 16 + 15) * M_ROW + i + 8, res151);
            _mm256_store_ps(result + (j * 16 + 0) * M_ROW + i + 16, res02);
            _mm256_store_ps(result + (j * 16 + 1) * M_ROW + i + 16, res12);
            _mm256_store_ps(result + (j * 16 + 2) * M_ROW + i + 16, res22);
            _mm256_store_ps(result + (j * 16 + 3) * M_ROW + i + 16, res32);
            _mm256_store_ps(result + (j * 16 + 4) * M_ROW + i + 16, res42);
            _mm256_store_ps(result + (j * 16 + 5) * M_ROW + i + 16, res52);
            _mm256_store_ps(result + (j * 16 + 6) * M_ROW + i + 16, res62);
            _mm256_store_ps(result + (j * 16 + 7) * M_ROW + i + 16, res72);
            _mm256_store_ps(result + (j * 16 + 8) * M_ROW + i + 16, res82);
            _mm256_store_ps(result + (j * 16 + 9) * M_ROW + i + 16, res92);
            _mm256_store_ps(result + (j * 16 + 10) * M_ROW + i + 16, res102);
            _mm256_store_ps(result + (j * 16 + 11) * M_ROW + i + 16, res112);
            _mm256_store_ps(result + (j * 16 + 12) * M_ROW + i + 16, res122);
            _mm256_store_ps(result + (j * 16 + 13) * M_ROW + i + 16, res132);
            _mm256_store_ps(result + (j * 16 + 14) * M_ROW + i + 16, res142);
            _mm256_store_ps(result + (j * 16 + 15) * M_ROW + i + 16, res152);
            _mm256_store_ps(result + (j * 16 + 0) * M_ROW + i + 24, res03);
            _mm256_store_ps(result + (j * 16 + 1) * M_ROW + i + 24, res13);
            _mm256_store_ps(result + (j * 16 + 2) * M_ROW + i + 24, res23);
            _mm256_store_ps(result + (j * 16 + 3) * M_ROW + i + 24, res33);
            _mm256_store_ps(result + (j * 16 + 4) * M_ROW + i + 24, res43);
            _mm256_store_ps(result + (j * 16 + 5) * M_ROW + i + 24, res53);
            _mm256_store_ps(result + (j * 16 + 6) * M_ROW + i + 24, res63);
            _mm256_store_ps(result + (j * 16 + 7) * M_ROW + i + 24, res73);
            _mm256_store_ps(result + (j * 16 + 8) * M_ROW + i + 24, res83);
            _mm256_store_ps(result + (j * 16 + 9) * M_ROW + i + 24, res93);
            _mm256_store_ps(result + (j * 16 + 10) * M_ROW + i + 24, res103);
            _mm256_store_ps(result + (j * 16 + 11) * M_ROW + i + 24, res113);
            _mm256_store_ps(result + (j * 16 + 12) * M_ROW + i + 24, res123);
            _mm256_store_ps(result + (j * 16 + 13) * M_ROW + i + 24, res133);
            _mm256_store_ps(result + (j * 16 + 14) * M_ROW + i + 24, res143);
            _mm256_store_ps(result + (j * 16 + 15) * M_ROW + i + 24, res153);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Uniform_32xG32_AVX2_OpenMP(const float* X, const int32_t NonZeroPerCol, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 32; j++) {
        for (int i = 0; i < M_ROW; i += 32) {
            __m256 res00 = _mm256_setzero_ps();
            __m256 res01 = _mm256_setzero_ps();
            __m256 res02 = _mm256_setzero_ps();
            __m256 res03 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res11 = _mm256_setzero_ps();
            __m256 res12 = _mm256_setzero_ps();
            __m256 res13 = _mm256_setzero_ps();
            __m256 res20 = _mm256_setzero_ps();
            __m256 res21 = _mm256_setzero_ps();
            __m256 res22 = _mm256_setzero_ps();
            __m256 res23 = _mm256_setzero_ps();
            __m256 res30 = _mm256_setzero_ps();
            __m256 res31 = _mm256_setzero_ps();
            __m256 res32 = _mm256_setzero_ps();
            __m256 res33 = _mm256_setzero_ps();
            __m256 res40 = _mm256_setzero_ps();
            __m256 res41 = _mm256_setzero_ps();
            __m256 res42 = _mm256_setzero_ps();
            __m256 res43 = _mm256_setzero_ps();
            __m256 res50 = _mm256_setzero_ps();
            __m256 res51 = _mm256_setzero_ps();
            __m256 res52 = _mm256_setzero_ps();
            __m256 res53 = _mm256_setzero_ps();
            __m256 res60 = _mm256_setzero_ps();
            __m256 res61 = _mm256_setzero_ps();
            __m256 res62 = _mm256_setzero_ps();
            __m256 res63 = _mm256_setzero_ps();
            __m256 res70 = _mm256_setzero_ps();
            __m256 res71 = _mm256_setzero_ps();
            __m256 res72 = _mm256_setzero_ps();
            __m256 res73 = _mm256_setzero_ps();
            __m256 res80 = _mm256_setzero_ps();
            __m256 res81 = _mm256_setzero_ps();
            __m256 res82 = _mm256_setzero_ps();
            __m256 res83 = _mm256_setzero_ps();
            __m256 res90 = _mm256_setzero_ps();
            __m256 res91 = _mm256_setzero_ps();
            __m256 res92 = _mm256_setzero_ps();
            __m256 res93 = _mm256_setzero_ps();
            __m256 res100 = _mm256_setzero_ps();
            __m256 res101 = _mm256_setzero_ps();
            __m256 res102 = _mm256_setzero_ps();
            __m256 res103 = _mm256_setzero_ps();
            __m256 res110 = _mm256_setzero_ps();
            __m256 res111 = _mm256_setzero_ps();
            __m256 res112 = _mm256_setzero_ps();
            __m256 res113 = _mm256_setzero_ps();
            __m256 res120 = _mm256_setzero_ps();
            __m256 res121 = _mm256_setzero_ps();
            __m256 res122 = _mm256_setzero_ps();
            __m256 res123 = _mm256_setzero_ps();
            __m256 res130 = _mm256_setzero_ps();
            __m256 res131 = _mm256_setzero_ps();
            __m256 res132 = _mm256_setzero_ps();
            __m256 res133 = _mm256_setzero_ps();
            __m256 res140 = _mm256_setzero_ps();
            __m256 res141 = _mm256_setzero_ps();
            __m256 res142 = _mm256_setzero_ps();
            __m256 res143 = _mm256_setzero_ps();
            __m256 res150 = _mm256_setzero_ps();
            __m256 res151 = _mm256_setzero_ps();
            __m256 res152 = _mm256_setzero_ps();
            __m256 res153 = _mm256_setzero_ps();
            __m256 res160 = _mm256_setzero_ps();
            __m256 res161 = _mm256_setzero_ps();
            __m256 res162 = _mm256_setzero_ps();
            __m256 res163 = _mm256_setzero_ps();
            __m256 res170 = _mm256_setzero_ps();
            __m256 res171 = _mm256_setzero_ps();
            __m256 res172 = _mm256_setzero_ps();
            __m256 res173 = _mm256_setzero_ps();
            __m256 res180 = _mm256_setzero_ps();
            __m256 res181 = _mm256_setzero_ps();
            __m256 res182 = _mm256_setzero_ps();
            __m256 res183 = _mm256_setzero_ps();
            __m256 res190 = _mm256_setzero_ps();
            __m256 res191 = _mm256_setzero_ps();
            __m256 res192 = _mm256_setzero_ps();
            __m256 res193 = _mm256_setzero_ps();
            __m256 res200 = _mm256_setzero_ps();
            __m256 res201 = _mm256_setzero_ps();
            __m256 res202 = _mm256_setzero_ps();
            __m256 res203 = _mm256_setzero_ps();
            __m256 res210 = _mm256_setzero_ps();
            __m256 res211 = _mm256_setzero_ps();
            __m256 res212 = _mm256_setzero_ps();
            __m256 res213 = _mm256_setzero_ps();
            __m256 res220 = _mm256_setzero_ps();
            __m256 res221 = _mm256_setzero_ps();
            __m256 res222 = _mm256_setzero_ps();
            __m256 res223 = _mm256_setzero_ps();
            __m256 res230 = _mm256_setzero_ps();
            __m256 res231 = _mm256_setzero_ps();
            __m256 res232 = _mm256_setzero_ps();
            __m256 res233 = _mm256_setzero_ps();
            __m256 res240 = _mm256_setzero_ps();
            __m256 res241 = _mm256_setzero_ps();
            __m256 res242 = _mm256_setzero_ps();
            __m256 res243 = _mm256_setzero_ps();
            __m256 res250 = _mm256_setzero_ps();
            __m256 res251 = _mm256_setzero_ps();
            __m256 res252 = _mm256_setzero_ps();
            __m256 res253 = _mm256_setzero_ps();
            __m256 res260 = _mm256_setzero_ps();
            __m256 res261 = _mm256_setzero_ps();
            __m256 res262 = _mm256_setzero_ps();
            __m256 res263 = _mm256_setzero_ps();
            __m256 res270 = _mm256_setzero_ps();
            __m256 res271 = _mm256_setzero_ps();
            __m256 res272 = _mm256_setzero_ps();
            __m256 res273 = _mm256_setzero_ps();
            __m256 res280 = _mm256_setzero_ps();
            __m256 res281 = _mm256_setzero_ps();
            __m256 res282 = _mm256_setzero_ps();
            __m256 res283 = _mm256_setzero_ps();
            __m256 res290 = _mm256_setzero_ps();
            __m256 res291 = _mm256_setzero_ps();
            __m256 res292 = _mm256_setzero_ps();
            __m256 res293 = _mm256_setzero_ps();
            __m256 res300 = _mm256_setzero_ps();
            __m256 res301 = _mm256_setzero_ps();
            __m256 res302 = _mm256_setzero_ps();
            __m256 res303 = _mm256_setzero_ps();
            __m256 res310 = _mm256_setzero_ps();
            __m256 res311 = _mm256_setzero_ps();
            __m256 res312 = _mm256_setzero_ps();
            __m256 res313 = _mm256_setzero_ps();
            for (int k = j * 32 * NonZeroPerCol; k < (j + 1) * 32 * NonZeroPerCol; k += 64) {
                __m256 pos00 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m256 neg00 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m256 pos10 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m256 neg10 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m256 pos20 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m256 neg20 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m256 pos30 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m256 neg30 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m256 pos40 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 0);
                __m256 neg40 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 0);
                __m256 pos50 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 0);
                __m256 neg50 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 0);
                __m256 pos60 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 0);
                __m256 neg60 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 0);
                __m256 pos70 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 0);
                __m256 neg70 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 0);
                __m256 pos80 = _mm256_load_ps(X + row_index[k + 16] * M_ROW + i + 0);
                __m256 neg80 = _mm256_load_ps(X + row_index[k + 17] * M_ROW + i + 0);
                __m256 pos90 = _mm256_load_ps(X + row_index[k + 18] * M_ROW + i + 0);
                __m256 neg90 = _mm256_load_ps(X + row_index[k + 19] * M_ROW + i + 0);
                __m256 pos100 = _mm256_load_ps(X + row_index[k + 20] * M_ROW + i + 0);
                __m256 neg100 = _mm256_load_ps(X + row_index[k + 21] * M_ROW + i + 0);
                __m256 pos110 = _mm256_load_ps(X + row_index[k + 22] * M_ROW + i + 0);
                __m256 neg110 = _mm256_load_ps(X + row_index[k + 23] * M_ROW + i + 0);
                __m256 pos120 = _mm256_load_ps(X + row_index[k + 24] * M_ROW + i + 0);
                __m256 neg120 = _mm256_load_ps(X + row_index[k + 25] * M_ROW + i + 0);
                __m256 pos130 = _mm256_load_ps(X + row_index[k + 26] * M_ROW + i + 0);
                __m256 neg130 = _mm256_load_ps(X + row_index[k + 27] * M_ROW + i + 0);
                __m256 pos140 = _mm256_load_ps(X + row_index[k + 28] * M_ROW + i + 0);
                __m256 neg140 = _mm256_load_ps(X + row_index[k + 29] * M_ROW + i + 0);
                __m256 pos150 = _mm256_load_ps(X + row_index[k + 30] * M_ROW + i + 0);
                __m256 neg150 = _mm256_load_ps(X + row_index[k + 31] * M_ROW + i + 0);
                __m256 pos160 = _mm256_load_ps(X + row_index[k + 32] * M_ROW + i + 0);
                __m256 neg160 = _mm256_load_ps(X + row_index[k + 33] * M_ROW + i + 0);
                __m256 pos170 = _mm256_load_ps(X + row_index[k + 34] * M_ROW + i + 0);
                __m256 neg170 = _mm256_load_ps(X + row_index[k + 35] * M_ROW + i + 0);
                __m256 pos180 = _mm256_load_ps(X + row_index[k + 36] * M_ROW + i + 0);
                __m256 neg180 = _mm256_load_ps(X + row_index[k + 37] * M_ROW + i + 0);
                __m256 pos190 = _mm256_load_ps(X + row_index[k + 38] * M_ROW + i + 0);
                __m256 neg190 = _mm256_load_ps(X + row_index[k + 39] * M_ROW + i + 0);
                __m256 pos200 = _mm256_load_ps(X + row_index[k + 40] * M_ROW + i + 0);
                __m256 neg200 = _mm256_load_ps(X + row_index[k + 41] * M_ROW + i + 0);
                __m256 pos210 = _mm256_load_ps(X + row_index[k + 42] * M_ROW + i + 0);
                __m256 neg210 = _mm256_load_ps(X + row_index[k + 43] * M_ROW + i + 0);
                __m256 pos220 = _mm256_load_ps(X + row_index[k + 44] * M_ROW + i + 0);
                __m256 neg220 = _mm256_load_ps(X + row_index[k + 45] * M_ROW + i + 0);
                __m256 pos230 = _mm256_load_ps(X + row_index[k + 46] * M_ROW + i + 0);
                __m256 neg230 = _mm256_load_ps(X + row_index[k + 47] * M_ROW + i + 0);
                __m256 pos240 = _mm256_load_ps(X + row_index[k + 48] * M_ROW + i + 0);
                __m256 neg240 = _mm256_load_ps(X + row_index[k + 49] * M_ROW + i + 0);
                __m256 pos250 = _mm256_load_ps(X + row_index[k + 50] * M_ROW + i + 0);
                __m256 neg250 = _mm256_load_ps(X + row_index[k + 51] * M_ROW + i + 0);
                __m256 pos260 = _mm256_load_ps(X + row_index[k + 52] * M_ROW + i + 0);
                __m256 neg260 = _mm256_load_ps(X + row_index[k + 53] * M_ROW + i + 0);
                __m256 pos270 = _mm256_load_ps(X + row_index[k + 54] * M_ROW + i + 0);
                __m256 neg270 = _mm256_load_ps(X + row_index[k + 55] * M_ROW + i + 0);
                __m256 pos280 = _mm256_load_ps(X + row_index[k + 56] * M_ROW + i + 0);
                __m256 neg280 = _mm256_load_ps(X + row_index[k + 57] * M_ROW + i + 0);
                __m256 pos290 = _mm256_load_ps(X + row_index[k + 58] * M_ROW + i + 0);
                __m256 neg290 = _mm256_load_ps(X + row_index[k + 59] * M_ROW + i + 0);
                __m256 pos300 = _mm256_load_ps(X + row_index[k + 60] * M_ROW + i + 0);
                __m256 neg300 = _mm256_load_ps(X + row_index[k + 61] * M_ROW + i + 0);
                __m256 pos310 = _mm256_load_ps(X + row_index[k + 62] * M_ROW + i + 0);
                __m256 neg310 = _mm256_load_ps(X + row_index[k + 63] * M_ROW + i + 0);
                __m256 pos01 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 8);
                __m256 neg01 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 8);
                __m256 pos11 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 8);
                __m256 neg11 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 8);
                __m256 pos21 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 8);
                __m256 neg21 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 8);
                __m256 pos31 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 8);
                __m256 neg31 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 8);
                __m256 pos41 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 8);
                __m256 neg41 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 8);
                __m256 pos51 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 8);
                __m256 neg51 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 8);
                __m256 pos61 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 8);
                __m256 neg61 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 8);
                __m256 pos71 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 8);
                __m256 neg71 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 8);
                __m256 pos81 = _mm256_load_ps(X + row_index[k + 16] * M_ROW + i + 8);
                __m256 neg81 = _mm256_load_ps(X + row_index[k + 17] * M_ROW + i + 8);
                __m256 pos91 = _mm256_load_ps(X + row_index[k + 18] * M_ROW + i + 8);
                __m256 neg91 = _mm256_load_ps(X + row_index[k + 19] * M_ROW + i + 8);
                __m256 pos101 = _mm256_load_ps(X + row_index[k + 20] * M_ROW + i + 8);
                __m256 neg101 = _mm256_load_ps(X + row_index[k + 21] * M_ROW + i + 8);
                __m256 pos111 = _mm256_load_ps(X + row_index[k + 22] * M_ROW + i + 8);
                __m256 neg111 = _mm256_load_ps(X + row_index[k + 23] * M_ROW + i + 8);
                __m256 pos121 = _mm256_load_ps(X + row_index[k + 24] * M_ROW + i + 8);
                __m256 neg121 = _mm256_load_ps(X + row_index[k + 25] * M_ROW + i + 8);
                __m256 pos131 = _mm256_load_ps(X + row_index[k + 26] * M_ROW + i + 8);
                __m256 neg131 = _mm256_load_ps(X + row_index[k + 27] * M_ROW + i + 8);
                __m256 pos141 = _mm256_load_ps(X + row_index[k + 28] * M_ROW + i + 8);
                __m256 neg141 = _mm256_load_ps(X + row_index[k + 29] * M_ROW + i + 8);
                __m256 pos151 = _mm256_load_ps(X + row_index[k + 30] * M_ROW + i + 8);
                __m256 neg151 = _mm256_load_ps(X + row_index[k + 31] * M_ROW + i + 8);
                __m256 pos161 = _mm256_load_ps(X + row_index[k + 32] * M_ROW + i + 8);
                __m256 neg161 = _mm256_load_ps(X + row_index[k + 33] * M_ROW + i + 8);
                __m256 pos171 = _mm256_load_ps(X + row_index[k + 34] * M_ROW + i + 8);
                __m256 neg171 = _mm256_load_ps(X + row_index[k + 35] * M_ROW + i + 8);
                __m256 pos181 = _mm256_load_ps(X + row_index[k + 36] * M_ROW + i + 8);
                __m256 neg181 = _mm256_load_ps(X + row_index[k + 37] * M_ROW + i + 8);
                __m256 pos191 = _mm256_load_ps(X + row_index[k + 38] * M_ROW + i + 8);
                __m256 neg191 = _mm256_load_ps(X + row_index[k + 39] * M_ROW + i + 8);
                __m256 pos201 = _mm256_load_ps(X + row_index[k + 40] * M_ROW + i + 8);
                __m256 neg201 = _mm256_load_ps(X + row_index[k + 41] * M_ROW + i + 8);
                __m256 pos211 = _mm256_load_ps(X + row_index[k + 42] * M_ROW + i + 8);
                __m256 neg211 = _mm256_load_ps(X + row_index[k + 43] * M_ROW + i + 8);
                __m256 pos221 = _mm256_load_ps(X + row_index[k + 44] * M_ROW + i + 8);
                __m256 neg221 = _mm256_load_ps(X + row_index[k + 45] * M_ROW + i + 8);
                __m256 pos231 = _mm256_load_ps(X + row_index[k + 46] * M_ROW + i + 8);
                __m256 neg231 = _mm256_load_ps(X + row_index[k + 47] * M_ROW + i + 8);
                __m256 pos241 = _mm256_load_ps(X + row_index[k + 48] * M_ROW + i + 8);
                __m256 neg241 = _mm256_load_ps(X + row_index[k + 49] * M_ROW + i + 8);
                __m256 pos251 = _mm256_load_ps(X + row_index[k + 50] * M_ROW + i + 8);
                __m256 neg251 = _mm256_load_ps(X + row_index[k + 51] * M_ROW + i + 8);
                __m256 pos261 = _mm256_load_ps(X + row_index[k + 52] * M_ROW + i + 8);
                __m256 neg261 = _mm256_load_ps(X + row_index[k + 53] * M_ROW + i + 8);
                __m256 pos271 = _mm256_load_ps(X + row_index[k + 54] * M_ROW + i + 8);
                __m256 neg271 = _mm256_load_ps(X + row_index[k + 55] * M_ROW + i + 8);
                __m256 pos281 = _mm256_load_ps(X + row_index[k + 56] * M_ROW + i + 8);
                __m256 neg281 = _mm256_load_ps(X + row_index[k + 57] * M_ROW + i + 8);
                __m256 pos291 = _mm256_load_ps(X + row_index[k + 58] * M_ROW + i + 8);
                __m256 neg291 = _mm256_load_ps(X + row_index[k + 59] * M_ROW + i + 8);
                __m256 pos301 = _mm256_load_ps(X + row_index[k + 60] * M_ROW + i + 8);
                __m256 neg301 = _mm256_load_ps(X + row_index[k + 61] * M_ROW + i + 8);
                __m256 pos311 = _mm256_load_ps(X + row_index[k + 62] * M_ROW + i + 8);
                __m256 neg311 = _mm256_load_ps(X + row_index[k + 63] * M_ROW + i + 8);
                __m256 pos02 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 16);
                __m256 neg02 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 16);
                __m256 pos12 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 16);
                __m256 neg12 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 16);
                __m256 pos22 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 16);
                __m256 neg22 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 16);
                __m256 pos32 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 16);
                __m256 neg32 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 16);
                __m256 pos42 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 16);
                __m256 neg42 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 16);
                __m256 pos52 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 16);
                __m256 neg52 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 16);
                __m256 pos62 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 16);
                __m256 neg62 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 16);
                __m256 pos72 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 16);
                __m256 neg72 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 16);
                __m256 pos82 = _mm256_load_ps(X + row_index[k + 16] * M_ROW + i + 16);
                __m256 neg82 = _mm256_load_ps(X + row_index[k + 17] * M_ROW + i + 16);
                __m256 pos92 = _mm256_load_ps(X + row_index[k + 18] * M_ROW + i + 16);
                __m256 neg92 = _mm256_load_ps(X + row_index[k + 19] * M_ROW + i + 16);
                __m256 pos102 = _mm256_load_ps(X + row_index[k + 20] * M_ROW + i + 16);
                __m256 neg102 = _mm256_load_ps(X + row_index[k + 21] * M_ROW + i + 16);
                __m256 pos112 = _mm256_load_ps(X + row_index[k + 22] * M_ROW + i + 16);
                __m256 neg112 = _mm256_load_ps(X + row_index[k + 23] * M_ROW + i + 16);
                __m256 pos122 = _mm256_load_ps(X + row_index[k + 24] * M_ROW + i + 16);
                __m256 neg122 = _mm256_load_ps(X + row_index[k + 25] * M_ROW + i + 16);
                __m256 pos132 = _mm256_load_ps(X + row_index[k + 26] * M_ROW + i + 16);
                __m256 neg132 = _mm256_load_ps(X + row_index[k + 27] * M_ROW + i + 16);
                __m256 pos142 = _mm256_load_ps(X + row_index[k + 28] * M_ROW + i + 16);
                __m256 neg142 = _mm256_load_ps(X + row_index[k + 29] * M_ROW + i + 16);
                __m256 pos152 = _mm256_load_ps(X + row_index[k + 30] * M_ROW + i + 16);
                __m256 neg152 = _mm256_load_ps(X + row_index[k + 31] * M_ROW + i + 16);
                __m256 pos162 = _mm256_load_ps(X + row_index[k + 32] * M_ROW + i + 16);
                __m256 neg162 = _mm256_load_ps(X + row_index[k + 33] * M_ROW + i + 16);
                __m256 pos172 = _mm256_load_ps(X + row_index[k + 34] * M_ROW + i + 16);
                __m256 neg172 = _mm256_load_ps(X + row_index[k + 35] * M_ROW + i + 16);
                __m256 pos182 = _mm256_load_ps(X + row_index[k + 36] * M_ROW + i + 16);
                __m256 neg182 = _mm256_load_ps(X + row_index[k + 37] * M_ROW + i + 16);
                __m256 pos192 = _mm256_load_ps(X + row_index[k + 38] * M_ROW + i + 16);
                __m256 neg192 = _mm256_load_ps(X + row_index[k + 39] * M_ROW + i + 16);
                __m256 pos202 = _mm256_load_ps(X + row_index[k + 40] * M_ROW + i + 16);
                __m256 neg202 = _mm256_load_ps(X + row_index[k + 41] * M_ROW + i + 16);
                __m256 pos212 = _mm256_load_ps(X + row_index[k + 42] * M_ROW + i + 16);
                __m256 neg212 = _mm256_load_ps(X + row_index[k + 43] * M_ROW + i + 16);
                __m256 pos222 = _mm256_load_ps(X + row_index[k + 44] * M_ROW + i + 16);
                __m256 neg222 = _mm256_load_ps(X + row_index[k + 45] * M_ROW + i + 16);
                __m256 pos232 = _mm256_load_ps(X + row_index[k + 46] * M_ROW + i + 16);
                __m256 neg232 = _mm256_load_ps(X + row_index[k + 47] * M_ROW + i + 16);
                __m256 pos242 = _mm256_load_ps(X + row_index[k + 48] * M_ROW + i + 16);
                __m256 neg242 = _mm256_load_ps(X + row_index[k + 49] * M_ROW + i + 16);
                __m256 pos252 = _mm256_load_ps(X + row_index[k + 50] * M_ROW + i + 16);
                __m256 neg252 = _mm256_load_ps(X + row_index[k + 51] * M_ROW + i + 16);
                __m256 pos262 = _mm256_load_ps(X + row_index[k + 52] * M_ROW + i + 16);
                __m256 neg262 = _mm256_load_ps(X + row_index[k + 53] * M_ROW + i + 16);
                __m256 pos272 = _mm256_load_ps(X + row_index[k + 54] * M_ROW + i + 16);
                __m256 neg272 = _mm256_load_ps(X + row_index[k + 55] * M_ROW + i + 16);
                __m256 pos282 = _mm256_load_ps(X + row_index[k + 56] * M_ROW + i + 16);
                __m256 neg282 = _mm256_load_ps(X + row_index[k + 57] * M_ROW + i + 16);
                __m256 pos292 = _mm256_load_ps(X + row_index[k + 58] * M_ROW + i + 16);
                __m256 neg292 = _mm256_load_ps(X + row_index[k + 59] * M_ROW + i + 16);
                __m256 pos302 = _mm256_load_ps(X + row_index[k + 60] * M_ROW + i + 16);
                __m256 neg302 = _mm256_load_ps(X + row_index[k + 61] * M_ROW + i + 16);
                __m256 pos312 = _mm256_load_ps(X + row_index[k + 62] * M_ROW + i + 16);
                __m256 neg312 = _mm256_load_ps(X + row_index[k + 63] * M_ROW + i + 16);
                __m256 pos03 = _mm256_load_ps(X + row_index[k + 0] * M_ROW + i + 24);
                __m256 neg03 = _mm256_load_ps(X + row_index[k + 1] * M_ROW + i + 24);
                __m256 pos13 = _mm256_load_ps(X + row_index[k + 2] * M_ROW + i + 24);
                __m256 neg13 = _mm256_load_ps(X + row_index[k + 3] * M_ROW + i + 24);
                __m256 pos23 = _mm256_load_ps(X + row_index[k + 4] * M_ROW + i + 24);
                __m256 neg23 = _mm256_load_ps(X + row_index[k + 5] * M_ROW + i + 24);
                __m256 pos33 = _mm256_load_ps(X + row_index[k + 6] * M_ROW + i + 24);
                __m256 neg33 = _mm256_load_ps(X + row_index[k + 7] * M_ROW + i + 24);
                __m256 pos43 = _mm256_load_ps(X + row_index[k + 8] * M_ROW + i + 24);
                __m256 neg43 = _mm256_load_ps(X + row_index[k + 9] * M_ROW + i + 24);
                __m256 pos53 = _mm256_load_ps(X + row_index[k + 10] * M_ROW + i + 24);
                __m256 neg53 = _mm256_load_ps(X + row_index[k + 11] * M_ROW + i + 24);
                __m256 pos63 = _mm256_load_ps(X + row_index[k + 12] * M_ROW + i + 24);
                __m256 neg63 = _mm256_load_ps(X + row_index[k + 13] * M_ROW + i + 24);
                __m256 pos73 = _mm256_load_ps(X + row_index[k + 14] * M_ROW + i + 24);
                __m256 neg73 = _mm256_load_ps(X + row_index[k + 15] * M_ROW + i + 24);
                __m256 pos83 = _mm256_load_ps(X + row_index[k + 16] * M_ROW + i + 24);
                __m256 neg83 = _mm256_load_ps(X + row_index[k + 17] * M_ROW + i + 24);
                __m256 pos93 = _mm256_load_ps(X + row_index[k + 18] * M_ROW + i + 24);
                __m256 neg93 = _mm256_load_ps(X + row_index[k + 19] * M_ROW + i + 24);
                __m256 pos103 = _mm256_load_ps(X + row_index[k + 20] * M_ROW + i + 24);
                __m256 neg103 = _mm256_load_ps(X + row_index[k + 21] * M_ROW + i + 24);
                __m256 pos113 = _mm256_load_ps(X + row_index[k + 22] * M_ROW + i + 24);
                __m256 neg113 = _mm256_load_ps(X + row_index[k + 23] * M_ROW + i + 24);
                __m256 pos123 = _mm256_load_ps(X + row_index[k + 24] * M_ROW + i + 24);
                __m256 neg123 = _mm256_load_ps(X + row_index[k + 25] * M_ROW + i + 24);
                __m256 pos133 = _mm256_load_ps(X + row_index[k + 26] * M_ROW + i + 24);
                __m256 neg133 = _mm256_load_ps(X + row_index[k + 27] * M_ROW + i + 24);
                __m256 pos143 = _mm256_load_ps(X + row_index[k + 28] * M_ROW + i + 24);
                __m256 neg143 = _mm256_load_ps(X + row_index[k + 29] * M_ROW + i + 24);
                __m256 pos153 = _mm256_load_ps(X + row_index[k + 30] * M_ROW + i + 24);
                __m256 neg153 = _mm256_load_ps(X + row_index[k + 31] * M_ROW + i + 24);
                __m256 pos163 = _mm256_load_ps(X + row_index[k + 32] * M_ROW + i + 24);
                __m256 neg163 = _mm256_load_ps(X + row_index[k + 33] * M_ROW + i + 24);
                __m256 pos173 = _mm256_load_ps(X + row_index[k + 34] * M_ROW + i + 24);
                __m256 neg173 = _mm256_load_ps(X + row_index[k + 35] * M_ROW + i + 24);
                __m256 pos183 = _mm256_load_ps(X + row_index[k + 36] * M_ROW + i + 24);
                __m256 neg183 = _mm256_load_ps(X + row_index[k + 37] * M_ROW + i + 24);
                __m256 pos193 = _mm256_load_ps(X + row_index[k + 38] * M_ROW + i + 24);
                __m256 neg193 = _mm256_load_ps(X + row_index[k + 39] * M_ROW + i + 24);
                __m256 pos203 = _mm256_load_ps(X + row_index[k + 40] * M_ROW + i + 24);
                __m256 neg203 = _mm256_load_ps(X + row_index[k + 41] * M_ROW + i + 24);
                __m256 pos213 = _mm256_load_ps(X + row_index[k + 42] * M_ROW + i + 24);
                __m256 neg213 = _mm256_load_ps(X + row_index[k + 43] * M_ROW + i + 24);
                __m256 pos223 = _mm256_load_ps(X + row_index[k + 44] * M_ROW + i + 24);
                __m256 neg223 = _mm256_load_ps(X + row_index[k + 45] * M_ROW + i + 24);
                __m256 pos233 = _mm256_load_ps(X + row_index[k + 46] * M_ROW + i + 24);
                __m256 neg233 = _mm256_load_ps(X + row_index[k + 47] * M_ROW + i + 24);
                __m256 pos243 = _mm256_load_ps(X + row_index[k + 48] * M_ROW + i + 24);
                __m256 neg243 = _mm256_load_ps(X + row_index[k + 49] * M_ROW + i + 24);
                __m256 pos253 = _mm256_load_ps(X + row_index[k + 50] * M_ROW + i + 24);
                __m256 neg253 = _mm256_load_ps(X + row_index[k + 51] * M_ROW + i + 24);
                __m256 pos263 = _mm256_load_ps(X + row_index[k + 52] * M_ROW + i + 24);
                __m256 neg263 = _mm256_load_ps(X + row_index[k + 53] * M_ROW + i + 24);
                __m256 pos273 = _mm256_load_ps(X + row_index[k + 54] * M_ROW + i + 24);
                __m256 neg273 = _mm256_load_ps(X + row_index[k + 55] * M_ROW + i + 24);
                __m256 pos283 = _mm256_load_ps(X + row_index[k + 56] * M_ROW + i + 24);
                __m256 neg283 = _mm256_load_ps(X + row_index[k + 57] * M_ROW + i + 24);
                __m256 pos293 = _mm256_load_ps(X + row_index[k + 58] * M_ROW + i + 24);
                __m256 neg293 = _mm256_load_ps(X + row_index[k + 59] * M_ROW + i + 24);
                __m256 pos303 = _mm256_load_ps(X + row_index[k + 60] * M_ROW + i + 24);
                __m256 neg303 = _mm256_load_ps(X + row_index[k + 61] * M_ROW + i + 24);
                __m256 pos313 = _mm256_load_ps(X + row_index[k + 62] * M_ROW + i + 24);
                __m256 neg313 = _mm256_load_ps(X + row_index[k + 63] * M_ROW + i + 24);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res20 = _mm256_add_ps(res20, _mm256_sub_ps(pos20, neg20));
                res30 = _mm256_add_ps(res30, _mm256_sub_ps(pos30, neg30));
                res40 = _mm256_add_ps(res40, _mm256_sub_ps(pos40, neg40));
                res50 = _mm256_add_ps(res50, _mm256_sub_ps(pos50, neg50));
                res60 = _mm256_add_ps(res60, _mm256_sub_ps(pos60, neg60));
                res70 = _mm256_add_ps(res70, _mm256_sub_ps(pos70, neg70));
                res80 = _mm256_add_ps(res80, _mm256_sub_ps(pos80, neg80));
                res90 = _mm256_add_ps(res90, _mm256_sub_ps(pos90, neg90));
                res100 = _mm256_add_ps(res100, _mm256_sub_ps(pos100, neg100));
                res110 = _mm256_add_ps(res110, _mm256_sub_ps(pos110, neg110));
                res120 = _mm256_add_ps(res120, _mm256_sub_ps(pos120, neg120));
                res130 = _mm256_add_ps(res130, _mm256_sub_ps(pos130, neg130));
                res140 = _mm256_add_ps(res140, _mm256_sub_ps(pos140, neg140));
                res150 = _mm256_add_ps(res150, _mm256_sub_ps(pos150, neg150));
                res160 = _mm256_add_ps(res160, _mm256_sub_ps(pos160, neg160));
                res170 = _mm256_add_ps(res170, _mm256_sub_ps(pos170, neg170));
                res180 = _mm256_add_ps(res180, _mm256_sub_ps(pos180, neg180));
                res190 = _mm256_add_ps(res190, _mm256_sub_ps(pos190, neg190));
                res200 = _mm256_add_ps(res200, _mm256_sub_ps(pos200, neg200));
                res210 = _mm256_add_ps(res210, _mm256_sub_ps(pos210, neg210));
                res220 = _mm256_add_ps(res220, _mm256_sub_ps(pos220, neg220));
                res230 = _mm256_add_ps(res230, _mm256_sub_ps(pos230, neg230));
                res240 = _mm256_add_ps(res240, _mm256_sub_ps(pos240, neg240));
                res250 = _mm256_add_ps(res250, _mm256_sub_ps(pos250, neg250));
                res260 = _mm256_add_ps(res260, _mm256_sub_ps(pos260, neg260));
                res270 = _mm256_add_ps(res270, _mm256_sub_ps(pos270, neg270));
                res280 = _mm256_add_ps(res280, _mm256_sub_ps(pos280, neg280));
                res290 = _mm256_add_ps(res290, _mm256_sub_ps(pos290, neg290));
                res300 = _mm256_add_ps(res300, _mm256_sub_ps(pos300, neg300));
                res310 = _mm256_add_ps(res310, _mm256_sub_ps(pos310, neg310));
                res01 = _mm256_add_ps(res01, _mm256_sub_ps(pos01, neg01));
                res11 = _mm256_add_ps(res11, _mm256_sub_ps(pos11, neg11));
                res21 = _mm256_add_ps(res21, _mm256_sub_ps(pos21, neg21));
                res31 = _mm256_add_ps(res31, _mm256_sub_ps(pos31, neg31));
                res41 = _mm256_add_ps(res41, _mm256_sub_ps(pos41, neg41));
                res51 = _mm256_add_ps(res51, _mm256_sub_ps(pos51, neg51));
                res61 = _mm256_add_ps(res61, _mm256_sub_ps(pos61, neg61));
                res71 = _mm256_add_ps(res71, _mm256_sub_ps(pos71, neg71));
                res81 = _mm256_add_ps(res81, _mm256_sub_ps(pos81, neg81));
                res91 = _mm256_add_ps(res91, _mm256_sub_ps(pos91, neg91));
                res101 = _mm256_add_ps(res101, _mm256_sub_ps(pos101, neg101));
                res111 = _mm256_add_ps(res111, _mm256_sub_ps(pos111, neg111));
                res121 = _mm256_add_ps(res121, _mm256_sub_ps(pos121, neg121));
                res131 = _mm256_add_ps(res131, _mm256_sub_ps(pos131, neg131));
                res141 = _mm256_add_ps(res141, _mm256_sub_ps(pos141, neg141));
                res151 = _mm256_add_ps(res151, _mm256_sub_ps(pos151, neg151));
                res161 = _mm256_add_ps(res161, _mm256_sub_ps(pos161, neg161));
                res171 = _mm256_add_ps(res171, _mm256_sub_ps(pos171, neg171));
                res181 = _mm256_add_ps(res181, _mm256_sub_ps(pos181, neg181));
                res191 = _mm256_add_ps(res191, _mm256_sub_ps(pos191, neg191));
                res201 = _mm256_add_ps(res201, _mm256_sub_ps(pos201, neg201));
                res211 = _mm256_add_ps(res211, _mm256_sub_ps(pos211, neg211));
                res221 = _mm256_add_ps(res221, _mm256_sub_ps(pos221, neg221));
                res231 = _mm256_add_ps(res231, _mm256_sub_ps(pos231, neg231));
                res241 = _mm256_add_ps(res241, _mm256_sub_ps(pos241, neg241));
                res251 = _mm256_add_ps(res251, _mm256_sub_ps(pos251, neg251));
                res261 = _mm256_add_ps(res261, _mm256_sub_ps(pos261, neg261));
                res271 = _mm256_add_ps(res271, _mm256_sub_ps(pos271, neg271));
                res281 = _mm256_add_ps(res281, _mm256_sub_ps(pos281, neg281));
                res291 = _mm256_add_ps(res291, _mm256_sub_ps(pos291, neg291));
                res301 = _mm256_add_ps(res301, _mm256_sub_ps(pos301, neg301));
                res311 = _mm256_add_ps(res311, _mm256_sub_ps(pos311, neg311));
                res02 = _mm256_add_ps(res02, _mm256_sub_ps(pos02, neg02));
                res12 = _mm256_add_ps(res12, _mm256_sub_ps(pos12, neg12));
                res22 = _mm256_add_ps(res22, _mm256_sub_ps(pos22, neg22));
                res32 = _mm256_add_ps(res32, _mm256_sub_ps(pos32, neg32));
                res42 = _mm256_add_ps(res42, _mm256_sub_ps(pos42, neg42));
                res52 = _mm256_add_ps(res52, _mm256_sub_ps(pos52, neg52));
                res62 = _mm256_add_ps(res62, _mm256_sub_ps(pos62, neg62));
                res72 = _mm256_add_ps(res72, _mm256_sub_ps(pos72, neg72));
                res82 = _mm256_add_ps(res82, _mm256_sub_ps(pos82, neg82));
                res92 = _mm256_add_ps(res92, _mm256_sub_ps(pos92, neg92));
                res102 = _mm256_add_ps(res102, _mm256_sub_ps(pos102, neg102));
                res112 = _mm256_add_ps(res112, _mm256_sub_ps(pos112, neg112));
                res122 = _mm256_add_ps(res122, _mm256_sub_ps(pos122, neg122));
                res132 = _mm256_add_ps(res132, _mm256_sub_ps(pos132, neg132));
                res142 = _mm256_add_ps(res142, _mm256_sub_ps(pos142, neg142));
                res152 = _mm256_add_ps(res152, _mm256_sub_ps(pos152, neg152));
                res162 = _mm256_add_ps(res162, _mm256_sub_ps(pos162, neg162));
                res172 = _mm256_add_ps(res172, _mm256_sub_ps(pos172, neg172));
                res182 = _mm256_add_ps(res182, _mm256_sub_ps(pos182, neg182));
                res192 = _mm256_add_ps(res192, _mm256_sub_ps(pos192, neg192));
                res202 = _mm256_add_ps(res202, _mm256_sub_ps(pos202, neg202));
                res212 = _mm256_add_ps(res212, _mm256_sub_ps(pos212, neg212));
                res222 = _mm256_add_ps(res222, _mm256_sub_ps(pos222, neg222));
                res232 = _mm256_add_ps(res232, _mm256_sub_ps(pos232, neg232));
                res242 = _mm256_add_ps(res242, _mm256_sub_ps(pos242, neg242));
                res252 = _mm256_add_ps(res252, _mm256_sub_ps(pos252, neg252));
                res262 = _mm256_add_ps(res262, _mm256_sub_ps(pos262, neg262));
                res272 = _mm256_add_ps(res272, _mm256_sub_ps(pos272, neg272));
                res282 = _mm256_add_ps(res282, _mm256_sub_ps(pos282, neg282));
                res292 = _mm256_add_ps(res292, _mm256_sub_ps(pos292, neg292));
                res302 = _mm256_add_ps(res302, _mm256_sub_ps(pos302, neg302));
                res312 = _mm256_add_ps(res312, _mm256_sub_ps(pos312, neg312));
                res03 = _mm256_add_ps(res03, _mm256_sub_ps(pos03, neg03));
                res13 = _mm256_add_ps(res13, _mm256_sub_ps(pos13, neg13));
                res23 = _mm256_add_ps(res23, _mm256_sub_ps(pos23, neg23));
                res33 = _mm256_add_ps(res33, _mm256_sub_ps(pos33, neg33));
                res43 = _mm256_add_ps(res43, _mm256_sub_ps(pos43, neg43));
                res53 = _mm256_add_ps(res53, _mm256_sub_ps(pos53, neg53));
                res63 = _mm256_add_ps(res63, _mm256_sub_ps(pos63, neg63));
                res73 = _mm256_add_ps(res73, _mm256_sub_ps(pos73, neg73));
                res83 = _mm256_add_ps(res83, _mm256_sub_ps(pos83, neg83));
                res93 = _mm256_add_ps(res93, _mm256_sub_ps(pos93, neg93));
                res103 = _mm256_add_ps(res103, _mm256_sub_ps(pos103, neg103));
                res113 = _mm256_add_ps(res113, _mm256_sub_ps(pos113, neg113));
                res123 = _mm256_add_ps(res123, _mm256_sub_ps(pos123, neg123));
                res133 = _mm256_add_ps(res133, _mm256_sub_ps(pos133, neg133));
                res143 = _mm256_add_ps(res143, _mm256_sub_ps(pos143, neg143));
                res153 = _mm256_add_ps(res153, _mm256_sub_ps(pos153, neg153));
                res163 = _mm256_add_ps(res163, _mm256_sub_ps(pos163, neg163));
                res173 = _mm256_add_ps(res173, _mm256_sub_ps(pos173, neg173));
                res183 = _mm256_add_ps(res183, _mm256_sub_ps(pos183, neg183));
                res193 = _mm256_add_ps(res193, _mm256_sub_ps(pos193, neg193));
                res203 = _mm256_add_ps(res203, _mm256_sub_ps(pos203, neg203));
                res213 = _mm256_add_ps(res213, _mm256_sub_ps(pos213, neg213));
                res223 = _mm256_add_ps(res223, _mm256_sub_ps(pos223, neg223));
                res233 = _mm256_add_ps(res233, _mm256_sub_ps(pos233, neg233));
                res243 = _mm256_add_ps(res243, _mm256_sub_ps(pos243, neg243));
                res253 = _mm256_add_ps(res253, _mm256_sub_ps(pos253, neg253));
                res263 = _mm256_add_ps(res263, _mm256_sub_ps(pos263, neg263));
                res273 = _mm256_add_ps(res273, _mm256_sub_ps(pos273, neg273));
                res283 = _mm256_add_ps(res283, _mm256_sub_ps(pos283, neg283));
                res293 = _mm256_add_ps(res293, _mm256_sub_ps(pos293, neg293));
                res303 = _mm256_add_ps(res303, _mm256_sub_ps(pos303, neg303));
                res313 = _mm256_add_ps(res313, _mm256_sub_ps(pos313, neg313));
            }
            _mm256_store_ps(result + (j * 32 + 0) * M_ROW + i + 0, res00);
            _mm256_store_ps(result + (j * 32 + 1) * M_ROW + i + 0, res10);
            _mm256_store_ps(result + (j * 32 + 2) * M_ROW + i + 0, res20);
            _mm256_store_ps(result + (j * 32 + 3) * M_ROW + i + 0, res30);
            _mm256_store_ps(result + (j * 32 + 4) * M_ROW + i + 0, res40);
            _mm256_store_ps(result + (j * 32 + 5) * M_ROW + i + 0, res50);
            _mm256_store_ps(result + (j * 32 + 6) * M_ROW + i + 0, res60);
            _mm256_store_ps(result + (j * 32 + 7) * M_ROW + i + 0, res70);
            _mm256_store_ps(result + (j * 32 + 8) * M_ROW + i + 0, res80);
            _mm256_store_ps(result + (j * 32 + 9) * M_ROW + i + 0, res90);
            _mm256_store_ps(result + (j * 32 + 10) * M_ROW + i + 0, res100);
            _mm256_store_ps(result + (j * 32 + 11) * M_ROW + i + 0, res110);
            _mm256_store_ps(result + (j * 32 + 12) * M_ROW + i + 0, res120);
            _mm256_store_ps(result + (j * 32 + 13) * M_ROW + i + 0, res130);
            _mm256_store_ps(result + (j * 32 + 14) * M_ROW + i + 0, res140);
            _mm256_store_ps(result + (j * 32 + 15) * M_ROW + i + 0, res150);
            _mm256_store_ps(result + (j * 32 + 16) * M_ROW + i + 0, res160);
            _mm256_store_ps(result + (j * 32 + 17) * M_ROW + i + 0, res170);
            _mm256_store_ps(result + (j * 32 + 18) * M_ROW + i + 0, res180);
            _mm256_store_ps(result + (j * 32 + 19) * M_ROW + i + 0, res190);
            _mm256_store_ps(result + (j * 32 + 20) * M_ROW + i + 0, res200);
            _mm256_store_ps(result + (j * 32 + 21) * M_ROW + i + 0, res210);
            _mm256_store_ps(result + (j * 32 + 22) * M_ROW + i + 0, res220);
            _mm256_store_ps(result + (j * 32 + 23) * M_ROW + i + 0, res230);
            _mm256_store_ps(result + (j * 32 + 24) * M_ROW + i + 0, res240);
            _mm256_store_ps(result + (j * 32 + 25) * M_ROW + i + 0, res250);
            _mm256_store_ps(result + (j * 32 + 26) * M_ROW + i + 0, res260);
            _mm256_store_ps(result + (j * 32 + 27) * M_ROW + i + 0, res270);
            _mm256_store_ps(result + (j * 32 + 28) * M_ROW + i + 0, res280);
            _mm256_store_ps(result + (j * 32 + 29) * M_ROW + i + 0, res290);
            _mm256_store_ps(result + (j * 32 + 30) * M_ROW + i + 0, res300);
            _mm256_store_ps(result + (j * 32 + 31) * M_ROW + i + 0, res310);
            _mm256_store_ps(result + (j * 32 + 0) * M_ROW + i + 8, res01);
            _mm256_store_ps(result + (j * 32 + 1) * M_ROW + i + 8, res11);
            _mm256_store_ps(result + (j * 32 + 2) * M_ROW + i + 8, res21);
            _mm256_store_ps(result + (j * 32 + 3) * M_ROW + i + 8, res31);
            _mm256_store_ps(result + (j * 32 + 4) * M_ROW + i + 8, res41);
            _mm256_store_ps(result + (j * 32 + 5) * M_ROW + i + 8, res51);
            _mm256_store_ps(result + (j * 32 + 6) * M_ROW + i + 8, res61);
            _mm256_store_ps(result + (j * 32 + 7) * M_ROW + i + 8, res71);
            _mm256_store_ps(result + (j * 32 + 8) * M_ROW + i + 8, res81);
            _mm256_store_ps(result + (j * 32 + 9) * M_ROW + i + 8, res91);
            _mm256_store_ps(result + (j * 32 + 10) * M_ROW + i + 8, res101);
            _mm256_store_ps(result + (j * 32 + 11) * M_ROW + i + 8, res111);
            _mm256_store_ps(result + (j * 32 + 12) * M_ROW + i + 8, res121);
            _mm256_store_ps(result + (j * 32 + 13) * M_ROW + i + 8, res131);
            _mm256_store_ps(result + (j * 32 + 14) * M_ROW + i + 8, res141);
            _mm256_store_ps(result + (j * 32 + 15) * M_ROW + i + 8, res151);
            _mm256_store_ps(result + (j * 32 + 16) * M_ROW + i + 8, res161);
            _mm256_store_ps(result + (j * 32 + 17) * M_ROW + i + 8, res171);
            _mm256_store_ps(result + (j * 32 + 18) * M_ROW + i + 8, res181);
            _mm256_store_ps(result + (j * 32 + 19) * M_ROW + i + 8, res191);
            _mm256_store_ps(result + (j * 32 + 20) * M_ROW + i + 8, res201);
            _mm256_store_ps(result + (j * 32 + 21) * M_ROW + i + 8, res211);
            _mm256_store_ps(result + (j * 32 + 22) * M_ROW + i + 8, res221);
            _mm256_store_ps(result + (j * 32 + 23) * M_ROW + i + 8, res231);
            _mm256_store_ps(result + (j * 32 + 24) * M_ROW + i + 8, res241);
            _mm256_store_ps(result + (j * 32 + 25) * M_ROW + i + 8, res251);
            _mm256_store_ps(result + (j * 32 + 26) * M_ROW + i + 8, res261);
            _mm256_store_ps(result + (j * 32 + 27) * M_ROW + i + 8, res271);
            _mm256_store_ps(result + (j * 32 + 28) * M_ROW + i + 8, res281);
            _mm256_store_ps(result + (j * 32 + 29) * M_ROW + i + 8, res291);
            _mm256_store_ps(result + (j * 32 + 30) * M_ROW + i + 8, res301);
            _mm256_store_ps(result + (j * 32 + 31) * M_ROW + i + 8, res311);
            _mm256_store_ps(result + (j * 32 + 0) * M_ROW + i + 16, res02);
            _mm256_store_ps(result + (j * 32 + 1) * M_ROW + i + 16, res12);
            _mm256_store_ps(result + (j * 32 + 2) * M_ROW + i + 16, res22);
            _mm256_store_ps(result + (j * 32 + 3) * M_ROW + i + 16, res32);
            _mm256_store_ps(result + (j * 32 + 4) * M_ROW + i + 16, res42);
            _mm256_store_ps(result + (j * 32 + 5) * M_ROW + i + 16, res52);
            _mm256_store_ps(result + (j * 32 + 6) * M_ROW + i + 16, res62);
            _mm256_store_ps(result + (j * 32 + 7) * M_ROW + i + 16, res72);
            _mm256_store_ps(result + (j * 32 + 8) * M_ROW + i + 16, res82);
            _mm256_store_ps(result + (j * 32 + 9) * M_ROW + i + 16, res92);
            _mm256_store_ps(result + (j * 32 + 10) * M_ROW + i + 16, res102);
            _mm256_store_ps(result + (j * 32 + 11) * M_ROW + i + 16, res112);
            _mm256_store_ps(result + (j * 32 + 12) * M_ROW + i + 16, res122);
            _mm256_store_ps(result + (j * 32 + 13) * M_ROW + i + 16, res132);
            _mm256_store_ps(result + (j * 32 + 14) * M_ROW + i + 16, res142);
            _mm256_store_ps(result + (j * 32 + 15) * M_ROW + i + 16, res152);
            _mm256_store_ps(result + (j * 32 + 16) * M_ROW + i + 16, res162);
            _mm256_store_ps(result + (j * 32 + 17) * M_ROW + i + 16, res172);
            _mm256_store_ps(result + (j * 32 + 18) * M_ROW + i + 16, res182);
            _mm256_store_ps(result + (j * 32 + 19) * M_ROW + i + 16, res192);
            _mm256_store_ps(result + (j * 32 + 20) * M_ROW + i + 16, res202);
            _mm256_store_ps(result + (j * 32 + 21) * M_ROW + i + 16, res212);
            _mm256_store_ps(result + (j * 32 + 22) * M_ROW + i + 16, res222);
            _mm256_store_ps(result + (j * 32 + 23) * M_ROW + i + 16, res232);
            _mm256_store_ps(result + (j * 32 + 24) * M_ROW + i + 16, res242);
            _mm256_store_ps(result + (j * 32 + 25) * M_ROW + i + 16, res252);
            _mm256_store_ps(result + (j * 32 + 26) * M_ROW + i + 16, res262);
            _mm256_store_ps(result + (j * 32 + 27) * M_ROW + i + 16, res272);
            _mm256_store_ps(result + (j * 32 + 28) * M_ROW + i + 16, res282);
            _mm256_store_ps(result + (j * 32 + 29) * M_ROW + i + 16, res292);
            _mm256_store_ps(result + (j * 32 + 30) * M_ROW + i + 16, res302);
            _mm256_store_ps(result + (j * 32 + 31) * M_ROW + i + 16, res312);
            _mm256_store_ps(result + (j * 32 + 0) * M_ROW + i + 24, res03);
            _mm256_store_ps(result + (j * 32 + 1) * M_ROW + i + 24, res13);
            _mm256_store_ps(result + (j * 32 + 2) * M_ROW + i + 24, res23);
            _mm256_store_ps(result + (j * 32 + 3) * M_ROW + i + 24, res33);
            _mm256_store_ps(result + (j * 32 + 4) * M_ROW + i + 24, res43);
            _mm256_store_ps(result + (j * 32 + 5) * M_ROW + i + 24, res53);
            _mm256_store_ps(result + (j * 32 + 6) * M_ROW + i + 24, res63);
            _mm256_store_ps(result + (j * 32 + 7) * M_ROW + i + 24, res73);
            _mm256_store_ps(result + (j * 32 + 8) * M_ROW + i + 24, res83);
            _mm256_store_ps(result + (j * 32 + 9) * M_ROW + i + 24, res93);
            _mm256_store_ps(result + (j * 32 + 10) * M_ROW + i + 24, res103);
            _mm256_store_ps(result + (j * 32 + 11) * M_ROW + i + 24, res113);
            _mm256_store_ps(result + (j * 32 + 12) * M_ROW + i + 24, res123);
            _mm256_store_ps(result + (j * 32 + 13) * M_ROW + i + 24, res133);
            _mm256_store_ps(result + (j * 32 + 14) * M_ROW + i + 24, res143);
            _mm256_store_ps(result + (j * 32 + 15) * M_ROW + i + 24, res153);
            _mm256_store_ps(result + (j * 32 + 16) * M_ROW + i + 24, res163);
            _mm256_store_ps(result + (j * 32 + 17) * M_ROW + i + 24, res173);
            _mm256_store_ps(result + (j * 32 + 18) * M_ROW + i + 24, res183);
            _mm256_store_ps(result + (j * 32 + 19) * M_ROW + i + 24, res193);
            _mm256_store_ps(result + (j * 32 + 20) * M_ROW + i + 24, res203);
            _mm256_store_ps(result + (j * 32 + 21) * M_ROW + i + 24, res213);
            _mm256_store_ps(result + (j * 32 + 22) * M_ROW + i + 24, res223);
            _mm256_store_ps(result + (j * 32 + 23) * M_ROW + i + 24, res233);
            _mm256_store_ps(result + (j * 32 + 24) * M_ROW + i + 24, res243);
            _mm256_store_ps(result + (j * 32 + 25) * M_ROW + i + 24, res253);
            _mm256_store_ps(result + (j * 32 + 26) * M_ROW + i + 24, res263);
            _mm256_store_ps(result + (j * 32 + 27) * M_ROW + i + 24, res273);
            _mm256_store_ps(result + (j * 32 + 28) * M_ROW + i + 24, res283);
            _mm256_store_ps(result + (j * 32 + 29) * M_ROW + i + 24, res293);
            _mm256_store_ps(result + (j * 32 + 30) * M_ROW + i + 24, res303);
            _mm256_store_ps(result + (j * 32 + 31) * M_ROW + i + 24, res313);
        }
    }
}



void GEMM_CPU_FP32_colMajor_TCSC_Merged_16xG1_AVX512_OpenMP(const float* X, const int32_t* metadata, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 1; j++) {
        const int* groupData  = &metadata[j * 4];
        for (int i = 0; i < M_ROW; i += 16) {
            __m512 res00 = _mm512_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 2) {
                __m512 pos00 = _mm512_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m512 neg00 = _mm512_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                res00 = _mm512_add_ps(res00, _mm512_sub_ps(pos00, neg00));
            }
            if (groupData[3] > 0) {
                for (int k = groupData[1]; k < groupData[2]; k++) {
                    res00 = _mm512_add_ps(res00, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                }
            }
            else {
                for (int k = groupData[1]; k < groupData[2]; k++) {
                    res00 = _mm512_sub_ps(res00, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                }
            }          
            _mm512_store_ps(result + (j * 1 + 0) * M_ROW + i + 0, res00);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Merged_GroupMin_16xG4_AVX512_OpenMP(const float* X, const int32_t* metadata, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 4; j++) {
        const int* groupData  = &metadata[j * 10];
        for (int i = 0; i < M_ROW; i += 16) {
            __m512 res00 = _mm512_setzero_ps();
            __m512 res10 = _mm512_setzero_ps();
            __m512 res20 = _mm512_setzero_ps();
            __m512 res30 = _mm512_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 8) {
                __m512 pos00 = _mm512_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m512 neg00 = _mm512_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m512 pos10 = _mm512_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m512 neg10 = _mm512_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m512 pos20 = _mm512_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m512 neg20 = _mm512_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m512 pos30 = _mm512_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m512 neg30 = _mm512_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                res00 = _mm512_add_ps(res00, _mm512_sub_ps(pos00, neg00));
                res10 = _mm512_add_ps(res10, _mm512_sub_ps(pos10, neg10));
                res20 = _mm512_add_ps(res20, _mm512_sub_ps(pos20, neg20));
                res30 = _mm512_add_ps(res30, _mm512_sub_ps(pos30, neg30));
            }
            for (int k = groupData[1]; k < groupData[2]; k++) {
                res00 = _mm512_add_ps(res00, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[2]; k < groupData[3]; k++) {
                res00 = _mm512_sub_ps(res00, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[3]; k < groupData[4]; k++) {
                res10 = _mm512_add_ps(res10, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[4]; k < groupData[5]; k++) {
                res10 = _mm512_sub_ps(res10, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[5]; k < groupData[6]; k++) {
                res20 = _mm512_add_ps(res20, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[6]; k < groupData[7]; k++) {
                res20 = _mm512_sub_ps(res20, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[7]; k < groupData[8]; k++) {
                res30 = _mm512_add_ps(res30, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[8]; k < groupData[9]; k++) {
                res30 = _mm512_sub_ps(res30, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            _mm512_store_ps(result + (j * 4 + 0) * M_ROW + i + 0, res00);
            _mm512_store_ps(result + (j * 4 + 1) * M_ROW + i + 0, res10);
            _mm512_store_ps(result + (j * 4 + 2) * M_ROW + i + 0, res20);
            _mm512_store_ps(result + (j * 4 + 3) * M_ROW + i + 0, res30);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Merged_GroupMin_16xG8_AVX512_OpenMP(const float* X, const int32_t* metadata, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 8; j++) {
        const int* groupData  = &metadata[j * 18];
        for (int i = 0; i < M_ROW; i += 16) {
            __m512 res00 = _mm512_setzero_ps();
            __m512 res10 = _mm512_setzero_ps();
            __m512 res20 = _mm512_setzero_ps();
            __m512 res30 = _mm512_setzero_ps();
            __m512 res40 = _mm512_setzero_ps();
            __m512 res50 = _mm512_setzero_ps();
            __m512 res60 = _mm512_setzero_ps();
            __m512 res70 = _mm512_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 16) {
                __m512 pos00 = _mm512_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m512 neg00 = _mm512_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m512 pos10 = _mm512_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m512 neg10 = _mm512_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m512 pos20 = _mm512_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m512 neg20 = _mm512_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m512 pos30 = _mm512_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m512 neg30 = _mm512_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m512 pos40 = _mm512_load_ps(X + row_index[k + 8] * M_ROW + i + 0);
                __m512 neg40 = _mm512_load_ps(X + row_index[k + 9] * M_ROW + i + 0);
                __m512 pos50 = _mm512_load_ps(X + row_index[k + 10] * M_ROW + i + 0);
                __m512 neg50 = _mm512_load_ps(X + row_index[k + 11] * M_ROW + i + 0);
                __m512 pos60 = _mm512_load_ps(X + row_index[k + 12] * M_ROW + i + 0);
                __m512 neg60 = _mm512_load_ps(X + row_index[k + 13] * M_ROW + i + 0);
                __m512 pos70 = _mm512_load_ps(X + row_index[k + 14] * M_ROW + i + 0);
                __m512 neg70 = _mm512_load_ps(X + row_index[k + 15] * M_ROW + i + 0);
                res00 = _mm512_add_ps(res00, _mm512_sub_ps(pos00, neg00));
                res10 = _mm512_add_ps(res10, _mm512_sub_ps(pos10, neg10));
                res20 = _mm512_add_ps(res20, _mm512_sub_ps(pos20, neg20));
                res30 = _mm512_add_ps(res30, _mm512_sub_ps(pos30, neg30));
                res40 = _mm512_add_ps(res40, _mm512_sub_ps(pos40, neg40));
                res50 = _mm512_add_ps(res50, _mm512_sub_ps(pos50, neg50));
                res60 = _mm512_add_ps(res60, _mm512_sub_ps(pos60, neg60));
                res70 = _mm512_add_ps(res70, _mm512_sub_ps(pos70, neg70));
            }
            for (int k = groupData[1]; k < groupData[2]; k++) {
                res00 = _mm512_add_ps(res00, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[2]; k < groupData[3]; k++) {
                res00 = _mm512_sub_ps(res00, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[3]; k < groupData[4]; k++) {
                res10 = _mm512_add_ps(res10, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[4]; k < groupData[5]; k++) {
                res10 = _mm512_sub_ps(res10, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[5]; k < groupData[6]; k++) {
                res20 = _mm512_add_ps(res20, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[6]; k < groupData[7]; k++) {
                res20 = _mm512_sub_ps(res20, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[7]; k < groupData[8]; k++) {
                res30 = _mm512_add_ps(res30, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[8]; k < groupData[9]; k++) {
                res30 = _mm512_sub_ps(res30, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[9]; k < groupData[10]; k++) {
                res40 = _mm512_add_ps(res40, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[10]; k < groupData[11]; k++) {
                res40 = _mm512_sub_ps(res40, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[11]; k < groupData[12]; k++) {
                res50 = _mm512_add_ps(res50, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[12]; k < groupData[13]; k++) {
                res50 = _mm512_sub_ps(res50, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[13]; k < groupData[14]; k++) {
                res60 = _mm512_add_ps(res60, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[14]; k < groupData[15]; k++) {
                res60 = _mm512_sub_ps(res60, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[15]; k < groupData[16]; k++) {
                res70 = _mm512_add_ps(res70, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[16]; k < groupData[17]; k++) {
                res70 = _mm512_sub_ps(res70, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            _mm512_store_ps(result + (j * 8 + 0) * M_ROW + i + 0, res00);
            _mm512_store_ps(result + (j * 8 + 1) * M_ROW + i + 0, res10);
            _mm512_store_ps(result + (j * 8 + 2) * M_ROW + i + 0, res20);
            _mm512_store_ps(result + (j * 8 + 3) * M_ROW + i + 0, res30);
            _mm512_store_ps(result + (j * 8 + 4) * M_ROW + i + 0, res40);
            _mm512_store_ps(result + (j * 8 + 5) * M_ROW + i + 0, res50);
            _mm512_store_ps(result + (j * 8 + 6) * M_ROW + i + 0, res60);
            _mm512_store_ps(result + (j * 8 + 7) * M_ROW + i + 0, res70);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Merged_GroupMin_16xG16_AVX512_OpenMP(const float* X, const int32_t* metadata, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 16; j++) {
        const int* groupData  = &metadata[j * 34];
        for (int i = 0; i < M_ROW; i += 16) {
            __m512 res00 = _mm512_setzero_ps();
            __m512 res10 = _mm512_setzero_ps();
            __m512 res20 = _mm512_setzero_ps();
            __m512 res30 = _mm512_setzero_ps();
            __m512 res40 = _mm512_setzero_ps();
            __m512 res50 = _mm512_setzero_ps();
            __m512 res60 = _mm512_setzero_ps();
            __m512 res70 = _mm512_setzero_ps();
            __m512 res80 = _mm512_setzero_ps();
            __m512 res90 = _mm512_setzero_ps();
            __m512 res100 = _mm512_setzero_ps();
            __m512 res110 = _mm512_setzero_ps();
            __m512 res120 = _mm512_setzero_ps();
            __m512 res130 = _mm512_setzero_ps();
            __m512 res140 = _mm512_setzero_ps();
            __m512 res150 = _mm512_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 32) {
                __m512 pos00 = _mm512_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m512 neg00 = _mm512_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m512 pos10 = _mm512_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m512 neg10 = _mm512_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m512 pos20 = _mm512_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m512 neg20 = _mm512_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m512 pos30 = _mm512_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m512 neg30 = _mm512_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m512 pos40 = _mm512_load_ps(X + row_index[k + 8] * M_ROW + i + 0);
                __m512 neg40 = _mm512_load_ps(X + row_index[k + 9] * M_ROW + i + 0);
                __m512 pos50 = _mm512_load_ps(X + row_index[k + 10] * M_ROW + i + 0);
                __m512 neg50 = _mm512_load_ps(X + row_index[k + 11] * M_ROW + i + 0);
                __m512 pos60 = _mm512_load_ps(X + row_index[k + 12] * M_ROW + i + 0);
                __m512 neg60 = _mm512_load_ps(X + row_index[k + 13] * M_ROW + i + 0);
                __m512 pos70 = _mm512_load_ps(X + row_index[k + 14] * M_ROW + i + 0);
                __m512 neg70 = _mm512_load_ps(X + row_index[k + 15] * M_ROW + i + 0);
                __m512 pos80 = _mm512_load_ps(X + row_index[k + 16] * M_ROW + i + 0);
                __m512 neg80 = _mm512_load_ps(X + row_index[k + 17] * M_ROW + i + 0);
                __m512 pos90 = _mm512_load_ps(X + row_index[k + 18] * M_ROW + i + 0);
                __m512 neg90 = _mm512_load_ps(X + row_index[k + 19] * M_ROW + i + 0);
                __m512 pos100 = _mm512_load_ps(X + row_index[k + 20] * M_ROW + i + 0);
                __m512 neg100 = _mm512_load_ps(X + row_index[k + 21] * M_ROW + i + 0);
                __m512 pos110 = _mm512_load_ps(X + row_index[k + 22] * M_ROW + i + 0);
                __m512 neg110 = _mm512_load_ps(X + row_index[k + 23] * M_ROW + i + 0);
                __m512 pos120 = _mm512_load_ps(X + row_index[k + 24] * M_ROW + i + 0);
                __m512 neg120 = _mm512_load_ps(X + row_index[k + 25] * M_ROW + i + 0);
                __m512 pos130 = _mm512_load_ps(X + row_index[k + 26] * M_ROW + i + 0);
                __m512 neg130 = _mm512_load_ps(X + row_index[k + 27] * M_ROW + i + 0);
                __m512 pos140 = _mm512_load_ps(X + row_index[k + 28] * M_ROW + i + 0);
                __m512 neg140 = _mm512_load_ps(X + row_index[k + 29] * M_ROW + i + 0);
                __m512 pos150 = _mm512_load_ps(X + row_index[k + 30] * M_ROW + i + 0);
                __m512 neg150 = _mm512_load_ps(X + row_index[k + 31] * M_ROW + i + 0);
                res00 = _mm512_add_ps(res00, _mm512_sub_ps(pos00, neg00));
                res10 = _mm512_add_ps(res10, _mm512_sub_ps(pos10, neg10));
                res20 = _mm512_add_ps(res20, _mm512_sub_ps(pos20, neg20));
                res30 = _mm512_add_ps(res30, _mm512_sub_ps(pos30, neg30));
                res40 = _mm512_add_ps(res40, _mm512_sub_ps(pos40, neg40));
                res50 = _mm512_add_ps(res50, _mm512_sub_ps(pos50, neg50));
                res60 = _mm512_add_ps(res60, _mm512_sub_ps(pos60, neg60));
                res70 = _mm512_add_ps(res70, _mm512_sub_ps(pos70, neg70));
                res80 = _mm512_add_ps(res80, _mm512_sub_ps(pos80, neg80));
                res90 = _mm512_add_ps(res90, _mm512_sub_ps(pos90, neg90));
                res100 = _mm512_add_ps(res100, _mm512_sub_ps(pos100, neg100));
                res110 = _mm512_add_ps(res110, _mm512_sub_ps(pos110, neg110));
                res120 = _mm512_add_ps(res120, _mm512_sub_ps(pos120, neg120));
                res130 = _mm512_add_ps(res130, _mm512_sub_ps(pos130, neg130));
                res140 = _mm512_add_ps(res140, _mm512_sub_ps(pos140, neg140));
                res150 = _mm512_add_ps(res150, _mm512_sub_ps(pos150, neg150));
            }
            for (int k = groupData[1]; k < groupData[2]; k++) {
                res00 = _mm512_add_ps(res00, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[2]; k < groupData[3]; k++) {
                res00 = _mm512_sub_ps(res00, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[3]; k < groupData[4]; k++) {
                res10 = _mm512_add_ps(res10, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[4]; k < groupData[5]; k++) {
                res10 = _mm512_sub_ps(res10, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[5]; k < groupData[6]; k++) {
                res20 = _mm512_add_ps(res20, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[6]; k < groupData[7]; k++) {
                res20 = _mm512_sub_ps(res20, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[7]; k < groupData[8]; k++) {
                res30 = _mm512_add_ps(res30, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[8]; k < groupData[9]; k++) {
                res30 = _mm512_sub_ps(res30, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[9]; k < groupData[10]; k++) {
                res40 = _mm512_add_ps(res40, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[10]; k < groupData[11]; k++) {
                res40 = _mm512_sub_ps(res40, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[11]; k < groupData[12]; k++) {
                res50 = _mm512_add_ps(res50, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[12]; k < groupData[13]; k++) {
                res50 = _mm512_sub_ps(res50, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[13]; k < groupData[14]; k++) {
                res60 = _mm512_add_ps(res60, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[14]; k < groupData[15]; k++) {
                res60 = _mm512_sub_ps(res60, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[15]; k < groupData[16]; k++) {
                res70 = _mm512_add_ps(res70, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[16]; k < groupData[17]; k++) {
                res70 = _mm512_sub_ps(res70, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[17]; k < groupData[18]; k++) {
                res80 = _mm512_add_ps(res80, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[18]; k < groupData[19]; k++) {
                res80 = _mm512_sub_ps(res80, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[19]; k < groupData[20]; k++) {
                res90 = _mm512_add_ps(res90, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[20]; k < groupData[21]; k++) {
                res90 = _mm512_sub_ps(res90, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[21]; k < groupData[22]; k++) {
                res100 = _mm512_add_ps(res100, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[22]; k < groupData[23]; k++) {
                res100 = _mm512_sub_ps(res100, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[23]; k < groupData[24]; k++) {
                res110 = _mm512_add_ps(res110, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[24]; k < groupData[25]; k++) {
                res110 = _mm512_sub_ps(res110, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[25]; k < groupData[26]; k++) {
                res120 = _mm512_add_ps(res120, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[26]; k < groupData[27]; k++) {
                res120 = _mm512_sub_ps(res120, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[27]; k < groupData[28]; k++) {
                res130 = _mm512_add_ps(res130, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[28]; k < groupData[29]; k++) {
                res130 = _mm512_sub_ps(res130, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[29]; k < groupData[30]; k++) {
                res140 = _mm512_add_ps(res140, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[30]; k < groupData[31]; k++) {
                res140 = _mm512_sub_ps(res140, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[31]; k < groupData[32]; k++) {
                res150 = _mm512_add_ps(res150, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[32]; k < groupData[33]; k++) {
                res150 = _mm512_sub_ps(res150, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            _mm512_store_ps(result + (j * 16 + 0) * M_ROW + i + 0, res00);
            _mm512_store_ps(result + (j * 16 + 1) * M_ROW + i + 0, res10);
            _mm512_store_ps(result + (j * 16 + 2) * M_ROW + i + 0, res20);
            _mm512_store_ps(result + (j * 16 + 3) * M_ROW + i + 0, res30);
            _mm512_store_ps(result + (j * 16 + 4) * M_ROW + i + 0, res40);
            _mm512_store_ps(result + (j * 16 + 5) * M_ROW + i + 0, res50);
            _mm512_store_ps(result + (j * 16 + 6) * M_ROW + i + 0, res60);
            _mm512_store_ps(result + (j * 16 + 7) * M_ROW + i + 0, res70);
            _mm512_store_ps(result + (j * 16 + 8) * M_ROW + i + 0, res80);
            _mm512_store_ps(result + (j * 16 + 9) * M_ROW + i + 0, res90);
            _mm512_store_ps(result + (j * 16 + 10) * M_ROW + i + 0, res100);
            _mm512_store_ps(result + (j * 16 + 11) * M_ROW + i + 0, res110);
            _mm512_store_ps(result + (j * 16 + 12) * M_ROW + i + 0, res120);
            _mm512_store_ps(result + (j * 16 + 13) * M_ROW + i + 0, res130);
            _mm512_store_ps(result + (j * 16 + 14) * M_ROW + i + 0, res140);
            _mm512_store_ps(result + (j * 16 + 15) * M_ROW + i + 0, res150);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Merged_GroupMin_16xG32_AVX512_OpenMP(const float* X, const int32_t* metadata, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 32; j++) {
        const int* groupData  = &metadata[j * 66];
        for (int i = 0; i < M_ROW; i += 16) {
            __m512 res00 = _mm512_setzero_ps();
            __m512 res10 = _mm512_setzero_ps();
            __m512 res20 = _mm512_setzero_ps();
            __m512 res30 = _mm512_setzero_ps();
            __m512 res40 = _mm512_setzero_ps();
            __m512 res50 = _mm512_setzero_ps();
            __m512 res60 = _mm512_setzero_ps();
            __m512 res70 = _mm512_setzero_ps();
            __m512 res80 = _mm512_setzero_ps();
            __m512 res90 = _mm512_setzero_ps();
            __m512 res100 = _mm512_setzero_ps();
            __m512 res110 = _mm512_setzero_ps();
            __m512 res120 = _mm512_setzero_ps();
            __m512 res130 = _mm512_setzero_ps();
            __m512 res140 = _mm512_setzero_ps();
            __m512 res150 = _mm512_setzero_ps();
            __m512 res160 = _mm512_setzero_ps();
            __m512 res170 = _mm512_setzero_ps();
            __m512 res180 = _mm512_setzero_ps();
            __m512 res190 = _mm512_setzero_ps();
            __m512 res200 = _mm512_setzero_ps();
            __m512 res210 = _mm512_setzero_ps();
            __m512 res220 = _mm512_setzero_ps();
            __m512 res230 = _mm512_setzero_ps();
            __m512 res240 = _mm512_setzero_ps();
            __m512 res250 = _mm512_setzero_ps();
            __m512 res260 = _mm512_setzero_ps();
            __m512 res270 = _mm512_setzero_ps();
            __m512 res280 = _mm512_setzero_ps();
            __m512 res290 = _mm512_setzero_ps();
            __m512 res300 = _mm512_setzero_ps();
            __m512 res310 = _mm512_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 64) {
                __m512 pos00 = _mm512_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m512 neg00 = _mm512_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m512 pos10 = _mm512_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m512 neg10 = _mm512_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m512 pos20 = _mm512_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m512 neg20 = _mm512_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m512 pos30 = _mm512_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m512 neg30 = _mm512_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m512 pos40 = _mm512_load_ps(X + row_index[k + 8] * M_ROW + i + 0);
                __m512 neg40 = _mm512_load_ps(X + row_index[k + 9] * M_ROW + i + 0);
                __m512 pos50 = _mm512_load_ps(X + row_index[k + 10] * M_ROW + i + 0);
                __m512 neg50 = _mm512_load_ps(X + row_index[k + 11] * M_ROW + i + 0);
                __m512 pos60 = _mm512_load_ps(X + row_index[k + 12] * M_ROW + i + 0);
                __m512 neg60 = _mm512_load_ps(X + row_index[k + 13] * M_ROW + i + 0);
                __m512 pos70 = _mm512_load_ps(X + row_index[k + 14] * M_ROW + i + 0);
                __m512 neg70 = _mm512_load_ps(X + row_index[k + 15] * M_ROW + i + 0);
                __m512 pos80 = _mm512_load_ps(X + row_index[k + 16] * M_ROW + i + 0);
                __m512 neg80 = _mm512_load_ps(X + row_index[k + 17] * M_ROW + i + 0);
                __m512 pos90 = _mm512_load_ps(X + row_index[k + 18] * M_ROW + i + 0);
                __m512 neg90 = _mm512_load_ps(X + row_index[k + 19] * M_ROW + i + 0);
                __m512 pos100 = _mm512_load_ps(X + row_index[k + 20] * M_ROW + i + 0);
                __m512 neg100 = _mm512_load_ps(X + row_index[k + 21] * M_ROW + i + 0);
                __m512 pos110 = _mm512_load_ps(X + row_index[k + 22] * M_ROW + i + 0);
                __m512 neg110 = _mm512_load_ps(X + row_index[k + 23] * M_ROW + i + 0);
                __m512 pos120 = _mm512_load_ps(X + row_index[k + 24] * M_ROW + i + 0);
                __m512 neg120 = _mm512_load_ps(X + row_index[k + 25] * M_ROW + i + 0);
                __m512 pos130 = _mm512_load_ps(X + row_index[k + 26] * M_ROW + i + 0);
                __m512 neg130 = _mm512_load_ps(X + row_index[k + 27] * M_ROW + i + 0);
                __m512 pos140 = _mm512_load_ps(X + row_index[k + 28] * M_ROW + i + 0);
                __m512 neg140 = _mm512_load_ps(X + row_index[k + 29] * M_ROW + i + 0);
                __m512 pos150 = _mm512_load_ps(X + row_index[k + 30] * M_ROW + i + 0);
                __m512 neg150 = _mm512_load_ps(X + row_index[k + 31] * M_ROW + i + 0);
                __m512 pos160 = _mm512_load_ps(X + row_index[k + 32] * M_ROW + i + 0);
                __m512 neg160 = _mm512_load_ps(X + row_index[k + 33] * M_ROW + i + 0);
                __m512 pos170 = _mm512_load_ps(X + row_index[k + 34] * M_ROW + i + 0);
                __m512 neg170 = _mm512_load_ps(X + row_index[k + 35] * M_ROW + i + 0);
                __m512 pos180 = _mm512_load_ps(X + row_index[k + 36] * M_ROW + i + 0);
                __m512 neg180 = _mm512_load_ps(X + row_index[k + 37] * M_ROW + i + 0);
                __m512 pos190 = _mm512_load_ps(X + row_index[k + 38] * M_ROW + i + 0);
                __m512 neg190 = _mm512_load_ps(X + row_index[k + 39] * M_ROW + i + 0);
                __m512 pos200 = _mm512_load_ps(X + row_index[k + 40] * M_ROW + i + 0);
                __m512 neg200 = _mm512_load_ps(X + row_index[k + 41] * M_ROW + i + 0);
                __m512 pos210 = _mm512_load_ps(X + row_index[k + 42] * M_ROW + i + 0);
                __m512 neg210 = _mm512_load_ps(X + row_index[k + 43] * M_ROW + i + 0);
                __m512 pos220 = _mm512_load_ps(X + row_index[k + 44] * M_ROW + i + 0);
                __m512 neg220 = _mm512_load_ps(X + row_index[k + 45] * M_ROW + i + 0);
                __m512 pos230 = _mm512_load_ps(X + row_index[k + 46] * M_ROW + i + 0);
                __m512 neg230 = _mm512_load_ps(X + row_index[k + 47] * M_ROW + i + 0);
                __m512 pos240 = _mm512_load_ps(X + row_index[k + 48] * M_ROW + i + 0);
                __m512 neg240 = _mm512_load_ps(X + row_index[k + 49] * M_ROW + i + 0);
                __m512 pos250 = _mm512_load_ps(X + row_index[k + 50] * M_ROW + i + 0);
                __m512 neg250 = _mm512_load_ps(X + row_index[k + 51] * M_ROW + i + 0);
                __m512 pos260 = _mm512_load_ps(X + row_index[k + 52] * M_ROW + i + 0);
                __m512 neg260 = _mm512_load_ps(X + row_index[k + 53] * M_ROW + i + 0);
                __m512 pos270 = _mm512_load_ps(X + row_index[k + 54] * M_ROW + i + 0);
                __m512 neg270 = _mm512_load_ps(X + row_index[k + 55] * M_ROW + i + 0);
                __m512 pos280 = _mm512_load_ps(X + row_index[k + 56] * M_ROW + i + 0);
                __m512 neg280 = _mm512_load_ps(X + row_index[k + 57] * M_ROW + i + 0);
                __m512 pos290 = _mm512_load_ps(X + row_index[k + 58] * M_ROW + i + 0);
                __m512 neg290 = _mm512_load_ps(X + row_index[k + 59] * M_ROW + i + 0);
                __m512 pos300 = _mm512_load_ps(X + row_index[k + 60] * M_ROW + i + 0);
                __m512 neg300 = _mm512_load_ps(X + row_index[k + 61] * M_ROW + i + 0);
                __m512 pos310 = _mm512_load_ps(X + row_index[k + 62] * M_ROW + i + 0);
                __m512 neg310 = _mm512_load_ps(X + row_index[k + 63] * M_ROW + i + 0);
                res00 = _mm512_add_ps(res00, _mm512_sub_ps(pos00, neg00));
                res10 = _mm512_add_ps(res10, _mm512_sub_ps(pos10, neg10));
                res20 = _mm512_add_ps(res20, _mm512_sub_ps(pos20, neg20));
                res30 = _mm512_add_ps(res30, _mm512_sub_ps(pos30, neg30));
                res40 = _mm512_add_ps(res40, _mm512_sub_ps(pos40, neg40));
                res50 = _mm512_add_ps(res50, _mm512_sub_ps(pos50, neg50));
                res60 = _mm512_add_ps(res60, _mm512_sub_ps(pos60, neg60));
                res70 = _mm512_add_ps(res70, _mm512_sub_ps(pos70, neg70));
                res80 = _mm512_add_ps(res80, _mm512_sub_ps(pos80, neg80));
                res90 = _mm512_add_ps(res90, _mm512_sub_ps(pos90, neg90));
                res100 = _mm512_add_ps(res100, _mm512_sub_ps(pos100, neg100));
                res110 = _mm512_add_ps(res110, _mm512_sub_ps(pos110, neg110));
                res120 = _mm512_add_ps(res120, _mm512_sub_ps(pos120, neg120));
                res130 = _mm512_add_ps(res130, _mm512_sub_ps(pos130, neg130));
                res140 = _mm512_add_ps(res140, _mm512_sub_ps(pos140, neg140));
                res150 = _mm512_add_ps(res150, _mm512_sub_ps(pos150, neg150));
                res160 = _mm512_add_ps(res160, _mm512_sub_ps(pos160, neg160));
                res170 = _mm512_add_ps(res170, _mm512_sub_ps(pos170, neg170));
                res180 = _mm512_add_ps(res180, _mm512_sub_ps(pos180, neg180));
                res190 = _mm512_add_ps(res190, _mm512_sub_ps(pos190, neg190));
                res200 = _mm512_add_ps(res200, _mm512_sub_ps(pos200, neg200));
                res210 = _mm512_add_ps(res210, _mm512_sub_ps(pos210, neg210));
                res220 = _mm512_add_ps(res220, _mm512_sub_ps(pos220, neg220));
                res230 = _mm512_add_ps(res230, _mm512_sub_ps(pos230, neg230));
                res240 = _mm512_add_ps(res240, _mm512_sub_ps(pos240, neg240));
                res250 = _mm512_add_ps(res250, _mm512_sub_ps(pos250, neg250));
                res260 = _mm512_add_ps(res260, _mm512_sub_ps(pos260, neg260));
                res270 = _mm512_add_ps(res270, _mm512_sub_ps(pos270, neg270));
                res280 = _mm512_add_ps(res280, _mm512_sub_ps(pos280, neg280));
                res290 = _mm512_add_ps(res290, _mm512_sub_ps(pos290, neg290));
                res300 = _mm512_add_ps(res300, _mm512_sub_ps(pos300, neg300));
                res310 = _mm512_add_ps(res310, _mm512_sub_ps(pos310, neg310));
            }
            for (int k = groupData[1]; k < groupData[2]; k++) {
                res00 = _mm512_add_ps(res00, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[2]; k < groupData[3]; k++) {
                res00 = _mm512_sub_ps(res00, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[3]; k < groupData[4]; k++) {
                res10 = _mm512_add_ps(res10, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[4]; k < groupData[5]; k++) {
                res10 = _mm512_sub_ps(res10, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[5]; k < groupData[6]; k++) {
                res20 = _mm512_add_ps(res20, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[6]; k < groupData[7]; k++) {
                res20 = _mm512_sub_ps(res20, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[7]; k < groupData[8]; k++) {
                res30 = _mm512_add_ps(res30, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[8]; k < groupData[9]; k++) {
                res30 = _mm512_sub_ps(res30, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[9]; k < groupData[10]; k++) {
                res40 = _mm512_add_ps(res40, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[10]; k < groupData[11]; k++) {
                res40 = _mm512_sub_ps(res40, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[11]; k < groupData[12]; k++) {
                res50 = _mm512_add_ps(res50, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[12]; k < groupData[13]; k++) {
                res50 = _mm512_sub_ps(res50, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[13]; k < groupData[14]; k++) {
                res60 = _mm512_add_ps(res60, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[14]; k < groupData[15]; k++) {
                res60 = _mm512_sub_ps(res60, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[15]; k < groupData[16]; k++) {
                res70 = _mm512_add_ps(res70, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[16]; k < groupData[17]; k++) {
                res70 = _mm512_sub_ps(res70, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[17]; k < groupData[18]; k++) {
                res80 = _mm512_add_ps(res80, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[18]; k < groupData[19]; k++) {
                res80 = _mm512_sub_ps(res80, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[19]; k < groupData[20]; k++) {
                res90 = _mm512_add_ps(res90, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[20]; k < groupData[21]; k++) {
                res90 = _mm512_sub_ps(res90, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[21]; k < groupData[22]; k++) {
                res100 = _mm512_add_ps(res100, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[22]; k < groupData[23]; k++) {
                res100 = _mm512_sub_ps(res100, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[23]; k < groupData[24]; k++) {
                res110 = _mm512_add_ps(res110, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[24]; k < groupData[25]; k++) {
                res110 = _mm512_sub_ps(res110, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[25]; k < groupData[26]; k++) {
                res120 = _mm512_add_ps(res120, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[26]; k < groupData[27]; k++) {
                res120 = _mm512_sub_ps(res120, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[27]; k < groupData[28]; k++) {
                res130 = _mm512_add_ps(res130, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[28]; k < groupData[29]; k++) {
                res130 = _mm512_sub_ps(res130, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[29]; k < groupData[30]; k++) {
                res140 = _mm512_add_ps(res140, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[30]; k < groupData[31]; k++) {
                res140 = _mm512_sub_ps(res140, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[31]; k < groupData[32]; k++) {
                res150 = _mm512_add_ps(res150, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[32]; k < groupData[33]; k++) {
                res150 = _mm512_sub_ps(res150, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[33]; k < groupData[34]; k++) {
                res160 = _mm512_add_ps(res160, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[34]; k < groupData[35]; k++) {
                res160 = _mm512_sub_ps(res160, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[35]; k < groupData[36]; k++) {
                res170 = _mm512_add_ps(res170, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[36]; k < groupData[37]; k++) {
                res170 = _mm512_sub_ps(res170, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[37]; k < groupData[38]; k++) {
                res180 = _mm512_add_ps(res180, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[38]; k < groupData[39]; k++) {
                res180 = _mm512_sub_ps(res180, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[39]; k < groupData[40]; k++) {
                res190 = _mm512_add_ps(res190, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[40]; k < groupData[41]; k++) {
                res190 = _mm512_sub_ps(res190, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[41]; k < groupData[42]; k++) {
                res200 = _mm512_add_ps(res200, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[42]; k < groupData[43]; k++) {
                res200 = _mm512_sub_ps(res200, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[43]; k < groupData[44]; k++) {
                res210 = _mm512_add_ps(res210, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[44]; k < groupData[45]; k++) {
                res210 = _mm512_sub_ps(res210, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[45]; k < groupData[46]; k++) {
                res220 = _mm512_add_ps(res220, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[46]; k < groupData[47]; k++) {
                res220 = _mm512_sub_ps(res220, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[47]; k < groupData[48]; k++) {
                res230 = _mm512_add_ps(res230, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[48]; k < groupData[49]; k++) {
                res230 = _mm512_sub_ps(res230, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[49]; k < groupData[50]; k++) {
                res240 = _mm512_add_ps(res240, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[50]; k < groupData[51]; k++) {
                res240 = _mm512_sub_ps(res240, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[51]; k < groupData[52]; k++) {
                res250 = _mm512_add_ps(res250, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[52]; k < groupData[53]; k++) {
                res250 = _mm512_sub_ps(res250, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[53]; k < groupData[54]; k++) {
                res260 = _mm512_add_ps(res260, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[54]; k < groupData[55]; k++) {
                res260 = _mm512_sub_ps(res260, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[55]; k < groupData[56]; k++) {
                res270 = _mm512_add_ps(res270, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[56]; k < groupData[57]; k++) {
                res270 = _mm512_sub_ps(res270, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[57]; k < groupData[58]; k++) {
                res280 = _mm512_add_ps(res280, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[58]; k < groupData[59]; k++) {
                res280 = _mm512_sub_ps(res280, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[59]; k < groupData[60]; k++) {
                res290 = _mm512_add_ps(res290, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[60]; k < groupData[61]; k++) {
                res290 = _mm512_sub_ps(res290, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[61]; k < groupData[62]; k++) {
                res300 = _mm512_add_ps(res300, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[62]; k < groupData[63]; k++) {
                res300 = _mm512_sub_ps(res300, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[63]; k < groupData[64]; k++) {
                res310 = _mm512_add_ps(res310, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            for (int k = groupData[64]; k < groupData[65]; k++) {
                res310 = _mm512_sub_ps(res310, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
            }
            _mm512_store_ps(result + (j * 32 + 0) * M_ROW + i + 0, res00);
            _mm512_store_ps(result + (j * 32 + 1) * M_ROW + i + 0, res10);
            _mm512_store_ps(result + (j * 32 + 2) * M_ROW + i + 0, res20);
            _mm512_store_ps(result + (j * 32 + 3) * M_ROW + i + 0, res30);
            _mm512_store_ps(result + (j * 32 + 4) * M_ROW + i + 0, res40);
            _mm512_store_ps(result + (j * 32 + 5) * M_ROW + i + 0, res50);
            _mm512_store_ps(result + (j * 32 + 6) * M_ROW + i + 0, res60);
            _mm512_store_ps(result + (j * 32 + 7) * M_ROW + i + 0, res70);
            _mm512_store_ps(result + (j * 32 + 8) * M_ROW + i + 0, res80);
            _mm512_store_ps(result + (j * 32 + 9) * M_ROW + i + 0, res90);
            _mm512_store_ps(result + (j * 32 + 10) * M_ROW + i + 0, res100);
            _mm512_store_ps(result + (j * 32 + 11) * M_ROW + i + 0, res110);
            _mm512_store_ps(result + (j * 32 + 12) * M_ROW + i + 0, res120);
            _mm512_store_ps(result + (j * 32 + 13) * M_ROW + i + 0, res130);
            _mm512_store_ps(result + (j * 32 + 14) * M_ROW + i + 0, res140);
            _mm512_store_ps(result + (j * 32 + 15) * M_ROW + i + 0, res150);
            _mm512_store_ps(result + (j * 32 + 16) * M_ROW + i + 0, res160);
            _mm512_store_ps(result + (j * 32 + 17) * M_ROW + i + 0, res170);
            _mm512_store_ps(result + (j * 32 + 18) * M_ROW + i + 0, res180);
            _mm512_store_ps(result + (j * 32 + 19) * M_ROW + i + 0, res190);
            _mm512_store_ps(result + (j * 32 + 20) * M_ROW + i + 0, res200);
            _mm512_store_ps(result + (j * 32 + 21) * M_ROW + i + 0, res210);
            _mm512_store_ps(result + (j * 32 + 22) * M_ROW + i + 0, res220);
            _mm512_store_ps(result + (j * 32 + 23) * M_ROW + i + 0, res230);
            _mm512_store_ps(result + (j * 32 + 24) * M_ROW + i + 0, res240);
            _mm512_store_ps(result + (j * 32 + 25) * M_ROW + i + 0, res250);
            _mm512_store_ps(result + (j * 32 + 26) * M_ROW + i + 0, res260);
            _mm512_store_ps(result + (j * 32 + 27) * M_ROW + i + 0, res270);
            _mm512_store_ps(result + (j * 32 + 28) * M_ROW + i + 0, res280);
            _mm512_store_ps(result + (j * 32 + 29) * M_ROW + i + 0, res290);
            _mm512_store_ps(result + (j * 32 + 30) * M_ROW + i + 0, res300);
            _mm512_store_ps(result + (j * 32 + 31) * M_ROW + i + 0, res310);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Merged_32xG1_AVX512_OpenMP(const float* X, const int32_t* metadata, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 1; j++) {
        const int* groupData  = &metadata[j * 4];
        for (int i = 0; i < M_ROW; i += 32) {
            __m512 res00 = _mm512_setzero_ps();
            __m512 res01 = _mm512_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 2) {
                __m512 pos00 = _mm512_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m512 neg00 = _mm512_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m512 pos01 = _mm512_load_ps(X + row_index[k + 0] * M_ROW + i + 16);
                __m512 neg01 = _mm512_load_ps(X + row_index[k + 1] * M_ROW + i + 16);
                res00 = _mm512_add_ps(res00, _mm512_sub_ps(pos00, neg00));
                res01 = _mm512_add_ps(res01, _mm512_sub_ps(pos01, neg01));
            }
            if (groupData[3] > 0) {
                for (int k = groupData[1]; k < groupData[2]; k++) {
                    res00 = _mm512_add_ps(res00, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                    res01 = _mm512_add_ps(res01, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
                }
            }
            else {
                for (int k = groupData[1]; k < groupData[2]; k++) {
                    res00 = _mm512_sub_ps(res00, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                    res01 = _mm512_sub_ps(res01, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
                }
            }                       
            _mm512_store_ps(result + (j * 1 + 0) * M_ROW + i + 0, res00);
            _mm512_store_ps(result + (j * 1 + 0) * M_ROW + i + 16, res01);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Merged_GroupMin_32xG4_AVX512_OpenMP(const float* X, const int32_t* metadata, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 4; j++) {
        const int* groupData  = &metadata[j * 10];
        for (int i = 0; i < M_ROW; i += 32) {
            __m512 res00 = _mm512_setzero_ps();
            __m512 res01 = _mm512_setzero_ps();
            __m512 res10 = _mm512_setzero_ps();
            __m512 res11 = _mm512_setzero_ps();
            __m512 res20 = _mm512_setzero_ps();
            __m512 res21 = _mm512_setzero_ps();
            __m512 res30 = _mm512_setzero_ps();
            __m512 res31 = _mm512_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 8) {
                __m512 pos00 = _mm512_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m512 neg00 = _mm512_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m512 pos10 = _mm512_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m512 neg10 = _mm512_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m512 pos20 = _mm512_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m512 neg20 = _mm512_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m512 pos30 = _mm512_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m512 neg30 = _mm512_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m512 pos01 = _mm512_load_ps(X + row_index[k + 0] * M_ROW + i + 16);
                __m512 neg01 = _mm512_load_ps(X + row_index[k + 1] * M_ROW + i + 16);
                __m512 pos11 = _mm512_load_ps(X + row_index[k + 2] * M_ROW + i + 16);
                __m512 neg11 = _mm512_load_ps(X + row_index[k + 3] * M_ROW + i + 16);
                __m512 pos21 = _mm512_load_ps(X + row_index[k + 4] * M_ROW + i + 16);
                __m512 neg21 = _mm512_load_ps(X + row_index[k + 5] * M_ROW + i + 16);
                __m512 pos31 = _mm512_load_ps(X + row_index[k + 6] * M_ROW + i + 16);
                __m512 neg31 = _mm512_load_ps(X + row_index[k + 7] * M_ROW + i + 16);
                res00 = _mm512_add_ps(res00, _mm512_sub_ps(pos00, neg00));
                res10 = _mm512_add_ps(res10, _mm512_sub_ps(pos10, neg10));
                res20 = _mm512_add_ps(res20, _mm512_sub_ps(pos20, neg20));
                res30 = _mm512_add_ps(res30, _mm512_sub_ps(pos30, neg30));
                res01 = _mm512_add_ps(res01, _mm512_sub_ps(pos01, neg01));
                res11 = _mm512_add_ps(res11, _mm512_sub_ps(pos11, neg11));
                res21 = _mm512_add_ps(res21, _mm512_sub_ps(pos21, neg21));
                res31 = _mm512_add_ps(res31, _mm512_sub_ps(pos31, neg31));
            }
            for (int k = groupData[1]; k < groupData[2]; k++) {
                res00 = _mm512_add_ps(res00, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res01 = _mm512_add_ps(res01, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[2]; k < groupData[3]; k++) {
                res00 = _mm512_sub_ps(res00, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res01 = _mm512_sub_ps(res01, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[3]; k < groupData[4]; k++) {
                res10 = _mm512_add_ps(res10, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res11 = _mm512_add_ps(res11, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[4]; k < groupData[5]; k++) {
                res10 = _mm512_sub_ps(res10, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res11 = _mm512_sub_ps(res11, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[5]; k < groupData[6]; k++) {
                res20 = _mm512_add_ps(res20, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res21 = _mm512_add_ps(res21, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[6]; k < groupData[7]; k++) {
                res20 = _mm512_sub_ps(res20, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res21 = _mm512_sub_ps(res21, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[7]; k < groupData[8]; k++) {
                res30 = _mm512_add_ps(res30, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res31 = _mm512_add_ps(res31, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[8]; k < groupData[9]; k++) {
                res30 = _mm512_sub_ps(res30, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res31 = _mm512_sub_ps(res31, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            _mm512_store_ps(result + (j * 4 + 0) * M_ROW + i + 0, res00);
            _mm512_store_ps(result + (j * 4 + 1) * M_ROW + i + 0, res10);
            _mm512_store_ps(result + (j * 4 + 2) * M_ROW + i + 0, res20);
            _mm512_store_ps(result + (j * 4 + 3) * M_ROW + i + 0, res30);
            _mm512_store_ps(result + (j * 4 + 0) * M_ROW + i + 16, res01);
            _mm512_store_ps(result + (j * 4 + 1) * M_ROW + i + 16, res11);
            _mm512_store_ps(result + (j * 4 + 2) * M_ROW + i + 16, res21);
            _mm512_store_ps(result + (j * 4 + 3) * M_ROW + i + 16, res31);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Merged_GroupMin_32xG8_AVX512_OpenMP(const float* X, const int32_t* metadata, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 8; j++) {
        const int* groupData  = &metadata[j * 18];
        for (int i = 0; i < M_ROW; i += 32) {
            __m512 res00 = _mm512_setzero_ps();
            __m512 res01 = _mm512_setzero_ps();
            __m512 res10 = _mm512_setzero_ps();
            __m512 res11 = _mm512_setzero_ps();
            __m512 res20 = _mm512_setzero_ps();
            __m512 res21 = _mm512_setzero_ps();
            __m512 res30 = _mm512_setzero_ps();
            __m512 res31 = _mm512_setzero_ps();
            __m512 res40 = _mm512_setzero_ps();
            __m512 res41 = _mm512_setzero_ps();
            __m512 res50 = _mm512_setzero_ps();
            __m512 res51 = _mm512_setzero_ps();
            __m512 res60 = _mm512_setzero_ps();
            __m512 res61 = _mm512_setzero_ps();
            __m512 res70 = _mm512_setzero_ps();
            __m512 res71 = _mm512_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 16) {
                __m512 pos00 = _mm512_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m512 neg00 = _mm512_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m512 pos10 = _mm512_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m512 neg10 = _mm512_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m512 pos20 = _mm512_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m512 neg20 = _mm512_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m512 pos30 = _mm512_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m512 neg30 = _mm512_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m512 pos40 = _mm512_load_ps(X + row_index[k + 8] * M_ROW + i + 0);
                __m512 neg40 = _mm512_load_ps(X + row_index[k + 9] * M_ROW + i + 0);
                __m512 pos50 = _mm512_load_ps(X + row_index[k + 10] * M_ROW + i + 0);
                __m512 neg50 = _mm512_load_ps(X + row_index[k + 11] * M_ROW + i + 0);
                __m512 pos60 = _mm512_load_ps(X + row_index[k + 12] * M_ROW + i + 0);
                __m512 neg60 = _mm512_load_ps(X + row_index[k + 13] * M_ROW + i + 0);
                __m512 pos70 = _mm512_load_ps(X + row_index[k + 14] * M_ROW + i + 0);
                __m512 neg70 = _mm512_load_ps(X + row_index[k + 15] * M_ROW + i + 0);
                __m512 pos01 = _mm512_load_ps(X + row_index[k + 0] * M_ROW + i + 16);
                __m512 neg01 = _mm512_load_ps(X + row_index[k + 1] * M_ROW + i + 16);
                __m512 pos11 = _mm512_load_ps(X + row_index[k + 2] * M_ROW + i + 16);
                __m512 neg11 = _mm512_load_ps(X + row_index[k + 3] * M_ROW + i + 16);
                __m512 pos21 = _mm512_load_ps(X + row_index[k + 4] * M_ROW + i + 16);
                __m512 neg21 = _mm512_load_ps(X + row_index[k + 5] * M_ROW + i + 16);
                __m512 pos31 = _mm512_load_ps(X + row_index[k + 6] * M_ROW + i + 16);
                __m512 neg31 = _mm512_load_ps(X + row_index[k + 7] * M_ROW + i + 16);
                __m512 pos41 = _mm512_load_ps(X + row_index[k + 8] * M_ROW + i + 16);
                __m512 neg41 = _mm512_load_ps(X + row_index[k + 9] * M_ROW + i + 16);
                __m512 pos51 = _mm512_load_ps(X + row_index[k + 10] * M_ROW + i + 16);
                __m512 neg51 = _mm512_load_ps(X + row_index[k + 11] * M_ROW + i + 16);
                __m512 pos61 = _mm512_load_ps(X + row_index[k + 12] * M_ROW + i + 16);
                __m512 neg61 = _mm512_load_ps(X + row_index[k + 13] * M_ROW + i + 16);
                __m512 pos71 = _mm512_load_ps(X + row_index[k + 14] * M_ROW + i + 16);
                __m512 neg71 = _mm512_load_ps(X + row_index[k + 15] * M_ROW + i + 16);
                res00 = _mm512_add_ps(res00, _mm512_sub_ps(pos00, neg00));
                res10 = _mm512_add_ps(res10, _mm512_sub_ps(pos10, neg10));
                res20 = _mm512_add_ps(res20, _mm512_sub_ps(pos20, neg20));
                res30 = _mm512_add_ps(res30, _mm512_sub_ps(pos30, neg30));
                res40 = _mm512_add_ps(res40, _mm512_sub_ps(pos40, neg40));
                res50 = _mm512_add_ps(res50, _mm512_sub_ps(pos50, neg50));
                res60 = _mm512_add_ps(res60, _mm512_sub_ps(pos60, neg60));
                res70 = _mm512_add_ps(res70, _mm512_sub_ps(pos70, neg70));
                res01 = _mm512_add_ps(res01, _mm512_sub_ps(pos01, neg01));
                res11 = _mm512_add_ps(res11, _mm512_sub_ps(pos11, neg11));
                res21 = _mm512_add_ps(res21, _mm512_sub_ps(pos21, neg21));
                res31 = _mm512_add_ps(res31, _mm512_sub_ps(pos31, neg31));
                res41 = _mm512_add_ps(res41, _mm512_sub_ps(pos41, neg41));
                res51 = _mm512_add_ps(res51, _mm512_sub_ps(pos51, neg51));
                res61 = _mm512_add_ps(res61, _mm512_sub_ps(pos61, neg61));
                res71 = _mm512_add_ps(res71, _mm512_sub_ps(pos71, neg71));
            }
            for (int k = groupData[1]; k < groupData[2]; k++) {
                res00 = _mm512_add_ps(res00, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res01 = _mm512_add_ps(res01, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[2]; k < groupData[3]; k++) {
                res00 = _mm512_sub_ps(res00, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res01 = _mm512_sub_ps(res01, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[3]; k < groupData[4]; k++) {
                res10 = _mm512_add_ps(res10, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res11 = _mm512_add_ps(res11, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[4]; k < groupData[5]; k++) {
                res10 = _mm512_sub_ps(res10, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res11 = _mm512_sub_ps(res11, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[5]; k < groupData[6]; k++) {
                res20 = _mm512_add_ps(res20, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res21 = _mm512_add_ps(res21, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[6]; k < groupData[7]; k++) {
                res20 = _mm512_sub_ps(res20, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res21 = _mm512_sub_ps(res21, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[7]; k < groupData[8]; k++) {
                res30 = _mm512_add_ps(res30, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res31 = _mm512_add_ps(res31, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[8]; k < groupData[9]; k++) {
                res30 = _mm512_sub_ps(res30, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res31 = _mm512_sub_ps(res31, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[9]; k < groupData[10]; k++) {
                res40 = _mm512_add_ps(res40, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res41 = _mm512_add_ps(res41, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[10]; k < groupData[11]; k++) {
                res40 = _mm512_sub_ps(res40, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res41 = _mm512_sub_ps(res41, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[11]; k < groupData[12]; k++) {
                res50 = _mm512_add_ps(res50, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res51 = _mm512_add_ps(res51, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[12]; k < groupData[13]; k++) {
                res50 = _mm512_sub_ps(res50, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res51 = _mm512_sub_ps(res51, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[13]; k < groupData[14]; k++) {
                res60 = _mm512_add_ps(res60, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res61 = _mm512_add_ps(res61, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[14]; k < groupData[15]; k++) {
                res60 = _mm512_sub_ps(res60, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res61 = _mm512_sub_ps(res61, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[15]; k < groupData[16]; k++) {
                res70 = _mm512_add_ps(res70, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res71 = _mm512_add_ps(res71, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[16]; k < groupData[17]; k++) {
                res70 = _mm512_sub_ps(res70, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res71 = _mm512_sub_ps(res71, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            _mm512_store_ps(result + (j * 8 + 0) * M_ROW + i + 0, res00);
            _mm512_store_ps(result + (j * 8 + 1) * M_ROW + i + 0, res10);
            _mm512_store_ps(result + (j * 8 + 2) * M_ROW + i + 0, res20);
            _mm512_store_ps(result + (j * 8 + 3) * M_ROW + i + 0, res30);
            _mm512_store_ps(result + (j * 8 + 4) * M_ROW + i + 0, res40);
            _mm512_store_ps(result + (j * 8 + 5) * M_ROW + i + 0, res50);
            _mm512_store_ps(result + (j * 8 + 6) * M_ROW + i + 0, res60);
            _mm512_store_ps(result + (j * 8 + 7) * M_ROW + i + 0, res70);
            _mm512_store_ps(result + (j * 8 + 0) * M_ROW + i + 16, res01);
            _mm512_store_ps(result + (j * 8 + 1) * M_ROW + i + 16, res11);
            _mm512_store_ps(result + (j * 8 + 2) * M_ROW + i + 16, res21);
            _mm512_store_ps(result + (j * 8 + 3) * M_ROW + i + 16, res31);
            _mm512_store_ps(result + (j * 8 + 4) * M_ROW + i + 16, res41);
            _mm512_store_ps(result + (j * 8 + 5) * M_ROW + i + 16, res51);
            _mm512_store_ps(result + (j * 8 + 6) * M_ROW + i + 16, res61);
            _mm512_store_ps(result + (j * 8 + 7) * M_ROW + i + 16, res71);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Merged_GroupMin_32xG16_AVX512_OpenMP(const float* X, const int32_t* metadata, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 16; j++) {
        const int* groupData  = &metadata[j * 34];
        for (int i = 0; i < M_ROW; i += 32) {
            __m512 res00 = _mm512_setzero_ps();
            __m512 res01 = _mm512_setzero_ps();
            __m512 res10 = _mm512_setzero_ps();
            __m512 res11 = _mm512_setzero_ps();
            __m512 res20 = _mm512_setzero_ps();
            __m512 res21 = _mm512_setzero_ps();
            __m512 res30 = _mm512_setzero_ps();
            __m512 res31 = _mm512_setzero_ps();
            __m512 res40 = _mm512_setzero_ps();
            __m512 res41 = _mm512_setzero_ps();
            __m512 res50 = _mm512_setzero_ps();
            __m512 res51 = _mm512_setzero_ps();
            __m512 res60 = _mm512_setzero_ps();
            __m512 res61 = _mm512_setzero_ps();
            __m512 res70 = _mm512_setzero_ps();
            __m512 res71 = _mm512_setzero_ps();
            __m512 res80 = _mm512_setzero_ps();
            __m512 res81 = _mm512_setzero_ps();
            __m512 res90 = _mm512_setzero_ps();
            __m512 res91 = _mm512_setzero_ps();
            __m512 res100 = _mm512_setzero_ps();
            __m512 res101 = _mm512_setzero_ps();
            __m512 res110 = _mm512_setzero_ps();
            __m512 res111 = _mm512_setzero_ps();
            __m512 res120 = _mm512_setzero_ps();
            __m512 res121 = _mm512_setzero_ps();
            __m512 res130 = _mm512_setzero_ps();
            __m512 res131 = _mm512_setzero_ps();
            __m512 res140 = _mm512_setzero_ps();
            __m512 res141 = _mm512_setzero_ps();
            __m512 res150 = _mm512_setzero_ps();
            __m512 res151 = _mm512_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 32) {
                __m512 pos00 = _mm512_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m512 neg00 = _mm512_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m512 pos10 = _mm512_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m512 neg10 = _mm512_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m512 pos20 = _mm512_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m512 neg20 = _mm512_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m512 pos30 = _mm512_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m512 neg30 = _mm512_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m512 pos40 = _mm512_load_ps(X + row_index[k + 8] * M_ROW + i + 0);
                __m512 neg40 = _mm512_load_ps(X + row_index[k + 9] * M_ROW + i + 0);
                __m512 pos50 = _mm512_load_ps(X + row_index[k + 10] * M_ROW + i + 0);
                __m512 neg50 = _mm512_load_ps(X + row_index[k + 11] * M_ROW + i + 0);
                __m512 pos60 = _mm512_load_ps(X + row_index[k + 12] * M_ROW + i + 0);
                __m512 neg60 = _mm512_load_ps(X + row_index[k + 13] * M_ROW + i + 0);
                __m512 pos70 = _mm512_load_ps(X + row_index[k + 14] * M_ROW + i + 0);
                __m512 neg70 = _mm512_load_ps(X + row_index[k + 15] * M_ROW + i + 0);
                __m512 pos80 = _mm512_load_ps(X + row_index[k + 16] * M_ROW + i + 0);
                __m512 neg80 = _mm512_load_ps(X + row_index[k + 17] * M_ROW + i + 0);
                __m512 pos90 = _mm512_load_ps(X + row_index[k + 18] * M_ROW + i + 0);
                __m512 neg90 = _mm512_load_ps(X + row_index[k + 19] * M_ROW + i + 0);
                __m512 pos100 = _mm512_load_ps(X + row_index[k + 20] * M_ROW + i + 0);
                __m512 neg100 = _mm512_load_ps(X + row_index[k + 21] * M_ROW + i + 0);
                __m512 pos110 = _mm512_load_ps(X + row_index[k + 22] * M_ROW + i + 0);
                __m512 neg110 = _mm512_load_ps(X + row_index[k + 23] * M_ROW + i + 0);
                __m512 pos120 = _mm512_load_ps(X + row_index[k + 24] * M_ROW + i + 0);
                __m512 neg120 = _mm512_load_ps(X + row_index[k + 25] * M_ROW + i + 0);
                __m512 pos130 = _mm512_load_ps(X + row_index[k + 26] * M_ROW + i + 0);
                __m512 neg130 = _mm512_load_ps(X + row_index[k + 27] * M_ROW + i + 0);
                __m512 pos140 = _mm512_load_ps(X + row_index[k + 28] * M_ROW + i + 0);
                __m512 neg140 = _mm512_load_ps(X + row_index[k + 29] * M_ROW + i + 0);
                __m512 pos150 = _mm512_load_ps(X + row_index[k + 30] * M_ROW + i + 0);
                __m512 neg150 = _mm512_load_ps(X + row_index[k + 31] * M_ROW + i + 0);
                __m512 pos01 = _mm512_load_ps(X + row_index[k + 0] * M_ROW + i + 16);
                __m512 neg01 = _mm512_load_ps(X + row_index[k + 1] * M_ROW + i + 16);
                __m512 pos11 = _mm512_load_ps(X + row_index[k + 2] * M_ROW + i + 16);
                __m512 neg11 = _mm512_load_ps(X + row_index[k + 3] * M_ROW + i + 16);
                __m512 pos21 = _mm512_load_ps(X + row_index[k + 4] * M_ROW + i + 16);
                __m512 neg21 = _mm512_load_ps(X + row_index[k + 5] * M_ROW + i + 16);
                __m512 pos31 = _mm512_load_ps(X + row_index[k + 6] * M_ROW + i + 16);
                __m512 neg31 = _mm512_load_ps(X + row_index[k + 7] * M_ROW + i + 16);
                __m512 pos41 = _mm512_load_ps(X + row_index[k + 8] * M_ROW + i + 16);
                __m512 neg41 = _mm512_load_ps(X + row_index[k + 9] * M_ROW + i + 16);
                __m512 pos51 = _mm512_load_ps(X + row_index[k + 10] * M_ROW + i + 16);
                __m512 neg51 = _mm512_load_ps(X + row_index[k + 11] * M_ROW + i + 16);
                __m512 pos61 = _mm512_load_ps(X + row_index[k + 12] * M_ROW + i + 16);
                __m512 neg61 = _mm512_load_ps(X + row_index[k + 13] * M_ROW + i + 16);
                __m512 pos71 = _mm512_load_ps(X + row_index[k + 14] * M_ROW + i + 16);
                __m512 neg71 = _mm512_load_ps(X + row_index[k + 15] * M_ROW + i + 16);
                __m512 pos81 = _mm512_load_ps(X + row_index[k + 16] * M_ROW + i + 16);
                __m512 neg81 = _mm512_load_ps(X + row_index[k + 17] * M_ROW + i + 16);
                __m512 pos91 = _mm512_load_ps(X + row_index[k + 18] * M_ROW + i + 16);
                __m512 neg91 = _mm512_load_ps(X + row_index[k + 19] * M_ROW + i + 16);
                __m512 pos101 = _mm512_load_ps(X + row_index[k + 20] * M_ROW + i + 16);
                __m512 neg101 = _mm512_load_ps(X + row_index[k + 21] * M_ROW + i + 16);
                __m512 pos111 = _mm512_load_ps(X + row_index[k + 22] * M_ROW + i + 16);
                __m512 neg111 = _mm512_load_ps(X + row_index[k + 23] * M_ROW + i + 16);
                __m512 pos121 = _mm512_load_ps(X + row_index[k + 24] * M_ROW + i + 16);
                __m512 neg121 = _mm512_load_ps(X + row_index[k + 25] * M_ROW + i + 16);
                __m512 pos131 = _mm512_load_ps(X + row_index[k + 26] * M_ROW + i + 16);
                __m512 neg131 = _mm512_load_ps(X + row_index[k + 27] * M_ROW + i + 16);
                __m512 pos141 = _mm512_load_ps(X + row_index[k + 28] * M_ROW + i + 16);
                __m512 neg141 = _mm512_load_ps(X + row_index[k + 29] * M_ROW + i + 16);
                __m512 pos151 = _mm512_load_ps(X + row_index[k + 30] * M_ROW + i + 16);
                __m512 neg151 = _mm512_load_ps(X + row_index[k + 31] * M_ROW + i + 16);
                res00 = _mm512_add_ps(res00, _mm512_sub_ps(pos00, neg00));
                res10 = _mm512_add_ps(res10, _mm512_sub_ps(pos10, neg10));
                res20 = _mm512_add_ps(res20, _mm512_sub_ps(pos20, neg20));
                res30 = _mm512_add_ps(res30, _mm512_sub_ps(pos30, neg30));
                res40 = _mm512_add_ps(res40, _mm512_sub_ps(pos40, neg40));
                res50 = _mm512_add_ps(res50, _mm512_sub_ps(pos50, neg50));
                res60 = _mm512_add_ps(res60, _mm512_sub_ps(pos60, neg60));
                res70 = _mm512_add_ps(res70, _mm512_sub_ps(pos70, neg70));
                res80 = _mm512_add_ps(res80, _mm512_sub_ps(pos80, neg80));
                res90 = _mm512_add_ps(res90, _mm512_sub_ps(pos90, neg90));
                res100 = _mm512_add_ps(res100, _mm512_sub_ps(pos100, neg100));
                res110 = _mm512_add_ps(res110, _mm512_sub_ps(pos110, neg110));
                res120 = _mm512_add_ps(res120, _mm512_sub_ps(pos120, neg120));
                res130 = _mm512_add_ps(res130, _mm512_sub_ps(pos130, neg130));
                res140 = _mm512_add_ps(res140, _mm512_sub_ps(pos140, neg140));
                res150 = _mm512_add_ps(res150, _mm512_sub_ps(pos150, neg150));
                res01 = _mm512_add_ps(res01, _mm512_sub_ps(pos01, neg01));
                res11 = _mm512_add_ps(res11, _mm512_sub_ps(pos11, neg11));
                res21 = _mm512_add_ps(res21, _mm512_sub_ps(pos21, neg21));
                res31 = _mm512_add_ps(res31, _mm512_sub_ps(pos31, neg31));
                res41 = _mm512_add_ps(res41, _mm512_sub_ps(pos41, neg41));
                res51 = _mm512_add_ps(res51, _mm512_sub_ps(pos51, neg51));
                res61 = _mm512_add_ps(res61, _mm512_sub_ps(pos61, neg61));
                res71 = _mm512_add_ps(res71, _mm512_sub_ps(pos71, neg71));
                res81 = _mm512_add_ps(res81, _mm512_sub_ps(pos81, neg81));
                res91 = _mm512_add_ps(res91, _mm512_sub_ps(pos91, neg91));
                res101 = _mm512_add_ps(res101, _mm512_sub_ps(pos101, neg101));
                res111 = _mm512_add_ps(res111, _mm512_sub_ps(pos111, neg111));
                res121 = _mm512_add_ps(res121, _mm512_sub_ps(pos121, neg121));
                res131 = _mm512_add_ps(res131, _mm512_sub_ps(pos131, neg131));
                res141 = _mm512_add_ps(res141, _mm512_sub_ps(pos141, neg141));
                res151 = _mm512_add_ps(res151, _mm512_sub_ps(pos151, neg151));
            }
            for (int k = groupData[1]; k < groupData[2]; k++) {
                res00 = _mm512_add_ps(res00, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res01 = _mm512_add_ps(res01, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[2]; k < groupData[3]; k++) {
                res00 = _mm512_sub_ps(res00, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res01 = _mm512_sub_ps(res01, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[3]; k < groupData[4]; k++) {
                res10 = _mm512_add_ps(res10, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res11 = _mm512_add_ps(res11, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[4]; k < groupData[5]; k++) {
                res10 = _mm512_sub_ps(res10, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res11 = _mm512_sub_ps(res11, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[5]; k < groupData[6]; k++) {
                res20 = _mm512_add_ps(res20, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res21 = _mm512_add_ps(res21, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[6]; k < groupData[7]; k++) {
                res20 = _mm512_sub_ps(res20, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res21 = _mm512_sub_ps(res21, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[7]; k < groupData[8]; k++) {
                res30 = _mm512_add_ps(res30, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res31 = _mm512_add_ps(res31, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[8]; k < groupData[9]; k++) {
                res30 = _mm512_sub_ps(res30, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res31 = _mm512_sub_ps(res31, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[9]; k < groupData[10]; k++) {
                res40 = _mm512_add_ps(res40, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res41 = _mm512_add_ps(res41, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[10]; k < groupData[11]; k++) {
                res40 = _mm512_sub_ps(res40, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res41 = _mm512_sub_ps(res41, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[11]; k < groupData[12]; k++) {
                res50 = _mm512_add_ps(res50, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res51 = _mm512_add_ps(res51, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[12]; k < groupData[13]; k++) {
                res50 = _mm512_sub_ps(res50, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res51 = _mm512_sub_ps(res51, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[13]; k < groupData[14]; k++) {
                res60 = _mm512_add_ps(res60, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res61 = _mm512_add_ps(res61, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[14]; k < groupData[15]; k++) {
                res60 = _mm512_sub_ps(res60, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res61 = _mm512_sub_ps(res61, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[15]; k < groupData[16]; k++) {
                res70 = _mm512_add_ps(res70, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res71 = _mm512_add_ps(res71, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[16]; k < groupData[17]; k++) {
                res70 = _mm512_sub_ps(res70, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res71 = _mm512_sub_ps(res71, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[17]; k < groupData[18]; k++) {
                res80 = _mm512_add_ps(res80, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res81 = _mm512_add_ps(res81, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[18]; k < groupData[19]; k++) {
                res80 = _mm512_sub_ps(res80, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res81 = _mm512_sub_ps(res81, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[19]; k < groupData[20]; k++) {
                res90 = _mm512_add_ps(res90, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res91 = _mm512_add_ps(res91, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[20]; k < groupData[21]; k++) {
                res90 = _mm512_sub_ps(res90, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res91 = _mm512_sub_ps(res91, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[21]; k < groupData[22]; k++) {
                res100 = _mm512_add_ps(res100, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res101 = _mm512_add_ps(res101, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[22]; k < groupData[23]; k++) {
                res100 = _mm512_sub_ps(res100, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res101 = _mm512_sub_ps(res101, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[23]; k < groupData[24]; k++) {
                res110 = _mm512_add_ps(res110, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res111 = _mm512_add_ps(res111, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[24]; k < groupData[25]; k++) {
                res110 = _mm512_sub_ps(res110, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res111 = _mm512_sub_ps(res111, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[25]; k < groupData[26]; k++) {
                res120 = _mm512_add_ps(res120, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res121 = _mm512_add_ps(res121, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[26]; k < groupData[27]; k++) {
                res120 = _mm512_sub_ps(res120, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res121 = _mm512_sub_ps(res121, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[27]; k < groupData[28]; k++) {
                res130 = _mm512_add_ps(res130, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res131 = _mm512_add_ps(res131, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[28]; k < groupData[29]; k++) {
                res130 = _mm512_sub_ps(res130, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res131 = _mm512_sub_ps(res131, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[29]; k < groupData[30]; k++) {
                res140 = _mm512_add_ps(res140, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res141 = _mm512_add_ps(res141, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[30]; k < groupData[31]; k++) {
                res140 = _mm512_sub_ps(res140, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res141 = _mm512_sub_ps(res141, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[31]; k < groupData[32]; k++) {
                res150 = _mm512_add_ps(res150, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res151 = _mm512_add_ps(res151, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[32]; k < groupData[33]; k++) {
                res150 = _mm512_sub_ps(res150, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res151 = _mm512_sub_ps(res151, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            _mm512_store_ps(result + (j * 16 + 0) * M_ROW + i + 0, res00);
            _mm512_store_ps(result + (j * 16 + 1) * M_ROW + i + 0, res10);
            _mm512_store_ps(result + (j * 16 + 2) * M_ROW + i + 0, res20);
            _mm512_store_ps(result + (j * 16 + 3) * M_ROW + i + 0, res30);
            _mm512_store_ps(result + (j * 16 + 4) * M_ROW + i + 0, res40);
            _mm512_store_ps(result + (j * 16 + 5) * M_ROW + i + 0, res50);
            _mm512_store_ps(result + (j * 16 + 6) * M_ROW + i + 0, res60);
            _mm512_store_ps(result + (j * 16 + 7) * M_ROW + i + 0, res70);
            _mm512_store_ps(result + (j * 16 + 8) * M_ROW + i + 0, res80);
            _mm512_store_ps(result + (j * 16 + 9) * M_ROW + i + 0, res90);
            _mm512_store_ps(result + (j * 16 + 10) * M_ROW + i + 0, res100);
            _mm512_store_ps(result + (j * 16 + 11) * M_ROW + i + 0, res110);
            _mm512_store_ps(result + (j * 16 + 12) * M_ROW + i + 0, res120);
            _mm512_store_ps(result + (j * 16 + 13) * M_ROW + i + 0, res130);
            _mm512_store_ps(result + (j * 16 + 14) * M_ROW + i + 0, res140);
            _mm512_store_ps(result + (j * 16 + 15) * M_ROW + i + 0, res150);
            _mm512_store_ps(result + (j * 16 + 0) * M_ROW + i + 16, res01);
            _mm512_store_ps(result + (j * 16 + 1) * M_ROW + i + 16, res11);
            _mm512_store_ps(result + (j * 16 + 2) * M_ROW + i + 16, res21);
            _mm512_store_ps(result + (j * 16 + 3) * M_ROW + i + 16, res31);
            _mm512_store_ps(result + (j * 16 + 4) * M_ROW + i + 16, res41);
            _mm512_store_ps(result + (j * 16 + 5) * M_ROW + i + 16, res51);
            _mm512_store_ps(result + (j * 16 + 6) * M_ROW + i + 16, res61);
            _mm512_store_ps(result + (j * 16 + 7) * M_ROW + i + 16, res71);
            _mm512_store_ps(result + (j * 16 + 8) * M_ROW + i + 16, res81);
            _mm512_store_ps(result + (j * 16 + 9) * M_ROW + i + 16, res91);
            _mm512_store_ps(result + (j * 16 + 10) * M_ROW + i + 16, res101);
            _mm512_store_ps(result + (j * 16 + 11) * M_ROW + i + 16, res111);
            _mm512_store_ps(result + (j * 16 + 12) * M_ROW + i + 16, res121);
            _mm512_store_ps(result + (j * 16 + 13) * M_ROW + i + 16, res131);
            _mm512_store_ps(result + (j * 16 + 14) * M_ROW + i + 16, res141);
            _mm512_store_ps(result + (j * 16 + 15) * M_ROW + i + 16, res151);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Merged_GroupMin_32xG32_AVX512_OpenMP(const float* X, const int32_t* metadata, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 32; j++) {
        const int* groupData  = &metadata[j * 66];
        for (int i = 0; i < M_ROW; i += 32) {
            __m512 res00 = _mm512_setzero_ps();
            __m512 res01 = _mm512_setzero_ps();
            __m512 res10 = _mm512_setzero_ps();
            __m512 res11 = _mm512_setzero_ps();
            __m512 res20 = _mm512_setzero_ps();
            __m512 res21 = _mm512_setzero_ps();
            __m512 res30 = _mm512_setzero_ps();
            __m512 res31 = _mm512_setzero_ps();
            __m512 res40 = _mm512_setzero_ps();
            __m512 res41 = _mm512_setzero_ps();
            __m512 res50 = _mm512_setzero_ps();
            __m512 res51 = _mm512_setzero_ps();
            __m512 res60 = _mm512_setzero_ps();
            __m512 res61 = _mm512_setzero_ps();
            __m512 res70 = _mm512_setzero_ps();
            __m512 res71 = _mm512_setzero_ps();
            __m512 res80 = _mm512_setzero_ps();
            __m512 res81 = _mm512_setzero_ps();
            __m512 res90 = _mm512_setzero_ps();
            __m512 res91 = _mm512_setzero_ps();
            __m512 res100 = _mm512_setzero_ps();
            __m512 res101 = _mm512_setzero_ps();
            __m512 res110 = _mm512_setzero_ps();
            __m512 res111 = _mm512_setzero_ps();
            __m512 res120 = _mm512_setzero_ps();
            __m512 res121 = _mm512_setzero_ps();
            __m512 res130 = _mm512_setzero_ps();
            __m512 res131 = _mm512_setzero_ps();
            __m512 res140 = _mm512_setzero_ps();
            __m512 res141 = _mm512_setzero_ps();
            __m512 res150 = _mm512_setzero_ps();
            __m512 res151 = _mm512_setzero_ps();
            __m512 res160 = _mm512_setzero_ps();
            __m512 res161 = _mm512_setzero_ps();
            __m512 res170 = _mm512_setzero_ps();
            __m512 res171 = _mm512_setzero_ps();
            __m512 res180 = _mm512_setzero_ps();
            __m512 res181 = _mm512_setzero_ps();
            __m512 res190 = _mm512_setzero_ps();
            __m512 res191 = _mm512_setzero_ps();
            __m512 res200 = _mm512_setzero_ps();
            __m512 res201 = _mm512_setzero_ps();
            __m512 res210 = _mm512_setzero_ps();
            __m512 res211 = _mm512_setzero_ps();
            __m512 res220 = _mm512_setzero_ps();
            __m512 res221 = _mm512_setzero_ps();
            __m512 res230 = _mm512_setzero_ps();
            __m512 res231 = _mm512_setzero_ps();
            __m512 res240 = _mm512_setzero_ps();
            __m512 res241 = _mm512_setzero_ps();
            __m512 res250 = _mm512_setzero_ps();
            __m512 res251 = _mm512_setzero_ps();
            __m512 res260 = _mm512_setzero_ps();
            __m512 res261 = _mm512_setzero_ps();
            __m512 res270 = _mm512_setzero_ps();
            __m512 res271 = _mm512_setzero_ps();
            __m512 res280 = _mm512_setzero_ps();
            __m512 res281 = _mm512_setzero_ps();
            __m512 res290 = _mm512_setzero_ps();
            __m512 res291 = _mm512_setzero_ps();
            __m512 res300 = _mm512_setzero_ps();
            __m512 res301 = _mm512_setzero_ps();
            __m512 res310 = _mm512_setzero_ps();
            __m512 res311 = _mm512_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 64) {
                __m512 pos00 = _mm512_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m512 neg00 = _mm512_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m512 pos10 = _mm512_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m512 neg10 = _mm512_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m512 pos20 = _mm512_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m512 neg20 = _mm512_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m512 pos30 = _mm512_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m512 neg30 = _mm512_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m512 pos40 = _mm512_load_ps(X + row_index[k + 8] * M_ROW + i + 0);
                __m512 neg40 = _mm512_load_ps(X + row_index[k + 9] * M_ROW + i + 0);
                __m512 pos50 = _mm512_load_ps(X + row_index[k + 10] * M_ROW + i + 0);
                __m512 neg50 = _mm512_load_ps(X + row_index[k + 11] * M_ROW + i + 0);
                __m512 pos60 = _mm512_load_ps(X + row_index[k + 12] * M_ROW + i + 0);
                __m512 neg60 = _mm512_load_ps(X + row_index[k + 13] * M_ROW + i + 0);
                __m512 pos70 = _mm512_load_ps(X + row_index[k + 14] * M_ROW + i + 0);
                __m512 neg70 = _mm512_load_ps(X + row_index[k + 15] * M_ROW + i + 0);
                __m512 pos80 = _mm512_load_ps(X + row_index[k + 16] * M_ROW + i + 0);
                __m512 neg80 = _mm512_load_ps(X + row_index[k + 17] * M_ROW + i + 0);
                __m512 pos90 = _mm512_load_ps(X + row_index[k + 18] * M_ROW + i + 0);
                __m512 neg90 = _mm512_load_ps(X + row_index[k + 19] * M_ROW + i + 0);
                __m512 pos100 = _mm512_load_ps(X + row_index[k + 20] * M_ROW + i + 0);
                __m512 neg100 = _mm512_load_ps(X + row_index[k + 21] * M_ROW + i + 0);
                __m512 pos110 = _mm512_load_ps(X + row_index[k + 22] * M_ROW + i + 0);
                __m512 neg110 = _mm512_load_ps(X + row_index[k + 23] * M_ROW + i + 0);
                __m512 pos120 = _mm512_load_ps(X + row_index[k + 24] * M_ROW + i + 0);
                __m512 neg120 = _mm512_load_ps(X + row_index[k + 25] * M_ROW + i + 0);
                __m512 pos130 = _mm512_load_ps(X + row_index[k + 26] * M_ROW + i + 0);
                __m512 neg130 = _mm512_load_ps(X + row_index[k + 27] * M_ROW + i + 0);
                __m512 pos140 = _mm512_load_ps(X + row_index[k + 28] * M_ROW + i + 0);
                __m512 neg140 = _mm512_load_ps(X + row_index[k + 29] * M_ROW + i + 0);
                __m512 pos150 = _mm512_load_ps(X + row_index[k + 30] * M_ROW + i + 0);
                __m512 neg150 = _mm512_load_ps(X + row_index[k + 31] * M_ROW + i + 0);
                __m512 pos160 = _mm512_load_ps(X + row_index[k + 32] * M_ROW + i + 0);
                __m512 neg160 = _mm512_load_ps(X + row_index[k + 33] * M_ROW + i + 0);
                __m512 pos170 = _mm512_load_ps(X + row_index[k + 34] * M_ROW + i + 0);
                __m512 neg170 = _mm512_load_ps(X + row_index[k + 35] * M_ROW + i + 0);
                __m512 pos180 = _mm512_load_ps(X + row_index[k + 36] * M_ROW + i + 0);
                __m512 neg180 = _mm512_load_ps(X + row_index[k + 37] * M_ROW + i + 0);
                __m512 pos190 = _mm512_load_ps(X + row_index[k + 38] * M_ROW + i + 0);
                __m512 neg190 = _mm512_load_ps(X + row_index[k + 39] * M_ROW + i + 0);
                __m512 pos200 = _mm512_load_ps(X + row_index[k + 40] * M_ROW + i + 0);
                __m512 neg200 = _mm512_load_ps(X + row_index[k + 41] * M_ROW + i + 0);
                __m512 pos210 = _mm512_load_ps(X + row_index[k + 42] * M_ROW + i + 0);
                __m512 neg210 = _mm512_load_ps(X + row_index[k + 43] * M_ROW + i + 0);
                __m512 pos220 = _mm512_load_ps(X + row_index[k + 44] * M_ROW + i + 0);
                __m512 neg220 = _mm512_load_ps(X + row_index[k + 45] * M_ROW + i + 0);
                __m512 pos230 = _mm512_load_ps(X + row_index[k + 46] * M_ROW + i + 0);
                __m512 neg230 = _mm512_load_ps(X + row_index[k + 47] * M_ROW + i + 0);
                __m512 pos240 = _mm512_load_ps(X + row_index[k + 48] * M_ROW + i + 0);
                __m512 neg240 = _mm512_load_ps(X + row_index[k + 49] * M_ROW + i + 0);
                __m512 pos250 = _mm512_load_ps(X + row_index[k + 50] * M_ROW + i + 0);
                __m512 neg250 = _mm512_load_ps(X + row_index[k + 51] * M_ROW + i + 0);
                __m512 pos260 = _mm512_load_ps(X + row_index[k + 52] * M_ROW + i + 0);
                __m512 neg260 = _mm512_load_ps(X + row_index[k + 53] * M_ROW + i + 0);
                __m512 pos270 = _mm512_load_ps(X + row_index[k + 54] * M_ROW + i + 0);
                __m512 neg270 = _mm512_load_ps(X + row_index[k + 55] * M_ROW + i + 0);
                __m512 pos280 = _mm512_load_ps(X + row_index[k + 56] * M_ROW + i + 0);
                __m512 neg280 = _mm512_load_ps(X + row_index[k + 57] * M_ROW + i + 0);
                __m512 pos290 = _mm512_load_ps(X + row_index[k + 58] * M_ROW + i + 0);
                __m512 neg290 = _mm512_load_ps(X + row_index[k + 59] * M_ROW + i + 0);
                __m512 pos300 = _mm512_load_ps(X + row_index[k + 60] * M_ROW + i + 0);
                __m512 neg300 = _mm512_load_ps(X + row_index[k + 61] * M_ROW + i + 0);
                __m512 pos310 = _mm512_load_ps(X + row_index[k + 62] * M_ROW + i + 0);
                __m512 neg310 = _mm512_load_ps(X + row_index[k + 63] * M_ROW + i + 0);
                __m512 pos01 = _mm512_load_ps(X + row_index[k + 0] * M_ROW + i + 16);
                __m512 neg01 = _mm512_load_ps(X + row_index[k + 1] * M_ROW + i + 16);
                __m512 pos11 = _mm512_load_ps(X + row_index[k + 2] * M_ROW + i + 16);
                __m512 neg11 = _mm512_load_ps(X + row_index[k + 3] * M_ROW + i + 16);
                __m512 pos21 = _mm512_load_ps(X + row_index[k + 4] * M_ROW + i + 16);
                __m512 neg21 = _mm512_load_ps(X + row_index[k + 5] * M_ROW + i + 16);
                __m512 pos31 = _mm512_load_ps(X + row_index[k + 6] * M_ROW + i + 16);
                __m512 neg31 = _mm512_load_ps(X + row_index[k + 7] * M_ROW + i + 16);
                __m512 pos41 = _mm512_load_ps(X + row_index[k + 8] * M_ROW + i + 16);
                __m512 neg41 = _mm512_load_ps(X + row_index[k + 9] * M_ROW + i + 16);
                __m512 pos51 = _mm512_load_ps(X + row_index[k + 10] * M_ROW + i + 16);
                __m512 neg51 = _mm512_load_ps(X + row_index[k + 11] * M_ROW + i + 16);
                __m512 pos61 = _mm512_load_ps(X + row_index[k + 12] * M_ROW + i + 16);
                __m512 neg61 = _mm512_load_ps(X + row_index[k + 13] * M_ROW + i + 16);
                __m512 pos71 = _mm512_load_ps(X + row_index[k + 14] * M_ROW + i + 16);
                __m512 neg71 = _mm512_load_ps(X + row_index[k + 15] * M_ROW + i + 16);
                __m512 pos81 = _mm512_load_ps(X + row_index[k + 16] * M_ROW + i + 16);
                __m512 neg81 = _mm512_load_ps(X + row_index[k + 17] * M_ROW + i + 16);
                __m512 pos91 = _mm512_load_ps(X + row_index[k + 18] * M_ROW + i + 16);
                __m512 neg91 = _mm512_load_ps(X + row_index[k + 19] * M_ROW + i + 16);
                __m512 pos101 = _mm512_load_ps(X + row_index[k + 20] * M_ROW + i + 16);
                __m512 neg101 = _mm512_load_ps(X + row_index[k + 21] * M_ROW + i + 16);
                __m512 pos111 = _mm512_load_ps(X + row_index[k + 22] * M_ROW + i + 16);
                __m512 neg111 = _mm512_load_ps(X + row_index[k + 23] * M_ROW + i + 16);
                __m512 pos121 = _mm512_load_ps(X + row_index[k + 24] * M_ROW + i + 16);
                __m512 neg121 = _mm512_load_ps(X + row_index[k + 25] * M_ROW + i + 16);
                __m512 pos131 = _mm512_load_ps(X + row_index[k + 26] * M_ROW + i + 16);
                __m512 neg131 = _mm512_load_ps(X + row_index[k + 27] * M_ROW + i + 16);
                __m512 pos141 = _mm512_load_ps(X + row_index[k + 28] * M_ROW + i + 16);
                __m512 neg141 = _mm512_load_ps(X + row_index[k + 29] * M_ROW + i + 16);
                __m512 pos151 = _mm512_load_ps(X + row_index[k + 30] * M_ROW + i + 16);
                __m512 neg151 = _mm512_load_ps(X + row_index[k + 31] * M_ROW + i + 16);
                __m512 pos161 = _mm512_load_ps(X + row_index[k + 32] * M_ROW + i + 16);
                __m512 neg161 = _mm512_load_ps(X + row_index[k + 33] * M_ROW + i + 16);
                __m512 pos171 = _mm512_load_ps(X + row_index[k + 34] * M_ROW + i + 16);
                __m512 neg171 = _mm512_load_ps(X + row_index[k + 35] * M_ROW + i + 16);
                __m512 pos181 = _mm512_load_ps(X + row_index[k + 36] * M_ROW + i + 16);
                __m512 neg181 = _mm512_load_ps(X + row_index[k + 37] * M_ROW + i + 16);
                __m512 pos191 = _mm512_load_ps(X + row_index[k + 38] * M_ROW + i + 16);
                __m512 neg191 = _mm512_load_ps(X + row_index[k + 39] * M_ROW + i + 16);
                __m512 pos201 = _mm512_load_ps(X + row_index[k + 40] * M_ROW + i + 16);
                __m512 neg201 = _mm512_load_ps(X + row_index[k + 41] * M_ROW + i + 16);
                __m512 pos211 = _mm512_load_ps(X + row_index[k + 42] * M_ROW + i + 16);
                __m512 neg211 = _mm512_load_ps(X + row_index[k + 43] * M_ROW + i + 16);
                __m512 pos221 = _mm512_load_ps(X + row_index[k + 44] * M_ROW + i + 16);
                __m512 neg221 = _mm512_load_ps(X + row_index[k + 45] * M_ROW + i + 16);
                __m512 pos231 = _mm512_load_ps(X + row_index[k + 46] * M_ROW + i + 16);
                __m512 neg231 = _mm512_load_ps(X + row_index[k + 47] * M_ROW + i + 16);
                __m512 pos241 = _mm512_load_ps(X + row_index[k + 48] * M_ROW + i + 16);
                __m512 neg241 = _mm512_load_ps(X + row_index[k + 49] * M_ROW + i + 16);
                __m512 pos251 = _mm512_load_ps(X + row_index[k + 50] * M_ROW + i + 16);
                __m512 neg251 = _mm512_load_ps(X + row_index[k + 51] * M_ROW + i + 16);
                __m512 pos261 = _mm512_load_ps(X + row_index[k + 52] * M_ROW + i + 16);
                __m512 neg261 = _mm512_load_ps(X + row_index[k + 53] * M_ROW + i + 16);
                __m512 pos271 = _mm512_load_ps(X + row_index[k + 54] * M_ROW + i + 16);
                __m512 neg271 = _mm512_load_ps(X + row_index[k + 55] * M_ROW + i + 16);
                __m512 pos281 = _mm512_load_ps(X + row_index[k + 56] * M_ROW + i + 16);
                __m512 neg281 = _mm512_load_ps(X + row_index[k + 57] * M_ROW + i + 16);
                __m512 pos291 = _mm512_load_ps(X + row_index[k + 58] * M_ROW + i + 16);
                __m512 neg291 = _mm512_load_ps(X + row_index[k + 59] * M_ROW + i + 16);
                __m512 pos301 = _mm512_load_ps(X + row_index[k + 60] * M_ROW + i + 16);
                __m512 neg301 = _mm512_load_ps(X + row_index[k + 61] * M_ROW + i + 16);
                __m512 pos311 = _mm512_load_ps(X + row_index[k + 62] * M_ROW + i + 16);
                __m512 neg311 = _mm512_load_ps(X + row_index[k + 63] * M_ROW + i + 16);
                res00 = _mm512_add_ps(res00, _mm512_sub_ps(pos00, neg00));
                res10 = _mm512_add_ps(res10, _mm512_sub_ps(pos10, neg10));
                res20 = _mm512_add_ps(res20, _mm512_sub_ps(pos20, neg20));
                res30 = _mm512_add_ps(res30, _mm512_sub_ps(pos30, neg30));
                res40 = _mm512_add_ps(res40, _mm512_sub_ps(pos40, neg40));
                res50 = _mm512_add_ps(res50, _mm512_sub_ps(pos50, neg50));
                res60 = _mm512_add_ps(res60, _mm512_sub_ps(pos60, neg60));
                res70 = _mm512_add_ps(res70, _mm512_sub_ps(pos70, neg70));
                res80 = _mm512_add_ps(res80, _mm512_sub_ps(pos80, neg80));
                res90 = _mm512_add_ps(res90, _mm512_sub_ps(pos90, neg90));
                res100 = _mm512_add_ps(res100, _mm512_sub_ps(pos100, neg100));
                res110 = _mm512_add_ps(res110, _mm512_sub_ps(pos110, neg110));
                res120 = _mm512_add_ps(res120, _mm512_sub_ps(pos120, neg120));
                res130 = _mm512_add_ps(res130, _mm512_sub_ps(pos130, neg130));
                res140 = _mm512_add_ps(res140, _mm512_sub_ps(pos140, neg140));
                res150 = _mm512_add_ps(res150, _mm512_sub_ps(pos150, neg150));
                res160 = _mm512_add_ps(res160, _mm512_sub_ps(pos160, neg160));
                res170 = _mm512_add_ps(res170, _mm512_sub_ps(pos170, neg170));
                res180 = _mm512_add_ps(res180, _mm512_sub_ps(pos180, neg180));
                res190 = _mm512_add_ps(res190, _mm512_sub_ps(pos190, neg190));
                res200 = _mm512_add_ps(res200, _mm512_sub_ps(pos200, neg200));
                res210 = _mm512_add_ps(res210, _mm512_sub_ps(pos210, neg210));
                res220 = _mm512_add_ps(res220, _mm512_sub_ps(pos220, neg220));
                res230 = _mm512_add_ps(res230, _mm512_sub_ps(pos230, neg230));
                res240 = _mm512_add_ps(res240, _mm512_sub_ps(pos240, neg240));
                res250 = _mm512_add_ps(res250, _mm512_sub_ps(pos250, neg250));
                res260 = _mm512_add_ps(res260, _mm512_sub_ps(pos260, neg260));
                res270 = _mm512_add_ps(res270, _mm512_sub_ps(pos270, neg270));
                res280 = _mm512_add_ps(res280, _mm512_sub_ps(pos280, neg280));
                res290 = _mm512_add_ps(res290, _mm512_sub_ps(pos290, neg290));
                res300 = _mm512_add_ps(res300, _mm512_sub_ps(pos300, neg300));
                res310 = _mm512_add_ps(res310, _mm512_sub_ps(pos310, neg310));
                res01 = _mm512_add_ps(res01, _mm512_sub_ps(pos01, neg01));
                res11 = _mm512_add_ps(res11, _mm512_sub_ps(pos11, neg11));
                res21 = _mm512_add_ps(res21, _mm512_sub_ps(pos21, neg21));
                res31 = _mm512_add_ps(res31, _mm512_sub_ps(pos31, neg31));
                res41 = _mm512_add_ps(res41, _mm512_sub_ps(pos41, neg41));
                res51 = _mm512_add_ps(res51, _mm512_sub_ps(pos51, neg51));
                res61 = _mm512_add_ps(res61, _mm512_sub_ps(pos61, neg61));
                res71 = _mm512_add_ps(res71, _mm512_sub_ps(pos71, neg71));
                res81 = _mm512_add_ps(res81, _mm512_sub_ps(pos81, neg81));
                res91 = _mm512_add_ps(res91, _mm512_sub_ps(pos91, neg91));
                res101 = _mm512_add_ps(res101, _mm512_sub_ps(pos101, neg101));
                res111 = _mm512_add_ps(res111, _mm512_sub_ps(pos111, neg111));
                res121 = _mm512_add_ps(res121, _mm512_sub_ps(pos121, neg121));
                res131 = _mm512_add_ps(res131, _mm512_sub_ps(pos131, neg131));
                res141 = _mm512_add_ps(res141, _mm512_sub_ps(pos141, neg141));
                res151 = _mm512_add_ps(res151, _mm512_sub_ps(pos151, neg151));
                res161 = _mm512_add_ps(res161, _mm512_sub_ps(pos161, neg161));
                res171 = _mm512_add_ps(res171, _mm512_sub_ps(pos171, neg171));
                res181 = _mm512_add_ps(res181, _mm512_sub_ps(pos181, neg181));
                res191 = _mm512_add_ps(res191, _mm512_sub_ps(pos191, neg191));
                res201 = _mm512_add_ps(res201, _mm512_sub_ps(pos201, neg201));
                res211 = _mm512_add_ps(res211, _mm512_sub_ps(pos211, neg211));
                res221 = _mm512_add_ps(res221, _mm512_sub_ps(pos221, neg221));
                res231 = _mm512_add_ps(res231, _mm512_sub_ps(pos231, neg231));
                res241 = _mm512_add_ps(res241, _mm512_sub_ps(pos241, neg241));
                res251 = _mm512_add_ps(res251, _mm512_sub_ps(pos251, neg251));
                res261 = _mm512_add_ps(res261, _mm512_sub_ps(pos261, neg261));
                res271 = _mm512_add_ps(res271, _mm512_sub_ps(pos271, neg271));
                res281 = _mm512_add_ps(res281, _mm512_sub_ps(pos281, neg281));
                res291 = _mm512_add_ps(res291, _mm512_sub_ps(pos291, neg291));
                res301 = _mm512_add_ps(res301, _mm512_sub_ps(pos301, neg301));
                res311 = _mm512_add_ps(res311, _mm512_sub_ps(pos311, neg311));
            }
            for (int k = groupData[1]; k < groupData[2]; k++) {
                res00 = _mm512_add_ps(res00, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res01 = _mm512_add_ps(res01, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[2]; k < groupData[3]; k++) {
                res00 = _mm512_sub_ps(res00, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res01 = _mm512_sub_ps(res01, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[3]; k < groupData[4]; k++) {
                res10 = _mm512_add_ps(res10, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res11 = _mm512_add_ps(res11, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[4]; k < groupData[5]; k++) {
                res10 = _mm512_sub_ps(res10, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res11 = _mm512_sub_ps(res11, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[5]; k < groupData[6]; k++) {
                res20 = _mm512_add_ps(res20, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res21 = _mm512_add_ps(res21, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[6]; k < groupData[7]; k++) {
                res20 = _mm512_sub_ps(res20, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res21 = _mm512_sub_ps(res21, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[7]; k < groupData[8]; k++) {
                res30 = _mm512_add_ps(res30, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res31 = _mm512_add_ps(res31, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[8]; k < groupData[9]; k++) {
                res30 = _mm512_sub_ps(res30, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res31 = _mm512_sub_ps(res31, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[9]; k < groupData[10]; k++) {
                res40 = _mm512_add_ps(res40, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res41 = _mm512_add_ps(res41, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[10]; k < groupData[11]; k++) {
                res40 = _mm512_sub_ps(res40, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res41 = _mm512_sub_ps(res41, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[11]; k < groupData[12]; k++) {
                res50 = _mm512_add_ps(res50, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res51 = _mm512_add_ps(res51, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[12]; k < groupData[13]; k++) {
                res50 = _mm512_sub_ps(res50, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res51 = _mm512_sub_ps(res51, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[13]; k < groupData[14]; k++) {
                res60 = _mm512_add_ps(res60, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res61 = _mm512_add_ps(res61, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[14]; k < groupData[15]; k++) {
                res60 = _mm512_sub_ps(res60, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res61 = _mm512_sub_ps(res61, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[15]; k < groupData[16]; k++) {
                res70 = _mm512_add_ps(res70, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res71 = _mm512_add_ps(res71, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[16]; k < groupData[17]; k++) {
                res70 = _mm512_sub_ps(res70, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res71 = _mm512_sub_ps(res71, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[17]; k < groupData[18]; k++) {
                res80 = _mm512_add_ps(res80, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res81 = _mm512_add_ps(res81, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[18]; k < groupData[19]; k++) {
                res80 = _mm512_sub_ps(res80, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res81 = _mm512_sub_ps(res81, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[19]; k < groupData[20]; k++) {
                res90 = _mm512_add_ps(res90, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res91 = _mm512_add_ps(res91, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[20]; k < groupData[21]; k++) {
                res90 = _mm512_sub_ps(res90, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res91 = _mm512_sub_ps(res91, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[21]; k < groupData[22]; k++) {
                res100 = _mm512_add_ps(res100, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res101 = _mm512_add_ps(res101, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[22]; k < groupData[23]; k++) {
                res100 = _mm512_sub_ps(res100, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res101 = _mm512_sub_ps(res101, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[23]; k < groupData[24]; k++) {
                res110 = _mm512_add_ps(res110, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res111 = _mm512_add_ps(res111, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[24]; k < groupData[25]; k++) {
                res110 = _mm512_sub_ps(res110, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res111 = _mm512_sub_ps(res111, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[25]; k < groupData[26]; k++) {
                res120 = _mm512_add_ps(res120, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res121 = _mm512_add_ps(res121, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[26]; k < groupData[27]; k++) {
                res120 = _mm512_sub_ps(res120, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res121 = _mm512_sub_ps(res121, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[27]; k < groupData[28]; k++) {
                res130 = _mm512_add_ps(res130, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res131 = _mm512_add_ps(res131, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[28]; k < groupData[29]; k++) {
                res130 = _mm512_sub_ps(res130, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res131 = _mm512_sub_ps(res131, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[29]; k < groupData[30]; k++) {
                res140 = _mm512_add_ps(res140, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res141 = _mm512_add_ps(res141, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[30]; k < groupData[31]; k++) {
                res140 = _mm512_sub_ps(res140, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res141 = _mm512_sub_ps(res141, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[31]; k < groupData[32]; k++) {
                res150 = _mm512_add_ps(res150, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res151 = _mm512_add_ps(res151, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[32]; k < groupData[33]; k++) {
                res150 = _mm512_sub_ps(res150, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res151 = _mm512_sub_ps(res151, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[33]; k < groupData[34]; k++) {
                res160 = _mm512_add_ps(res160, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res161 = _mm512_add_ps(res161, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[34]; k < groupData[35]; k++) {
                res160 = _mm512_sub_ps(res160, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res161 = _mm512_sub_ps(res161, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[35]; k < groupData[36]; k++) {
                res170 = _mm512_add_ps(res170, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res171 = _mm512_add_ps(res171, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[36]; k < groupData[37]; k++) {
                res170 = _mm512_sub_ps(res170, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res171 = _mm512_sub_ps(res171, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[37]; k < groupData[38]; k++) {
                res180 = _mm512_add_ps(res180, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res181 = _mm512_add_ps(res181, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[38]; k < groupData[39]; k++) {
                res180 = _mm512_sub_ps(res180, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res181 = _mm512_sub_ps(res181, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[39]; k < groupData[40]; k++) {
                res190 = _mm512_add_ps(res190, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res191 = _mm512_add_ps(res191, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[40]; k < groupData[41]; k++) {
                res190 = _mm512_sub_ps(res190, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res191 = _mm512_sub_ps(res191, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[41]; k < groupData[42]; k++) {
                res200 = _mm512_add_ps(res200, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res201 = _mm512_add_ps(res201, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[42]; k < groupData[43]; k++) {
                res200 = _mm512_sub_ps(res200, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res201 = _mm512_sub_ps(res201, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[43]; k < groupData[44]; k++) {
                res210 = _mm512_add_ps(res210, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res211 = _mm512_add_ps(res211, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[44]; k < groupData[45]; k++) {
                res210 = _mm512_sub_ps(res210, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res211 = _mm512_sub_ps(res211, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[45]; k < groupData[46]; k++) {
                res220 = _mm512_add_ps(res220, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res221 = _mm512_add_ps(res221, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[46]; k < groupData[47]; k++) {
                res220 = _mm512_sub_ps(res220, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res221 = _mm512_sub_ps(res221, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[47]; k < groupData[48]; k++) {
                res230 = _mm512_add_ps(res230, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res231 = _mm512_add_ps(res231, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[48]; k < groupData[49]; k++) {
                res230 = _mm512_sub_ps(res230, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res231 = _mm512_sub_ps(res231, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[49]; k < groupData[50]; k++) {
                res240 = _mm512_add_ps(res240, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res241 = _mm512_add_ps(res241, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[50]; k < groupData[51]; k++) {
                res240 = _mm512_sub_ps(res240, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res241 = _mm512_sub_ps(res241, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[51]; k < groupData[52]; k++) {
                res250 = _mm512_add_ps(res250, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res251 = _mm512_add_ps(res251, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[52]; k < groupData[53]; k++) {
                res250 = _mm512_sub_ps(res250, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res251 = _mm512_sub_ps(res251, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[53]; k < groupData[54]; k++) {
                res260 = _mm512_add_ps(res260, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res261 = _mm512_add_ps(res261, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[54]; k < groupData[55]; k++) {
                res260 = _mm512_sub_ps(res260, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res261 = _mm512_sub_ps(res261, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[55]; k < groupData[56]; k++) {
                res270 = _mm512_add_ps(res270, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res271 = _mm512_add_ps(res271, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[56]; k < groupData[57]; k++) {
                res270 = _mm512_sub_ps(res270, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res271 = _mm512_sub_ps(res271, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[57]; k < groupData[58]; k++) {
                res280 = _mm512_add_ps(res280, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res281 = _mm512_add_ps(res281, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[58]; k < groupData[59]; k++) {
                res280 = _mm512_sub_ps(res280, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res281 = _mm512_sub_ps(res281, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[59]; k < groupData[60]; k++) {
                res290 = _mm512_add_ps(res290, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res291 = _mm512_add_ps(res291, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[60]; k < groupData[61]; k++) {
                res290 = _mm512_sub_ps(res290, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res291 = _mm512_sub_ps(res291, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[61]; k < groupData[62]; k++) {
                res300 = _mm512_add_ps(res300, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res301 = _mm512_add_ps(res301, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[62]; k < groupData[63]; k++) {
                res300 = _mm512_sub_ps(res300, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res301 = _mm512_sub_ps(res301, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[63]; k < groupData[64]; k++) {
                res310 = _mm512_add_ps(res310, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res311 = _mm512_add_ps(res311, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            for (int k = groupData[64]; k < groupData[65]; k++) {
                res310 = _mm512_sub_ps(res310, _mm512_load_ps(X + row_index[k] * M_ROW + i + 0));
                res311 = _mm512_sub_ps(res311, _mm512_load_ps(X + row_index[k] * M_ROW + i + 16));
            }
            _mm512_store_ps(result + (j * 32 + 0) * M_ROW + i + 0, res00);
            _mm512_store_ps(result + (j * 32 + 1) * M_ROW + i + 0, res10);
            _mm512_store_ps(result + (j * 32 + 2) * M_ROW + i + 0, res20);
            _mm512_store_ps(result + (j * 32 + 3) * M_ROW + i + 0, res30);
            _mm512_store_ps(result + (j * 32 + 4) * M_ROW + i + 0, res40);
            _mm512_store_ps(result + (j * 32 + 5) * M_ROW + i + 0, res50);
            _mm512_store_ps(result + (j * 32 + 6) * M_ROW + i + 0, res60);
            _mm512_store_ps(result + (j * 32 + 7) * M_ROW + i + 0, res70);
            _mm512_store_ps(result + (j * 32 + 8) * M_ROW + i + 0, res80);
            _mm512_store_ps(result + (j * 32 + 9) * M_ROW + i + 0, res90);
            _mm512_store_ps(result + (j * 32 + 10) * M_ROW + i + 0, res100);
            _mm512_store_ps(result + (j * 32 + 11) * M_ROW + i + 0, res110);
            _mm512_store_ps(result + (j * 32 + 12) * M_ROW + i + 0, res120);
            _mm512_store_ps(result + (j * 32 + 13) * M_ROW + i + 0, res130);
            _mm512_store_ps(result + (j * 32 + 14) * M_ROW + i + 0, res140);
            _mm512_store_ps(result + (j * 32 + 15) * M_ROW + i + 0, res150);
            _mm512_store_ps(result + (j * 32 + 16) * M_ROW + i + 0, res160);
            _mm512_store_ps(result + (j * 32 + 17) * M_ROW + i + 0, res170);
            _mm512_store_ps(result + (j * 32 + 18) * M_ROW + i + 0, res180);
            _mm512_store_ps(result + (j * 32 + 19) * M_ROW + i + 0, res190);
            _mm512_store_ps(result + (j * 32 + 20) * M_ROW + i + 0, res200);
            _mm512_store_ps(result + (j * 32 + 21) * M_ROW + i + 0, res210);
            _mm512_store_ps(result + (j * 32 + 22) * M_ROW + i + 0, res220);
            _mm512_store_ps(result + (j * 32 + 23) * M_ROW + i + 0, res230);
            _mm512_store_ps(result + (j * 32 + 24) * M_ROW + i + 0, res240);
            _mm512_store_ps(result + (j * 32 + 25) * M_ROW + i + 0, res250);
            _mm512_store_ps(result + (j * 32 + 26) * M_ROW + i + 0, res260);
            _mm512_store_ps(result + (j * 32 + 27) * M_ROW + i + 0, res270);
            _mm512_store_ps(result + (j * 32 + 28) * M_ROW + i + 0, res280);
            _mm512_store_ps(result + (j * 32 + 29) * M_ROW + i + 0, res290);
            _mm512_store_ps(result + (j * 32 + 30) * M_ROW + i + 0, res300);
            _mm512_store_ps(result + (j * 32 + 31) * M_ROW + i + 0, res310);
            _mm512_store_ps(result + (j * 32 + 0) * M_ROW + i + 16, res01);
            _mm512_store_ps(result + (j * 32 + 1) * M_ROW + i + 16, res11);
            _mm512_store_ps(result + (j * 32 + 2) * M_ROW + i + 16, res21);
            _mm512_store_ps(result + (j * 32 + 3) * M_ROW + i + 16, res31);
            _mm512_store_ps(result + (j * 32 + 4) * M_ROW + i + 16, res41);
            _mm512_store_ps(result + (j * 32 + 5) * M_ROW + i + 16, res51);
            _mm512_store_ps(result + (j * 32 + 6) * M_ROW + i + 16, res61);
            _mm512_store_ps(result + (j * 32 + 7) * M_ROW + i + 16, res71);
            _mm512_store_ps(result + (j * 32 + 8) * M_ROW + i + 16, res81);
            _mm512_store_ps(result + (j * 32 + 9) * M_ROW + i + 16, res91);
            _mm512_store_ps(result + (j * 32 + 10) * M_ROW + i + 16, res101);
            _mm512_store_ps(result + (j * 32 + 11) * M_ROW + i + 16, res111);
            _mm512_store_ps(result + (j * 32 + 12) * M_ROW + i + 16, res121);
            _mm512_store_ps(result + (j * 32 + 13) * M_ROW + i + 16, res131);
            _mm512_store_ps(result + (j * 32 + 14) * M_ROW + i + 16, res141);
            _mm512_store_ps(result + (j * 32 + 15) * M_ROW + i + 16, res151);
            _mm512_store_ps(result + (j * 32 + 16) * M_ROW + i + 16, res161);
            _mm512_store_ps(result + (j * 32 + 17) * M_ROW + i + 16, res171);
            _mm512_store_ps(result + (j * 32 + 18) * M_ROW + i + 16, res181);
            _mm512_store_ps(result + (j * 32 + 19) * M_ROW + i + 16, res191);
            _mm512_store_ps(result + (j * 32 + 20) * M_ROW + i + 16, res201);
            _mm512_store_ps(result + (j * 32 + 21) * M_ROW + i + 16, res211);
            _mm512_store_ps(result + (j * 32 + 22) * M_ROW + i + 16, res221);
            _mm512_store_ps(result + (j * 32 + 23) * M_ROW + i + 16, res231);
            _mm512_store_ps(result + (j * 32 + 24) * M_ROW + i + 16, res241);
            _mm512_store_ps(result + (j * 32 + 25) * M_ROW + i + 16, res251);
            _mm512_store_ps(result + (j * 32 + 26) * M_ROW + i + 16, res261);
            _mm512_store_ps(result + (j * 32 + 27) * M_ROW + i + 16, res271);
            _mm512_store_ps(result + (j * 32 + 28) * M_ROW + i + 16, res281);
            _mm512_store_ps(result + (j * 32 + 29) * M_ROW + i + 16, res291);
            _mm512_store_ps(result + (j * 32 + 30) * M_ROW + i + 16, res301);
            _mm512_store_ps(result + (j * 32 + 31) * M_ROW + i + 16, res311);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Uniform_16xG1_AVX512_OpenMP(const float* X, const int32_t NonZeroPerCol, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 1; j++) {
        for (int i = 0; i < M_ROW; i += 16) {
            __m512 res00 = _mm512_setzero_ps();
            for (int k = j * 1 * NonZeroPerCol; k < (j + 1) * 1 * NonZeroPerCol; k += 2) {
                __m512 pos00 = _mm512_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m512 neg00 = _mm512_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                res00 = _mm512_add_ps(res00, _mm512_sub_ps(pos00, neg00));
            }
            _mm512_store_ps(result + (j * 1 + 0) * M_ROW + i + 0, res00);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Uniform_16xG4_AVX512_OpenMP(const float* X, const int32_t NonZeroPerCol, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 4; j++) {
        for (int i = 0; i < M_ROW; i += 16) {
            __m512 res00 = _mm512_setzero_ps();
            __m512 res10 = _mm512_setzero_ps();
            __m512 res20 = _mm512_setzero_ps();
            __m512 res30 = _mm512_setzero_ps();
            for (int k = j * 4 * NonZeroPerCol; k < (j + 1) * 4 * NonZeroPerCol; k += 8) {
                __m512 pos00 = _mm512_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m512 neg00 = _mm512_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m512 pos10 = _mm512_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m512 neg10 = _mm512_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m512 pos20 = _mm512_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m512 neg20 = _mm512_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m512 pos30 = _mm512_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m512 neg30 = _mm512_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                res00 = _mm512_add_ps(res00, _mm512_sub_ps(pos00, neg00));
                res10 = _mm512_add_ps(res10, _mm512_sub_ps(pos10, neg10));
                res20 = _mm512_add_ps(res20, _mm512_sub_ps(pos20, neg20));
                res30 = _mm512_add_ps(res30, _mm512_sub_ps(pos30, neg30));
            }
            _mm512_store_ps(result + (j * 4 + 0) * M_ROW + i + 0, res00);
            _mm512_store_ps(result + (j * 4 + 1) * M_ROW + i + 0, res10);
            _mm512_store_ps(result + (j * 4 + 2) * M_ROW + i + 0, res20);
            _mm512_store_ps(result + (j * 4 + 3) * M_ROW + i + 0, res30);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Uniform_16xG8_AVX512_OpenMP(const float* X, const int32_t NonZeroPerCol, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 8; j++) {
        for (int i = 0; i < M_ROW; i += 16) {
            __m512 res00 = _mm512_setzero_ps();
            __m512 res10 = _mm512_setzero_ps();
            __m512 res20 = _mm512_setzero_ps();
            __m512 res30 = _mm512_setzero_ps();
            __m512 res40 = _mm512_setzero_ps();
            __m512 res50 = _mm512_setzero_ps();
            __m512 res60 = _mm512_setzero_ps();
            __m512 res70 = _mm512_setzero_ps();
            for (int k = j * 8 * NonZeroPerCol; k < (j + 1) * 8 * NonZeroPerCol; k += 16) {
                __m512 pos00 = _mm512_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m512 neg00 = _mm512_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m512 pos10 = _mm512_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m512 neg10 = _mm512_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m512 pos20 = _mm512_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m512 neg20 = _mm512_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m512 pos30 = _mm512_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m512 neg30 = _mm512_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m512 pos40 = _mm512_load_ps(X + row_index[k + 8] * M_ROW + i + 0);
                __m512 neg40 = _mm512_load_ps(X + row_index[k + 9] * M_ROW + i + 0);
                __m512 pos50 = _mm512_load_ps(X + row_index[k + 10] * M_ROW + i + 0);
                __m512 neg50 = _mm512_load_ps(X + row_index[k + 11] * M_ROW + i + 0);
                __m512 pos60 = _mm512_load_ps(X + row_index[k + 12] * M_ROW + i + 0);
                __m512 neg60 = _mm512_load_ps(X + row_index[k + 13] * M_ROW + i + 0);
                __m512 pos70 = _mm512_load_ps(X + row_index[k + 14] * M_ROW + i + 0);
                __m512 neg70 = _mm512_load_ps(X + row_index[k + 15] * M_ROW + i + 0);
                res00 = _mm512_add_ps(res00, _mm512_sub_ps(pos00, neg00));
                res10 = _mm512_add_ps(res10, _mm512_sub_ps(pos10, neg10));
                res20 = _mm512_add_ps(res20, _mm512_sub_ps(pos20, neg20));
                res30 = _mm512_add_ps(res30, _mm512_sub_ps(pos30, neg30));
                res40 = _mm512_add_ps(res40, _mm512_sub_ps(pos40, neg40));
                res50 = _mm512_add_ps(res50, _mm512_sub_ps(pos50, neg50));
                res60 = _mm512_add_ps(res60, _mm512_sub_ps(pos60, neg60));
                res70 = _mm512_add_ps(res70, _mm512_sub_ps(pos70, neg70));
            }
            _mm512_store_ps(result + (j * 8 + 0) * M_ROW + i + 0, res00);
            _mm512_store_ps(result + (j * 8 + 1) * M_ROW + i + 0, res10);
            _mm512_store_ps(result + (j * 8 + 2) * M_ROW + i + 0, res20);
            _mm512_store_ps(result + (j * 8 + 3) * M_ROW + i + 0, res30);
            _mm512_store_ps(result + (j * 8 + 4) * M_ROW + i + 0, res40);
            _mm512_store_ps(result + (j * 8 + 5) * M_ROW + i + 0, res50);
            _mm512_store_ps(result + (j * 8 + 6) * M_ROW + i + 0, res60);
            _mm512_store_ps(result + (j * 8 + 7) * M_ROW + i + 0, res70);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Uniform_16xG16_AVX512_OpenMP(const float* X, const int32_t NonZeroPerCol, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 16; j++) {
        for (int i = 0; i < M_ROW; i += 16) {
            __m512 res00 = _mm512_setzero_ps();
            __m512 res10 = _mm512_setzero_ps();
            __m512 res20 = _mm512_setzero_ps();
            __m512 res30 = _mm512_setzero_ps();
            __m512 res40 = _mm512_setzero_ps();
            __m512 res50 = _mm512_setzero_ps();
            __m512 res60 = _mm512_setzero_ps();
            __m512 res70 = _mm512_setzero_ps();
            __m512 res80 = _mm512_setzero_ps();
            __m512 res90 = _mm512_setzero_ps();
            __m512 res100 = _mm512_setzero_ps();
            __m512 res110 = _mm512_setzero_ps();
            __m512 res120 = _mm512_setzero_ps();
            __m512 res130 = _mm512_setzero_ps();
            __m512 res140 = _mm512_setzero_ps();
            __m512 res150 = _mm512_setzero_ps();
            for (int k = j * 16 * NonZeroPerCol; k < (j + 1) * 16 * NonZeroPerCol; k += 32) {
                __m512 pos00 = _mm512_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m512 neg00 = _mm512_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m512 pos10 = _mm512_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m512 neg10 = _mm512_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m512 pos20 = _mm512_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m512 neg20 = _mm512_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m512 pos30 = _mm512_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m512 neg30 = _mm512_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m512 pos40 = _mm512_load_ps(X + row_index[k + 8] * M_ROW + i + 0);
                __m512 neg40 = _mm512_load_ps(X + row_index[k + 9] * M_ROW + i + 0);
                __m512 pos50 = _mm512_load_ps(X + row_index[k + 10] * M_ROW + i + 0);
                __m512 neg50 = _mm512_load_ps(X + row_index[k + 11] * M_ROW + i + 0);
                __m512 pos60 = _mm512_load_ps(X + row_index[k + 12] * M_ROW + i + 0);
                __m512 neg60 = _mm512_load_ps(X + row_index[k + 13] * M_ROW + i + 0);
                __m512 pos70 = _mm512_load_ps(X + row_index[k + 14] * M_ROW + i + 0);
                __m512 neg70 = _mm512_load_ps(X + row_index[k + 15] * M_ROW + i + 0);
                __m512 pos80 = _mm512_load_ps(X + row_index[k + 16] * M_ROW + i + 0);
                __m512 neg80 = _mm512_load_ps(X + row_index[k + 17] * M_ROW + i + 0);
                __m512 pos90 = _mm512_load_ps(X + row_index[k + 18] * M_ROW + i + 0);
                __m512 neg90 = _mm512_load_ps(X + row_index[k + 19] * M_ROW + i + 0);
                __m512 pos100 = _mm512_load_ps(X + row_index[k + 20] * M_ROW + i + 0);
                __m512 neg100 = _mm512_load_ps(X + row_index[k + 21] * M_ROW + i + 0);
                __m512 pos110 = _mm512_load_ps(X + row_index[k + 22] * M_ROW + i + 0);
                __m512 neg110 = _mm512_load_ps(X + row_index[k + 23] * M_ROW + i + 0);
                __m512 pos120 = _mm512_load_ps(X + row_index[k + 24] * M_ROW + i + 0);
                __m512 neg120 = _mm512_load_ps(X + row_index[k + 25] * M_ROW + i + 0);
                __m512 pos130 = _mm512_load_ps(X + row_index[k + 26] * M_ROW + i + 0);
                __m512 neg130 = _mm512_load_ps(X + row_index[k + 27] * M_ROW + i + 0);
                __m512 pos140 = _mm512_load_ps(X + row_index[k + 28] * M_ROW + i + 0);
                __m512 neg140 = _mm512_load_ps(X + row_index[k + 29] * M_ROW + i + 0);
                __m512 pos150 = _mm512_load_ps(X + row_index[k + 30] * M_ROW + i + 0);
                __m512 neg150 = _mm512_load_ps(X + row_index[k + 31] * M_ROW + i + 0);
                res00 = _mm512_add_ps(res00, _mm512_sub_ps(pos00, neg00));
                res10 = _mm512_add_ps(res10, _mm512_sub_ps(pos10, neg10));
                res20 = _mm512_add_ps(res20, _mm512_sub_ps(pos20, neg20));
                res30 = _mm512_add_ps(res30, _mm512_sub_ps(pos30, neg30));
                res40 = _mm512_add_ps(res40, _mm512_sub_ps(pos40, neg40));
                res50 = _mm512_add_ps(res50, _mm512_sub_ps(pos50, neg50));
                res60 = _mm512_add_ps(res60, _mm512_sub_ps(pos60, neg60));
                res70 = _mm512_add_ps(res70, _mm512_sub_ps(pos70, neg70));
                res80 = _mm512_add_ps(res80, _mm512_sub_ps(pos80, neg80));
                res90 = _mm512_add_ps(res90, _mm512_sub_ps(pos90, neg90));
                res100 = _mm512_add_ps(res100, _mm512_sub_ps(pos100, neg100));
                res110 = _mm512_add_ps(res110, _mm512_sub_ps(pos110, neg110));
                res120 = _mm512_add_ps(res120, _mm512_sub_ps(pos120, neg120));
                res130 = _mm512_add_ps(res130, _mm512_sub_ps(pos130, neg130));
                res140 = _mm512_add_ps(res140, _mm512_sub_ps(pos140, neg140));
                res150 = _mm512_add_ps(res150, _mm512_sub_ps(pos150, neg150));
            }
            _mm512_store_ps(result + (j * 16 + 0) * M_ROW + i + 0, res00);
            _mm512_store_ps(result + (j * 16 + 1) * M_ROW + i + 0, res10);
            _mm512_store_ps(result + (j * 16 + 2) * M_ROW + i + 0, res20);
            _mm512_store_ps(result + (j * 16 + 3) * M_ROW + i + 0, res30);
            _mm512_store_ps(result + (j * 16 + 4) * M_ROW + i + 0, res40);
            _mm512_store_ps(result + (j * 16 + 5) * M_ROW + i + 0, res50);
            _mm512_store_ps(result + (j * 16 + 6) * M_ROW + i + 0, res60);
            _mm512_store_ps(result + (j * 16 + 7) * M_ROW + i + 0, res70);
            _mm512_store_ps(result + (j * 16 + 8) * M_ROW + i + 0, res80);
            _mm512_store_ps(result + (j * 16 + 9) * M_ROW + i + 0, res90);
            _mm512_store_ps(result + (j * 16 + 10) * M_ROW + i + 0, res100);
            _mm512_store_ps(result + (j * 16 + 11) * M_ROW + i + 0, res110);
            _mm512_store_ps(result + (j * 16 + 12) * M_ROW + i + 0, res120);
            _mm512_store_ps(result + (j * 16 + 13) * M_ROW + i + 0, res130);
            _mm512_store_ps(result + (j * 16 + 14) * M_ROW + i + 0, res140);
            _mm512_store_ps(result + (j * 16 + 15) * M_ROW + i + 0, res150);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Uniform_16xG32_AVX512_OpenMP(const float* X, const int32_t NonZeroPerCol, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 32; j++) {
        for (int i = 0; i < M_ROW; i += 16) {
            __m512 res00 = _mm512_setzero_ps();
            __m512 res10 = _mm512_setzero_ps();
            __m512 res20 = _mm512_setzero_ps();
            __m512 res30 = _mm512_setzero_ps();
            __m512 res40 = _mm512_setzero_ps();
            __m512 res50 = _mm512_setzero_ps();
            __m512 res60 = _mm512_setzero_ps();
            __m512 res70 = _mm512_setzero_ps();
            __m512 res80 = _mm512_setzero_ps();
            __m512 res90 = _mm512_setzero_ps();
            __m512 res100 = _mm512_setzero_ps();
            __m512 res110 = _mm512_setzero_ps();
            __m512 res120 = _mm512_setzero_ps();
            __m512 res130 = _mm512_setzero_ps();
            __m512 res140 = _mm512_setzero_ps();
            __m512 res150 = _mm512_setzero_ps();
            __m512 res160 = _mm512_setzero_ps();
            __m512 res170 = _mm512_setzero_ps();
            __m512 res180 = _mm512_setzero_ps();
            __m512 res190 = _mm512_setzero_ps();
            __m512 res200 = _mm512_setzero_ps();
            __m512 res210 = _mm512_setzero_ps();
            __m512 res220 = _mm512_setzero_ps();
            __m512 res230 = _mm512_setzero_ps();
            __m512 res240 = _mm512_setzero_ps();
            __m512 res250 = _mm512_setzero_ps();
            __m512 res260 = _mm512_setzero_ps();
            __m512 res270 = _mm512_setzero_ps();
            __m512 res280 = _mm512_setzero_ps();
            __m512 res290 = _mm512_setzero_ps();
            __m512 res300 = _mm512_setzero_ps();
            __m512 res310 = _mm512_setzero_ps();
            for (int k = j * 32 * NonZeroPerCol; k < (j + 1) * 32 * NonZeroPerCol; k += 64) {
                __m512 pos00 = _mm512_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m512 neg00 = _mm512_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m512 pos10 = _mm512_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m512 neg10 = _mm512_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m512 pos20 = _mm512_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m512 neg20 = _mm512_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m512 pos30 = _mm512_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m512 neg30 = _mm512_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m512 pos40 = _mm512_load_ps(X + row_index[k + 8] * M_ROW + i + 0);
                __m512 neg40 = _mm512_load_ps(X + row_index[k + 9] * M_ROW + i + 0);
                __m512 pos50 = _mm512_load_ps(X + row_index[k + 10] * M_ROW + i + 0);
                __m512 neg50 = _mm512_load_ps(X + row_index[k + 11] * M_ROW + i + 0);
                __m512 pos60 = _mm512_load_ps(X + row_index[k + 12] * M_ROW + i + 0);
                __m512 neg60 = _mm512_load_ps(X + row_index[k + 13] * M_ROW + i + 0);
                __m512 pos70 = _mm512_load_ps(X + row_index[k + 14] * M_ROW + i + 0);
                __m512 neg70 = _mm512_load_ps(X + row_index[k + 15] * M_ROW + i + 0);
                __m512 pos80 = _mm512_load_ps(X + row_index[k + 16] * M_ROW + i + 0);
                __m512 neg80 = _mm512_load_ps(X + row_index[k + 17] * M_ROW + i + 0);
                __m512 pos90 = _mm512_load_ps(X + row_index[k + 18] * M_ROW + i + 0);
                __m512 neg90 = _mm512_load_ps(X + row_index[k + 19] * M_ROW + i + 0);
                __m512 pos100 = _mm512_load_ps(X + row_index[k + 20] * M_ROW + i + 0);
                __m512 neg100 = _mm512_load_ps(X + row_index[k + 21] * M_ROW + i + 0);
                __m512 pos110 = _mm512_load_ps(X + row_index[k + 22] * M_ROW + i + 0);
                __m512 neg110 = _mm512_load_ps(X + row_index[k + 23] * M_ROW + i + 0);
                __m512 pos120 = _mm512_load_ps(X + row_index[k + 24] * M_ROW + i + 0);
                __m512 neg120 = _mm512_load_ps(X + row_index[k + 25] * M_ROW + i + 0);
                __m512 pos130 = _mm512_load_ps(X + row_index[k + 26] * M_ROW + i + 0);
                __m512 neg130 = _mm512_load_ps(X + row_index[k + 27] * M_ROW + i + 0);
                __m512 pos140 = _mm512_load_ps(X + row_index[k + 28] * M_ROW + i + 0);
                __m512 neg140 = _mm512_load_ps(X + row_index[k + 29] * M_ROW + i + 0);
                __m512 pos150 = _mm512_load_ps(X + row_index[k + 30] * M_ROW + i + 0);
                __m512 neg150 = _mm512_load_ps(X + row_index[k + 31] * M_ROW + i + 0);
                __m512 pos160 = _mm512_load_ps(X + row_index[k + 32] * M_ROW + i + 0);
                __m512 neg160 = _mm512_load_ps(X + row_index[k + 33] * M_ROW + i + 0);
                __m512 pos170 = _mm512_load_ps(X + row_index[k + 34] * M_ROW + i + 0);
                __m512 neg170 = _mm512_load_ps(X + row_index[k + 35] * M_ROW + i + 0);
                __m512 pos180 = _mm512_load_ps(X + row_index[k + 36] * M_ROW + i + 0);
                __m512 neg180 = _mm512_load_ps(X + row_index[k + 37] * M_ROW + i + 0);
                __m512 pos190 = _mm512_load_ps(X + row_index[k + 38] * M_ROW + i + 0);
                __m512 neg190 = _mm512_load_ps(X + row_index[k + 39] * M_ROW + i + 0);
                __m512 pos200 = _mm512_load_ps(X + row_index[k + 40] * M_ROW + i + 0);
                __m512 neg200 = _mm512_load_ps(X + row_index[k + 41] * M_ROW + i + 0);
                __m512 pos210 = _mm512_load_ps(X + row_index[k + 42] * M_ROW + i + 0);
                __m512 neg210 = _mm512_load_ps(X + row_index[k + 43] * M_ROW + i + 0);
                __m512 pos220 = _mm512_load_ps(X + row_index[k + 44] * M_ROW + i + 0);
                __m512 neg220 = _mm512_load_ps(X + row_index[k + 45] * M_ROW + i + 0);
                __m512 pos230 = _mm512_load_ps(X + row_index[k + 46] * M_ROW + i + 0);
                __m512 neg230 = _mm512_load_ps(X + row_index[k + 47] * M_ROW + i + 0);
                __m512 pos240 = _mm512_load_ps(X + row_index[k + 48] * M_ROW + i + 0);
                __m512 neg240 = _mm512_load_ps(X + row_index[k + 49] * M_ROW + i + 0);
                __m512 pos250 = _mm512_load_ps(X + row_index[k + 50] * M_ROW + i + 0);
                __m512 neg250 = _mm512_load_ps(X + row_index[k + 51] * M_ROW + i + 0);
                __m512 pos260 = _mm512_load_ps(X + row_index[k + 52] * M_ROW + i + 0);
                __m512 neg260 = _mm512_load_ps(X + row_index[k + 53] * M_ROW + i + 0);
                __m512 pos270 = _mm512_load_ps(X + row_index[k + 54] * M_ROW + i + 0);
                __m512 neg270 = _mm512_load_ps(X + row_index[k + 55] * M_ROW + i + 0);
                __m512 pos280 = _mm512_load_ps(X + row_index[k + 56] * M_ROW + i + 0);
                __m512 neg280 = _mm512_load_ps(X + row_index[k + 57] * M_ROW + i + 0);
                __m512 pos290 = _mm512_load_ps(X + row_index[k + 58] * M_ROW + i + 0);
                __m512 neg290 = _mm512_load_ps(X + row_index[k + 59] * M_ROW + i + 0);
                __m512 pos300 = _mm512_load_ps(X + row_index[k + 60] * M_ROW + i + 0);
                __m512 neg300 = _mm512_load_ps(X + row_index[k + 61] * M_ROW + i + 0);
                __m512 pos310 = _mm512_load_ps(X + row_index[k + 62] * M_ROW + i + 0);
                __m512 neg310 = _mm512_load_ps(X + row_index[k + 63] * M_ROW + i + 0);
                res00 = _mm512_add_ps(res00, _mm512_sub_ps(pos00, neg00));
                res10 = _mm512_add_ps(res10, _mm512_sub_ps(pos10, neg10));
                res20 = _mm512_add_ps(res20, _mm512_sub_ps(pos20, neg20));
                res30 = _mm512_add_ps(res30, _mm512_sub_ps(pos30, neg30));
                res40 = _mm512_add_ps(res40, _mm512_sub_ps(pos40, neg40));
                res50 = _mm512_add_ps(res50, _mm512_sub_ps(pos50, neg50));
                res60 = _mm512_add_ps(res60, _mm512_sub_ps(pos60, neg60));
                res70 = _mm512_add_ps(res70, _mm512_sub_ps(pos70, neg70));
                res80 = _mm512_add_ps(res80, _mm512_sub_ps(pos80, neg80));
                res90 = _mm512_add_ps(res90, _mm512_sub_ps(pos90, neg90));
                res100 = _mm512_add_ps(res100, _mm512_sub_ps(pos100, neg100));
                res110 = _mm512_add_ps(res110, _mm512_sub_ps(pos110, neg110));
                res120 = _mm512_add_ps(res120, _mm512_sub_ps(pos120, neg120));
                res130 = _mm512_add_ps(res130, _mm512_sub_ps(pos130, neg130));
                res140 = _mm512_add_ps(res140, _mm512_sub_ps(pos140, neg140));
                res150 = _mm512_add_ps(res150, _mm512_sub_ps(pos150, neg150));
                res160 = _mm512_add_ps(res160, _mm512_sub_ps(pos160, neg160));
                res170 = _mm512_add_ps(res170, _mm512_sub_ps(pos170, neg170));
                res180 = _mm512_add_ps(res180, _mm512_sub_ps(pos180, neg180));
                res190 = _mm512_add_ps(res190, _mm512_sub_ps(pos190, neg190));
                res200 = _mm512_add_ps(res200, _mm512_sub_ps(pos200, neg200));
                res210 = _mm512_add_ps(res210, _mm512_sub_ps(pos210, neg210));
                res220 = _mm512_add_ps(res220, _mm512_sub_ps(pos220, neg220));
                res230 = _mm512_add_ps(res230, _mm512_sub_ps(pos230, neg230));
                res240 = _mm512_add_ps(res240, _mm512_sub_ps(pos240, neg240));
                res250 = _mm512_add_ps(res250, _mm512_sub_ps(pos250, neg250));
                res260 = _mm512_add_ps(res260, _mm512_sub_ps(pos260, neg260));
                res270 = _mm512_add_ps(res270, _mm512_sub_ps(pos270, neg270));
                res280 = _mm512_add_ps(res280, _mm512_sub_ps(pos280, neg280));
                res290 = _mm512_add_ps(res290, _mm512_sub_ps(pos290, neg290));
                res300 = _mm512_add_ps(res300, _mm512_sub_ps(pos300, neg300));
                res310 = _mm512_add_ps(res310, _mm512_sub_ps(pos310, neg310));
            }
            _mm512_store_ps(result + (j * 32 + 0) * M_ROW + i + 0, res00);
            _mm512_store_ps(result + (j * 32 + 1) * M_ROW + i + 0, res10);
            _mm512_store_ps(result + (j * 32 + 2) * M_ROW + i + 0, res20);
            _mm512_store_ps(result + (j * 32 + 3) * M_ROW + i + 0, res30);
            _mm512_store_ps(result + (j * 32 + 4) * M_ROW + i + 0, res40);
            _mm512_store_ps(result + (j * 32 + 5) * M_ROW + i + 0, res50);
            _mm512_store_ps(result + (j * 32 + 6) * M_ROW + i + 0, res60);
            _mm512_store_ps(result + (j * 32 + 7) * M_ROW + i + 0, res70);
            _mm512_store_ps(result + (j * 32 + 8) * M_ROW + i + 0, res80);
            _mm512_store_ps(result + (j * 32 + 9) * M_ROW + i + 0, res90);
            _mm512_store_ps(result + (j * 32 + 10) * M_ROW + i + 0, res100);
            _mm512_store_ps(result + (j * 32 + 11) * M_ROW + i + 0, res110);
            _mm512_store_ps(result + (j * 32 + 12) * M_ROW + i + 0, res120);
            _mm512_store_ps(result + (j * 32 + 13) * M_ROW + i + 0, res130);
            _mm512_store_ps(result + (j * 32 + 14) * M_ROW + i + 0, res140);
            _mm512_store_ps(result + (j * 32 + 15) * M_ROW + i + 0, res150);
            _mm512_store_ps(result + (j * 32 + 16) * M_ROW + i + 0, res160);
            _mm512_store_ps(result + (j * 32 + 17) * M_ROW + i + 0, res170);
            _mm512_store_ps(result + (j * 32 + 18) * M_ROW + i + 0, res180);
            _mm512_store_ps(result + (j * 32 + 19) * M_ROW + i + 0, res190);
            _mm512_store_ps(result + (j * 32 + 20) * M_ROW + i + 0, res200);
            _mm512_store_ps(result + (j * 32 + 21) * M_ROW + i + 0, res210);
            _mm512_store_ps(result + (j * 32 + 22) * M_ROW + i + 0, res220);
            _mm512_store_ps(result + (j * 32 + 23) * M_ROW + i + 0, res230);
            _mm512_store_ps(result + (j * 32 + 24) * M_ROW + i + 0, res240);
            _mm512_store_ps(result + (j * 32 + 25) * M_ROW + i + 0, res250);
            _mm512_store_ps(result + (j * 32 + 26) * M_ROW + i + 0, res260);
            _mm512_store_ps(result + (j * 32 + 27) * M_ROW + i + 0, res270);
            _mm512_store_ps(result + (j * 32 + 28) * M_ROW + i + 0, res280);
            _mm512_store_ps(result + (j * 32 + 29) * M_ROW + i + 0, res290);
            _mm512_store_ps(result + (j * 32 + 30) * M_ROW + i + 0, res300);
            _mm512_store_ps(result + (j * 32 + 31) * M_ROW + i + 0, res310);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Uniform_32xG1_AVX512_OpenMP(const float* X, const int32_t NonZeroPerCol, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 1; j++) {
        for (int i = 0; i < M_ROW; i += 32) {
            __m512 res00 = _mm512_setzero_ps();
            __m512 res01 = _mm512_setzero_ps();
            for (int k = j * 1 * NonZeroPerCol; k < (j + 1) * 1 * NonZeroPerCol; k += 2) {
                __m512 pos00 = _mm512_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m512 neg00 = _mm512_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m512 pos01 = _mm512_load_ps(X + row_index[k + 0] * M_ROW + i + 16);
                __m512 neg01 = _mm512_load_ps(X + row_index[k + 1] * M_ROW + i + 16);
                res00 = _mm512_add_ps(res00, _mm512_sub_ps(pos00, neg00));
                res01 = _mm512_add_ps(res01, _mm512_sub_ps(pos01, neg01));
            }
            _mm512_store_ps(result + (j * 1 + 0) * M_ROW + i + 0, res00);
            _mm512_store_ps(result + (j * 1 + 0) * M_ROW + i + 16, res01);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Uniform_32xG4_AVX512_OpenMP(const float* X, const int32_t NonZeroPerCol, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 4; j++) {
        for (int i = 0; i < M_ROW; i += 32) {
            __m512 res00 = _mm512_setzero_ps();
            __m512 res01 = _mm512_setzero_ps();
            __m512 res10 = _mm512_setzero_ps();
            __m512 res11 = _mm512_setzero_ps();
            __m512 res20 = _mm512_setzero_ps();
            __m512 res21 = _mm512_setzero_ps();
            __m512 res30 = _mm512_setzero_ps();
            __m512 res31 = _mm512_setzero_ps();
            for (int k = j * 4 * NonZeroPerCol; k < (j + 1) * 4 * NonZeroPerCol; k += 8) {
                __m512 pos00 = _mm512_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m512 neg00 = _mm512_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m512 pos10 = _mm512_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m512 neg10 = _mm512_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m512 pos20 = _mm512_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m512 neg20 = _mm512_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m512 pos30 = _mm512_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m512 neg30 = _mm512_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m512 pos01 = _mm512_load_ps(X + row_index[k + 0] * M_ROW + i + 16);
                __m512 neg01 = _mm512_load_ps(X + row_index[k + 1] * M_ROW + i + 16);
                __m512 pos11 = _mm512_load_ps(X + row_index[k + 2] * M_ROW + i + 16);
                __m512 neg11 = _mm512_load_ps(X + row_index[k + 3] * M_ROW + i + 16);
                __m512 pos21 = _mm512_load_ps(X + row_index[k + 4] * M_ROW + i + 16);
                __m512 neg21 = _mm512_load_ps(X + row_index[k + 5] * M_ROW + i + 16);
                __m512 pos31 = _mm512_load_ps(X + row_index[k + 6] * M_ROW + i + 16);
                __m512 neg31 = _mm512_load_ps(X + row_index[k + 7] * M_ROW + i + 16);
                res00 = _mm512_add_ps(res00, _mm512_sub_ps(pos00, neg00));
                res10 = _mm512_add_ps(res10, _mm512_sub_ps(pos10, neg10));
                res20 = _mm512_add_ps(res20, _mm512_sub_ps(pos20, neg20));
                res30 = _mm512_add_ps(res30, _mm512_sub_ps(pos30, neg30));
                res01 = _mm512_add_ps(res01, _mm512_sub_ps(pos01, neg01));
                res11 = _mm512_add_ps(res11, _mm512_sub_ps(pos11, neg11));
                res21 = _mm512_add_ps(res21, _mm512_sub_ps(pos21, neg21));
                res31 = _mm512_add_ps(res31, _mm512_sub_ps(pos31, neg31));
            }
            _mm512_store_ps(result + (j * 4 + 0) * M_ROW + i + 0, res00);
            _mm512_store_ps(result + (j * 4 + 1) * M_ROW + i + 0, res10);
            _mm512_store_ps(result + (j * 4 + 2) * M_ROW + i + 0, res20);
            _mm512_store_ps(result + (j * 4 + 3) * M_ROW + i + 0, res30);
            _mm512_store_ps(result + (j * 4 + 0) * M_ROW + i + 16, res01);
            _mm512_store_ps(result + (j * 4 + 1) * M_ROW + i + 16, res11);
            _mm512_store_ps(result + (j * 4 + 2) * M_ROW + i + 16, res21);
            _mm512_store_ps(result + (j * 4 + 3) * M_ROW + i + 16, res31);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Uniform_32xG8_AVX512_OpenMP(const float* X, const int32_t NonZeroPerCol, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 8; j++) {
        for (int i = 0; i < M_ROW; i += 32) {
            __m512 res00 = _mm512_setzero_ps();
            __m512 res01 = _mm512_setzero_ps();
            __m512 res10 = _mm512_setzero_ps();
            __m512 res11 = _mm512_setzero_ps();
            __m512 res20 = _mm512_setzero_ps();
            __m512 res21 = _mm512_setzero_ps();
            __m512 res30 = _mm512_setzero_ps();
            __m512 res31 = _mm512_setzero_ps();
            __m512 res40 = _mm512_setzero_ps();
            __m512 res41 = _mm512_setzero_ps();
            __m512 res50 = _mm512_setzero_ps();
            __m512 res51 = _mm512_setzero_ps();
            __m512 res60 = _mm512_setzero_ps();
            __m512 res61 = _mm512_setzero_ps();
            __m512 res70 = _mm512_setzero_ps();
            __m512 res71 = _mm512_setzero_ps();
            for (int k = j * 8 * NonZeroPerCol; k < (j + 1) * 8 * NonZeroPerCol; k += 16) {
                __m512 pos00 = _mm512_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m512 neg00 = _mm512_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m512 pos10 = _mm512_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m512 neg10 = _mm512_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m512 pos20 = _mm512_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m512 neg20 = _mm512_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m512 pos30 = _mm512_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m512 neg30 = _mm512_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m512 pos40 = _mm512_load_ps(X + row_index[k + 8] * M_ROW + i + 0);
                __m512 neg40 = _mm512_load_ps(X + row_index[k + 9] * M_ROW + i + 0);
                __m512 pos50 = _mm512_load_ps(X + row_index[k + 10] * M_ROW + i + 0);
                __m512 neg50 = _mm512_load_ps(X + row_index[k + 11] * M_ROW + i + 0);
                __m512 pos60 = _mm512_load_ps(X + row_index[k + 12] * M_ROW + i + 0);
                __m512 neg60 = _mm512_load_ps(X + row_index[k + 13] * M_ROW + i + 0);
                __m512 pos70 = _mm512_load_ps(X + row_index[k + 14] * M_ROW + i + 0);
                __m512 neg70 = _mm512_load_ps(X + row_index[k + 15] * M_ROW + i + 0);
                __m512 pos01 = _mm512_load_ps(X + row_index[k + 0] * M_ROW + i + 16);
                __m512 neg01 = _mm512_load_ps(X + row_index[k + 1] * M_ROW + i + 16);
                __m512 pos11 = _mm512_load_ps(X + row_index[k + 2] * M_ROW + i + 16);
                __m512 neg11 = _mm512_load_ps(X + row_index[k + 3] * M_ROW + i + 16);
                __m512 pos21 = _mm512_load_ps(X + row_index[k + 4] * M_ROW + i + 16);
                __m512 neg21 = _mm512_load_ps(X + row_index[k + 5] * M_ROW + i + 16);
                __m512 pos31 = _mm512_load_ps(X + row_index[k + 6] * M_ROW + i + 16);
                __m512 neg31 = _mm512_load_ps(X + row_index[k + 7] * M_ROW + i + 16);
                __m512 pos41 = _mm512_load_ps(X + row_index[k + 8] * M_ROW + i + 16);
                __m512 neg41 = _mm512_load_ps(X + row_index[k + 9] * M_ROW + i + 16);
                __m512 pos51 = _mm512_load_ps(X + row_index[k + 10] * M_ROW + i + 16);
                __m512 neg51 = _mm512_load_ps(X + row_index[k + 11] * M_ROW + i + 16);
                __m512 pos61 = _mm512_load_ps(X + row_index[k + 12] * M_ROW + i + 16);
                __m512 neg61 = _mm512_load_ps(X + row_index[k + 13] * M_ROW + i + 16);
                __m512 pos71 = _mm512_load_ps(X + row_index[k + 14] * M_ROW + i + 16);
                __m512 neg71 = _mm512_load_ps(X + row_index[k + 15] * M_ROW + i + 16);
                res00 = _mm512_add_ps(res00, _mm512_sub_ps(pos00, neg00));
                res10 = _mm512_add_ps(res10, _mm512_sub_ps(pos10, neg10));
                res20 = _mm512_add_ps(res20, _mm512_sub_ps(pos20, neg20));
                res30 = _mm512_add_ps(res30, _mm512_sub_ps(pos30, neg30));
                res40 = _mm512_add_ps(res40, _mm512_sub_ps(pos40, neg40));
                res50 = _mm512_add_ps(res50, _mm512_sub_ps(pos50, neg50));
                res60 = _mm512_add_ps(res60, _mm512_sub_ps(pos60, neg60));
                res70 = _mm512_add_ps(res70, _mm512_sub_ps(pos70, neg70));
                res01 = _mm512_add_ps(res01, _mm512_sub_ps(pos01, neg01));
                res11 = _mm512_add_ps(res11, _mm512_sub_ps(pos11, neg11));
                res21 = _mm512_add_ps(res21, _mm512_sub_ps(pos21, neg21));
                res31 = _mm512_add_ps(res31, _mm512_sub_ps(pos31, neg31));
                res41 = _mm512_add_ps(res41, _mm512_sub_ps(pos41, neg41));
                res51 = _mm512_add_ps(res51, _mm512_sub_ps(pos51, neg51));
                res61 = _mm512_add_ps(res61, _mm512_sub_ps(pos61, neg61));
                res71 = _mm512_add_ps(res71, _mm512_sub_ps(pos71, neg71));
            }
            _mm512_store_ps(result + (j * 8 + 0) * M_ROW + i + 0, res00);
            _mm512_store_ps(result + (j * 8 + 1) * M_ROW + i + 0, res10);
            _mm512_store_ps(result + (j * 8 + 2) * M_ROW + i + 0, res20);
            _mm512_store_ps(result + (j * 8 + 3) * M_ROW + i + 0, res30);
            _mm512_store_ps(result + (j * 8 + 4) * M_ROW + i + 0, res40);
            _mm512_store_ps(result + (j * 8 + 5) * M_ROW + i + 0, res50);
            _mm512_store_ps(result + (j * 8 + 6) * M_ROW + i + 0, res60);
            _mm512_store_ps(result + (j * 8 + 7) * M_ROW + i + 0, res70);
            _mm512_store_ps(result + (j * 8 + 0) * M_ROW + i + 16, res01);
            _mm512_store_ps(result + (j * 8 + 1) * M_ROW + i + 16, res11);
            _mm512_store_ps(result + (j * 8 + 2) * M_ROW + i + 16, res21);
            _mm512_store_ps(result + (j * 8 + 3) * M_ROW + i + 16, res31);
            _mm512_store_ps(result + (j * 8 + 4) * M_ROW + i + 16, res41);
            _mm512_store_ps(result + (j * 8 + 5) * M_ROW + i + 16, res51);
            _mm512_store_ps(result + (j * 8 + 6) * M_ROW + i + 16, res61);
            _mm512_store_ps(result + (j * 8 + 7) * M_ROW + i + 16, res71);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Uniform_32xG16_AVX512_OpenMP(const float* X, const int32_t NonZeroPerCol, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 16; j++) {
        for (int i = 0; i < M_ROW; i += 32) {
            __m512 res00 = _mm512_setzero_ps();
            __m512 res01 = _mm512_setzero_ps();
            __m512 res10 = _mm512_setzero_ps();
            __m512 res11 = _mm512_setzero_ps();
            __m512 res20 = _mm512_setzero_ps();
            __m512 res21 = _mm512_setzero_ps();
            __m512 res30 = _mm512_setzero_ps();
            __m512 res31 = _mm512_setzero_ps();
            __m512 res40 = _mm512_setzero_ps();
            __m512 res41 = _mm512_setzero_ps();
            __m512 res50 = _mm512_setzero_ps();
            __m512 res51 = _mm512_setzero_ps();
            __m512 res60 = _mm512_setzero_ps();
            __m512 res61 = _mm512_setzero_ps();
            __m512 res70 = _mm512_setzero_ps();
            __m512 res71 = _mm512_setzero_ps();
            __m512 res80 = _mm512_setzero_ps();
            __m512 res81 = _mm512_setzero_ps();
            __m512 res90 = _mm512_setzero_ps();
            __m512 res91 = _mm512_setzero_ps();
            __m512 res100 = _mm512_setzero_ps();
            __m512 res101 = _mm512_setzero_ps();
            __m512 res110 = _mm512_setzero_ps();
            __m512 res111 = _mm512_setzero_ps();
            __m512 res120 = _mm512_setzero_ps();
            __m512 res121 = _mm512_setzero_ps();
            __m512 res130 = _mm512_setzero_ps();
            __m512 res131 = _mm512_setzero_ps();
            __m512 res140 = _mm512_setzero_ps();
            __m512 res141 = _mm512_setzero_ps();
            __m512 res150 = _mm512_setzero_ps();
            __m512 res151 = _mm512_setzero_ps();
            for (int k = j * 16 * NonZeroPerCol; k < (j + 1) * 16 * NonZeroPerCol; k += 32) {
                __m512 pos00 = _mm512_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m512 neg00 = _mm512_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m512 pos10 = _mm512_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m512 neg10 = _mm512_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m512 pos20 = _mm512_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m512 neg20 = _mm512_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m512 pos30 = _mm512_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m512 neg30 = _mm512_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m512 pos40 = _mm512_load_ps(X + row_index[k + 8] * M_ROW + i + 0);
                __m512 neg40 = _mm512_load_ps(X + row_index[k + 9] * M_ROW + i + 0);
                __m512 pos50 = _mm512_load_ps(X + row_index[k + 10] * M_ROW + i + 0);
                __m512 neg50 = _mm512_load_ps(X + row_index[k + 11] * M_ROW + i + 0);
                __m512 pos60 = _mm512_load_ps(X + row_index[k + 12] * M_ROW + i + 0);
                __m512 neg60 = _mm512_load_ps(X + row_index[k + 13] * M_ROW + i + 0);
                __m512 pos70 = _mm512_load_ps(X + row_index[k + 14] * M_ROW + i + 0);
                __m512 neg70 = _mm512_load_ps(X + row_index[k + 15] * M_ROW + i + 0);
                __m512 pos80 = _mm512_load_ps(X + row_index[k + 16] * M_ROW + i + 0);
                __m512 neg80 = _mm512_load_ps(X + row_index[k + 17] * M_ROW + i + 0);
                __m512 pos90 = _mm512_load_ps(X + row_index[k + 18] * M_ROW + i + 0);
                __m512 neg90 = _mm512_load_ps(X + row_index[k + 19] * M_ROW + i + 0);
                __m512 pos100 = _mm512_load_ps(X + row_index[k + 20] * M_ROW + i + 0);
                __m512 neg100 = _mm512_load_ps(X + row_index[k + 21] * M_ROW + i + 0);
                __m512 pos110 = _mm512_load_ps(X + row_index[k + 22] * M_ROW + i + 0);
                __m512 neg110 = _mm512_load_ps(X + row_index[k + 23] * M_ROW + i + 0);
                __m512 pos120 = _mm512_load_ps(X + row_index[k + 24] * M_ROW + i + 0);
                __m512 neg120 = _mm512_load_ps(X + row_index[k + 25] * M_ROW + i + 0);
                __m512 pos130 = _mm512_load_ps(X + row_index[k + 26] * M_ROW + i + 0);
                __m512 neg130 = _mm512_load_ps(X + row_index[k + 27] * M_ROW + i + 0);
                __m512 pos140 = _mm512_load_ps(X + row_index[k + 28] * M_ROW + i + 0);
                __m512 neg140 = _mm512_load_ps(X + row_index[k + 29] * M_ROW + i + 0);
                __m512 pos150 = _mm512_load_ps(X + row_index[k + 30] * M_ROW + i + 0);
                __m512 neg150 = _mm512_load_ps(X + row_index[k + 31] * M_ROW + i + 0);
                __m512 pos01 = _mm512_load_ps(X + row_index[k + 0] * M_ROW + i + 16);
                __m512 neg01 = _mm512_load_ps(X + row_index[k + 1] * M_ROW + i + 16);
                __m512 pos11 = _mm512_load_ps(X + row_index[k + 2] * M_ROW + i + 16);
                __m512 neg11 = _mm512_load_ps(X + row_index[k + 3] * M_ROW + i + 16);
                __m512 pos21 = _mm512_load_ps(X + row_index[k + 4] * M_ROW + i + 16);
                __m512 neg21 = _mm512_load_ps(X + row_index[k + 5] * M_ROW + i + 16);
                __m512 pos31 = _mm512_load_ps(X + row_index[k + 6] * M_ROW + i + 16);
                __m512 neg31 = _mm512_load_ps(X + row_index[k + 7] * M_ROW + i + 16);
                __m512 pos41 = _mm512_load_ps(X + row_index[k + 8] * M_ROW + i + 16);
                __m512 neg41 = _mm512_load_ps(X + row_index[k + 9] * M_ROW + i + 16);
                __m512 pos51 = _mm512_load_ps(X + row_index[k + 10] * M_ROW + i + 16);
                __m512 neg51 = _mm512_load_ps(X + row_index[k + 11] * M_ROW + i + 16);
                __m512 pos61 = _mm512_load_ps(X + row_index[k + 12] * M_ROW + i + 16);
                __m512 neg61 = _mm512_load_ps(X + row_index[k + 13] * M_ROW + i + 16);
                __m512 pos71 = _mm512_load_ps(X + row_index[k + 14] * M_ROW + i + 16);
                __m512 neg71 = _mm512_load_ps(X + row_index[k + 15] * M_ROW + i + 16);
                __m512 pos81 = _mm512_load_ps(X + row_index[k + 16] * M_ROW + i + 16);
                __m512 neg81 = _mm512_load_ps(X + row_index[k + 17] * M_ROW + i + 16);
                __m512 pos91 = _mm512_load_ps(X + row_index[k + 18] * M_ROW + i + 16);
                __m512 neg91 = _mm512_load_ps(X + row_index[k + 19] * M_ROW + i + 16);
                __m512 pos101 = _mm512_load_ps(X + row_index[k + 20] * M_ROW + i + 16);
                __m512 neg101 = _mm512_load_ps(X + row_index[k + 21] * M_ROW + i + 16);
                __m512 pos111 = _mm512_load_ps(X + row_index[k + 22] * M_ROW + i + 16);
                __m512 neg111 = _mm512_load_ps(X + row_index[k + 23] * M_ROW + i + 16);
                __m512 pos121 = _mm512_load_ps(X + row_index[k + 24] * M_ROW + i + 16);
                __m512 neg121 = _mm512_load_ps(X + row_index[k + 25] * M_ROW + i + 16);
                __m512 pos131 = _mm512_load_ps(X + row_index[k + 26] * M_ROW + i + 16);
                __m512 neg131 = _mm512_load_ps(X + row_index[k + 27] * M_ROW + i + 16);
                __m512 pos141 = _mm512_load_ps(X + row_index[k + 28] * M_ROW + i + 16);
                __m512 neg141 = _mm512_load_ps(X + row_index[k + 29] * M_ROW + i + 16);
                __m512 pos151 = _mm512_load_ps(X + row_index[k + 30] * M_ROW + i + 16);
                __m512 neg151 = _mm512_load_ps(X + row_index[k + 31] * M_ROW + i + 16);
                res00 = _mm512_add_ps(res00, _mm512_sub_ps(pos00, neg00));
                res10 = _mm512_add_ps(res10, _mm512_sub_ps(pos10, neg10));
                res20 = _mm512_add_ps(res20, _mm512_sub_ps(pos20, neg20));
                res30 = _mm512_add_ps(res30, _mm512_sub_ps(pos30, neg30));
                res40 = _mm512_add_ps(res40, _mm512_sub_ps(pos40, neg40));
                res50 = _mm512_add_ps(res50, _mm512_sub_ps(pos50, neg50));
                res60 = _mm512_add_ps(res60, _mm512_sub_ps(pos60, neg60));
                res70 = _mm512_add_ps(res70, _mm512_sub_ps(pos70, neg70));
                res80 = _mm512_add_ps(res80, _mm512_sub_ps(pos80, neg80));
                res90 = _mm512_add_ps(res90, _mm512_sub_ps(pos90, neg90));
                res100 = _mm512_add_ps(res100, _mm512_sub_ps(pos100, neg100));
                res110 = _mm512_add_ps(res110, _mm512_sub_ps(pos110, neg110));
                res120 = _mm512_add_ps(res120, _mm512_sub_ps(pos120, neg120));
                res130 = _mm512_add_ps(res130, _mm512_sub_ps(pos130, neg130));
                res140 = _mm512_add_ps(res140, _mm512_sub_ps(pos140, neg140));
                res150 = _mm512_add_ps(res150, _mm512_sub_ps(pos150, neg150));
                res01 = _mm512_add_ps(res01, _mm512_sub_ps(pos01, neg01));
                res11 = _mm512_add_ps(res11, _mm512_sub_ps(pos11, neg11));
                res21 = _mm512_add_ps(res21, _mm512_sub_ps(pos21, neg21));
                res31 = _mm512_add_ps(res31, _mm512_sub_ps(pos31, neg31));
                res41 = _mm512_add_ps(res41, _mm512_sub_ps(pos41, neg41));
                res51 = _mm512_add_ps(res51, _mm512_sub_ps(pos51, neg51));
                res61 = _mm512_add_ps(res61, _mm512_sub_ps(pos61, neg61));
                res71 = _mm512_add_ps(res71, _mm512_sub_ps(pos71, neg71));
                res81 = _mm512_add_ps(res81, _mm512_sub_ps(pos81, neg81));
                res91 = _mm512_add_ps(res91, _mm512_sub_ps(pos91, neg91));
                res101 = _mm512_add_ps(res101, _mm512_sub_ps(pos101, neg101));
                res111 = _mm512_add_ps(res111, _mm512_sub_ps(pos111, neg111));
                res121 = _mm512_add_ps(res121, _mm512_sub_ps(pos121, neg121));
                res131 = _mm512_add_ps(res131, _mm512_sub_ps(pos131, neg131));
                res141 = _mm512_add_ps(res141, _mm512_sub_ps(pos141, neg141));
                res151 = _mm512_add_ps(res151, _mm512_sub_ps(pos151, neg151));
            }
            _mm512_store_ps(result + (j * 16 + 0) * M_ROW + i + 0, res00);
            _mm512_store_ps(result + (j * 16 + 1) * M_ROW + i + 0, res10);
            _mm512_store_ps(result + (j * 16 + 2) * M_ROW + i + 0, res20);
            _mm512_store_ps(result + (j * 16 + 3) * M_ROW + i + 0, res30);
            _mm512_store_ps(result + (j * 16 + 4) * M_ROW + i + 0, res40);
            _mm512_store_ps(result + (j * 16 + 5) * M_ROW + i + 0, res50);
            _mm512_store_ps(result + (j * 16 + 6) * M_ROW + i + 0, res60);
            _mm512_store_ps(result + (j * 16 + 7) * M_ROW + i + 0, res70);
            _mm512_store_ps(result + (j * 16 + 8) * M_ROW + i + 0, res80);
            _mm512_store_ps(result + (j * 16 + 9) * M_ROW + i + 0, res90);
            _mm512_store_ps(result + (j * 16 + 10) * M_ROW + i + 0, res100);
            _mm512_store_ps(result + (j * 16 + 11) * M_ROW + i + 0, res110);
            _mm512_store_ps(result + (j * 16 + 12) * M_ROW + i + 0, res120);
            _mm512_store_ps(result + (j * 16 + 13) * M_ROW + i + 0, res130);
            _mm512_store_ps(result + (j * 16 + 14) * M_ROW + i + 0, res140);
            _mm512_store_ps(result + (j * 16 + 15) * M_ROW + i + 0, res150);
            _mm512_store_ps(result + (j * 16 + 0) * M_ROW + i + 16, res01);
            _mm512_store_ps(result + (j * 16 + 1) * M_ROW + i + 16, res11);
            _mm512_store_ps(result + (j * 16 + 2) * M_ROW + i + 16, res21);
            _mm512_store_ps(result + (j * 16 + 3) * M_ROW + i + 16, res31);
            _mm512_store_ps(result + (j * 16 + 4) * M_ROW + i + 16, res41);
            _mm512_store_ps(result + (j * 16 + 5) * M_ROW + i + 16, res51);
            _mm512_store_ps(result + (j * 16 + 6) * M_ROW + i + 16, res61);
            _mm512_store_ps(result + (j * 16 + 7) * M_ROW + i + 16, res71);
            _mm512_store_ps(result + (j * 16 + 8) * M_ROW + i + 16, res81);
            _mm512_store_ps(result + (j * 16 + 9) * M_ROW + i + 16, res91);
            _mm512_store_ps(result + (j * 16 + 10) * M_ROW + i + 16, res101);
            _mm512_store_ps(result + (j * 16 + 11) * M_ROW + i + 16, res111);
            _mm512_store_ps(result + (j * 16 + 12) * M_ROW + i + 16, res121);
            _mm512_store_ps(result + (j * 16 + 13) * M_ROW + i + 16, res131);
            _mm512_store_ps(result + (j * 16 + 14) * M_ROW + i + 16, res141);
            _mm512_store_ps(result + (j * 16 + 15) * M_ROW + i + 16, res151);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Uniform_32xG32_AVX512_OpenMP(const float* X, const int32_t NonZeroPerCol, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 32; j++) {
        for (int i = 0; i < M_ROW; i += 32) {
            __m512 res00 = _mm512_setzero_ps();
            __m512 res01 = _mm512_setzero_ps();
            __m512 res10 = _mm512_setzero_ps();
            __m512 res11 = _mm512_setzero_ps();
            __m512 res20 = _mm512_setzero_ps();
            __m512 res21 = _mm512_setzero_ps();
            __m512 res30 = _mm512_setzero_ps();
            __m512 res31 = _mm512_setzero_ps();
            __m512 res40 = _mm512_setzero_ps();
            __m512 res41 = _mm512_setzero_ps();
            __m512 res50 = _mm512_setzero_ps();
            __m512 res51 = _mm512_setzero_ps();
            __m512 res60 = _mm512_setzero_ps();
            __m512 res61 = _mm512_setzero_ps();
            __m512 res70 = _mm512_setzero_ps();
            __m512 res71 = _mm512_setzero_ps();
            __m512 res80 = _mm512_setzero_ps();
            __m512 res81 = _mm512_setzero_ps();
            __m512 res90 = _mm512_setzero_ps();
            __m512 res91 = _mm512_setzero_ps();
            __m512 res100 = _mm512_setzero_ps();
            __m512 res101 = _mm512_setzero_ps();
            __m512 res110 = _mm512_setzero_ps();
            __m512 res111 = _mm512_setzero_ps();
            __m512 res120 = _mm512_setzero_ps();
            __m512 res121 = _mm512_setzero_ps();
            __m512 res130 = _mm512_setzero_ps();
            __m512 res131 = _mm512_setzero_ps();
            __m512 res140 = _mm512_setzero_ps();
            __m512 res141 = _mm512_setzero_ps();
            __m512 res150 = _mm512_setzero_ps();
            __m512 res151 = _mm512_setzero_ps();
            __m512 res160 = _mm512_setzero_ps();
            __m512 res161 = _mm512_setzero_ps();
            __m512 res170 = _mm512_setzero_ps();
            __m512 res171 = _mm512_setzero_ps();
            __m512 res180 = _mm512_setzero_ps();
            __m512 res181 = _mm512_setzero_ps();
            __m512 res190 = _mm512_setzero_ps();
            __m512 res191 = _mm512_setzero_ps();
            __m512 res200 = _mm512_setzero_ps();
            __m512 res201 = _mm512_setzero_ps();
            __m512 res210 = _mm512_setzero_ps();
            __m512 res211 = _mm512_setzero_ps();
            __m512 res220 = _mm512_setzero_ps();
            __m512 res221 = _mm512_setzero_ps();
            __m512 res230 = _mm512_setzero_ps();
            __m512 res231 = _mm512_setzero_ps();
            __m512 res240 = _mm512_setzero_ps();
            __m512 res241 = _mm512_setzero_ps();
            __m512 res250 = _mm512_setzero_ps();
            __m512 res251 = _mm512_setzero_ps();
            __m512 res260 = _mm512_setzero_ps();
            __m512 res261 = _mm512_setzero_ps();
            __m512 res270 = _mm512_setzero_ps();
            __m512 res271 = _mm512_setzero_ps();
            __m512 res280 = _mm512_setzero_ps();
            __m512 res281 = _mm512_setzero_ps();
            __m512 res290 = _mm512_setzero_ps();
            __m512 res291 = _mm512_setzero_ps();
            __m512 res300 = _mm512_setzero_ps();
            __m512 res301 = _mm512_setzero_ps();
            __m512 res310 = _mm512_setzero_ps();
            __m512 res311 = _mm512_setzero_ps();
            for (int k = j * 32 * NonZeroPerCol; k < (j + 1) * 32 * NonZeroPerCol; k += 64) {
                __m512 pos00 = _mm512_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m512 neg00 = _mm512_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m512 pos10 = _mm512_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m512 neg10 = _mm512_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m512 pos20 = _mm512_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m512 neg20 = _mm512_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m512 pos30 = _mm512_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m512 neg30 = _mm512_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m512 pos40 = _mm512_load_ps(X + row_index[k + 8] * M_ROW + i + 0);
                __m512 neg40 = _mm512_load_ps(X + row_index[k + 9] * M_ROW + i + 0);
                __m512 pos50 = _mm512_load_ps(X + row_index[k + 10] * M_ROW + i + 0);
                __m512 neg50 = _mm512_load_ps(X + row_index[k + 11] * M_ROW + i + 0);
                __m512 pos60 = _mm512_load_ps(X + row_index[k + 12] * M_ROW + i + 0);
                __m512 neg60 = _mm512_load_ps(X + row_index[k + 13] * M_ROW + i + 0);
                __m512 pos70 = _mm512_load_ps(X + row_index[k + 14] * M_ROW + i + 0);
                __m512 neg70 = _mm512_load_ps(X + row_index[k + 15] * M_ROW + i + 0);
                __m512 pos80 = _mm512_load_ps(X + row_index[k + 16] * M_ROW + i + 0);
                __m512 neg80 = _mm512_load_ps(X + row_index[k + 17] * M_ROW + i + 0);
                __m512 pos90 = _mm512_load_ps(X + row_index[k + 18] * M_ROW + i + 0);
                __m512 neg90 = _mm512_load_ps(X + row_index[k + 19] * M_ROW + i + 0);
                __m512 pos100 = _mm512_load_ps(X + row_index[k + 20] * M_ROW + i + 0);
                __m512 neg100 = _mm512_load_ps(X + row_index[k + 21] * M_ROW + i + 0);
                __m512 pos110 = _mm512_load_ps(X + row_index[k + 22] * M_ROW + i + 0);
                __m512 neg110 = _mm512_load_ps(X + row_index[k + 23] * M_ROW + i + 0);
                __m512 pos120 = _mm512_load_ps(X + row_index[k + 24] * M_ROW + i + 0);
                __m512 neg120 = _mm512_load_ps(X + row_index[k + 25] * M_ROW + i + 0);
                __m512 pos130 = _mm512_load_ps(X + row_index[k + 26] * M_ROW + i + 0);
                __m512 neg130 = _mm512_load_ps(X + row_index[k + 27] * M_ROW + i + 0);
                __m512 pos140 = _mm512_load_ps(X + row_index[k + 28] * M_ROW + i + 0);
                __m512 neg140 = _mm512_load_ps(X + row_index[k + 29] * M_ROW + i + 0);
                __m512 pos150 = _mm512_load_ps(X + row_index[k + 30] * M_ROW + i + 0);
                __m512 neg150 = _mm512_load_ps(X + row_index[k + 31] * M_ROW + i + 0);
                __m512 pos160 = _mm512_load_ps(X + row_index[k + 32] * M_ROW + i + 0);
                __m512 neg160 = _mm512_load_ps(X + row_index[k + 33] * M_ROW + i + 0);
                __m512 pos170 = _mm512_load_ps(X + row_index[k + 34] * M_ROW + i + 0);
                __m512 neg170 = _mm512_load_ps(X + row_index[k + 35] * M_ROW + i + 0);
                __m512 pos180 = _mm512_load_ps(X + row_index[k + 36] * M_ROW + i + 0);
                __m512 neg180 = _mm512_load_ps(X + row_index[k + 37] * M_ROW + i + 0);
                __m512 pos190 = _mm512_load_ps(X + row_index[k + 38] * M_ROW + i + 0);
                __m512 neg190 = _mm512_load_ps(X + row_index[k + 39] * M_ROW + i + 0);
                __m512 pos200 = _mm512_load_ps(X + row_index[k + 40] * M_ROW + i + 0);
                __m512 neg200 = _mm512_load_ps(X + row_index[k + 41] * M_ROW + i + 0);
                __m512 pos210 = _mm512_load_ps(X + row_index[k + 42] * M_ROW + i + 0);
                __m512 neg210 = _mm512_load_ps(X + row_index[k + 43] * M_ROW + i + 0);
                __m512 pos220 = _mm512_load_ps(X + row_index[k + 44] * M_ROW + i + 0);
                __m512 neg220 = _mm512_load_ps(X + row_index[k + 45] * M_ROW + i + 0);
                __m512 pos230 = _mm512_load_ps(X + row_index[k + 46] * M_ROW + i + 0);
                __m512 neg230 = _mm512_load_ps(X + row_index[k + 47] * M_ROW + i + 0);
                __m512 pos240 = _mm512_load_ps(X + row_index[k + 48] * M_ROW + i + 0);
                __m512 neg240 = _mm512_load_ps(X + row_index[k + 49] * M_ROW + i + 0);
                __m512 pos250 = _mm512_load_ps(X + row_index[k + 50] * M_ROW + i + 0);
                __m512 neg250 = _mm512_load_ps(X + row_index[k + 51] * M_ROW + i + 0);
                __m512 pos260 = _mm512_load_ps(X + row_index[k + 52] * M_ROW + i + 0);
                __m512 neg260 = _mm512_load_ps(X + row_index[k + 53] * M_ROW + i + 0);
                __m512 pos270 = _mm512_load_ps(X + row_index[k + 54] * M_ROW + i + 0);
                __m512 neg270 = _mm512_load_ps(X + row_index[k + 55] * M_ROW + i + 0);
                __m512 pos280 = _mm512_load_ps(X + row_index[k + 56] * M_ROW + i + 0);
                __m512 neg280 = _mm512_load_ps(X + row_index[k + 57] * M_ROW + i + 0);
                __m512 pos290 = _mm512_load_ps(X + row_index[k + 58] * M_ROW + i + 0);
                __m512 neg290 = _mm512_load_ps(X + row_index[k + 59] * M_ROW + i + 0);
                __m512 pos300 = _mm512_load_ps(X + row_index[k + 60] * M_ROW + i + 0);
                __m512 neg300 = _mm512_load_ps(X + row_index[k + 61] * M_ROW + i + 0);
                __m512 pos310 = _mm512_load_ps(X + row_index[k + 62] * M_ROW + i + 0);
                __m512 neg310 = _mm512_load_ps(X + row_index[k + 63] * M_ROW + i + 0);
                __m512 pos01 = _mm512_load_ps(X + row_index[k + 0] * M_ROW + i + 16);
                __m512 neg01 = _mm512_load_ps(X + row_index[k + 1] * M_ROW + i + 16);
                __m512 pos11 = _mm512_load_ps(X + row_index[k + 2] * M_ROW + i + 16);
                __m512 neg11 = _mm512_load_ps(X + row_index[k + 3] * M_ROW + i + 16);
                __m512 pos21 = _mm512_load_ps(X + row_index[k + 4] * M_ROW + i + 16);
                __m512 neg21 = _mm512_load_ps(X + row_index[k + 5] * M_ROW + i + 16);
                __m512 pos31 = _mm512_load_ps(X + row_index[k + 6] * M_ROW + i + 16);
                __m512 neg31 = _mm512_load_ps(X + row_index[k + 7] * M_ROW + i + 16);
                __m512 pos41 = _mm512_load_ps(X + row_index[k + 8] * M_ROW + i + 16);
                __m512 neg41 = _mm512_load_ps(X + row_index[k + 9] * M_ROW + i + 16);
                __m512 pos51 = _mm512_load_ps(X + row_index[k + 10] * M_ROW + i + 16);
                __m512 neg51 = _mm512_load_ps(X + row_index[k + 11] * M_ROW + i + 16);
                __m512 pos61 = _mm512_load_ps(X + row_index[k + 12] * M_ROW + i + 16);
                __m512 neg61 = _mm512_load_ps(X + row_index[k + 13] * M_ROW + i + 16);
                __m512 pos71 = _mm512_load_ps(X + row_index[k + 14] * M_ROW + i + 16);
                __m512 neg71 = _mm512_load_ps(X + row_index[k + 15] * M_ROW + i + 16);
                __m512 pos81 = _mm512_load_ps(X + row_index[k + 16] * M_ROW + i + 16);
                __m512 neg81 = _mm512_load_ps(X + row_index[k + 17] * M_ROW + i + 16);
                __m512 pos91 = _mm512_load_ps(X + row_index[k + 18] * M_ROW + i + 16);
                __m512 neg91 = _mm512_load_ps(X + row_index[k + 19] * M_ROW + i + 16);
                __m512 pos101 = _mm512_load_ps(X + row_index[k + 20] * M_ROW + i + 16);
                __m512 neg101 = _mm512_load_ps(X + row_index[k + 21] * M_ROW + i + 16);
                __m512 pos111 = _mm512_load_ps(X + row_index[k + 22] * M_ROW + i + 16);
                __m512 neg111 = _mm512_load_ps(X + row_index[k + 23] * M_ROW + i + 16);
                __m512 pos121 = _mm512_load_ps(X + row_index[k + 24] * M_ROW + i + 16);
                __m512 neg121 = _mm512_load_ps(X + row_index[k + 25] * M_ROW + i + 16);
                __m512 pos131 = _mm512_load_ps(X + row_index[k + 26] * M_ROW + i + 16);
                __m512 neg131 = _mm512_load_ps(X + row_index[k + 27] * M_ROW + i + 16);
                __m512 pos141 = _mm512_load_ps(X + row_index[k + 28] * M_ROW + i + 16);
                __m512 neg141 = _mm512_load_ps(X + row_index[k + 29] * M_ROW + i + 16);
                __m512 pos151 = _mm512_load_ps(X + row_index[k + 30] * M_ROW + i + 16);
                __m512 neg151 = _mm512_load_ps(X + row_index[k + 31] * M_ROW + i + 16);
                __m512 pos161 = _mm512_load_ps(X + row_index[k + 32] * M_ROW + i + 16);
                __m512 neg161 = _mm512_load_ps(X + row_index[k + 33] * M_ROW + i + 16);
                __m512 pos171 = _mm512_load_ps(X + row_index[k + 34] * M_ROW + i + 16);
                __m512 neg171 = _mm512_load_ps(X + row_index[k + 35] * M_ROW + i + 16);
                __m512 pos181 = _mm512_load_ps(X + row_index[k + 36] * M_ROW + i + 16);
                __m512 neg181 = _mm512_load_ps(X + row_index[k + 37] * M_ROW + i + 16);
                __m512 pos191 = _mm512_load_ps(X + row_index[k + 38] * M_ROW + i + 16);
                __m512 neg191 = _mm512_load_ps(X + row_index[k + 39] * M_ROW + i + 16);
                __m512 pos201 = _mm512_load_ps(X + row_index[k + 40] * M_ROW + i + 16);
                __m512 neg201 = _mm512_load_ps(X + row_index[k + 41] * M_ROW + i + 16);
                __m512 pos211 = _mm512_load_ps(X + row_index[k + 42] * M_ROW + i + 16);
                __m512 neg211 = _mm512_load_ps(X + row_index[k + 43] * M_ROW + i + 16);
                __m512 pos221 = _mm512_load_ps(X + row_index[k + 44] * M_ROW + i + 16);
                __m512 neg221 = _mm512_load_ps(X + row_index[k + 45] * M_ROW + i + 16);
                __m512 pos231 = _mm512_load_ps(X + row_index[k + 46] * M_ROW + i + 16);
                __m512 neg231 = _mm512_load_ps(X + row_index[k + 47] * M_ROW + i + 16);
                __m512 pos241 = _mm512_load_ps(X + row_index[k + 48] * M_ROW + i + 16);
                __m512 neg241 = _mm512_load_ps(X + row_index[k + 49] * M_ROW + i + 16);
                __m512 pos251 = _mm512_load_ps(X + row_index[k + 50] * M_ROW + i + 16);
                __m512 neg251 = _mm512_load_ps(X + row_index[k + 51] * M_ROW + i + 16);
                __m512 pos261 = _mm512_load_ps(X + row_index[k + 52] * M_ROW + i + 16);
                __m512 neg261 = _mm512_load_ps(X + row_index[k + 53] * M_ROW + i + 16);
                __m512 pos271 = _mm512_load_ps(X + row_index[k + 54] * M_ROW + i + 16);
                __m512 neg271 = _mm512_load_ps(X + row_index[k + 55] * M_ROW + i + 16);
                __m512 pos281 = _mm512_load_ps(X + row_index[k + 56] * M_ROW + i + 16);
                __m512 neg281 = _mm512_load_ps(X + row_index[k + 57] * M_ROW + i + 16);
                __m512 pos291 = _mm512_load_ps(X + row_index[k + 58] * M_ROW + i + 16);
                __m512 neg291 = _mm512_load_ps(X + row_index[k + 59] * M_ROW + i + 16);
                __m512 pos301 = _mm512_load_ps(X + row_index[k + 60] * M_ROW + i + 16);
                __m512 neg301 = _mm512_load_ps(X + row_index[k + 61] * M_ROW + i + 16);
                __m512 pos311 = _mm512_load_ps(X + row_index[k + 62] * M_ROW + i + 16);
                __m512 neg311 = _mm512_load_ps(X + row_index[k + 63] * M_ROW + i + 16);
                res00 = _mm512_add_ps(res00, _mm512_sub_ps(pos00, neg00));
                res10 = _mm512_add_ps(res10, _mm512_sub_ps(pos10, neg10));
                res20 = _mm512_add_ps(res20, _mm512_sub_ps(pos20, neg20));
                res30 = _mm512_add_ps(res30, _mm512_sub_ps(pos30, neg30));
                res40 = _mm512_add_ps(res40, _mm512_sub_ps(pos40, neg40));
                res50 = _mm512_add_ps(res50, _mm512_sub_ps(pos50, neg50));
                res60 = _mm512_add_ps(res60, _mm512_sub_ps(pos60, neg60));
                res70 = _mm512_add_ps(res70, _mm512_sub_ps(pos70, neg70));
                res80 = _mm512_add_ps(res80, _mm512_sub_ps(pos80, neg80));
                res90 = _mm512_add_ps(res90, _mm512_sub_ps(pos90, neg90));
                res100 = _mm512_add_ps(res100, _mm512_sub_ps(pos100, neg100));
                res110 = _mm512_add_ps(res110, _mm512_sub_ps(pos110, neg110));
                res120 = _mm512_add_ps(res120, _mm512_sub_ps(pos120, neg120));
                res130 = _mm512_add_ps(res130, _mm512_sub_ps(pos130, neg130));
                res140 = _mm512_add_ps(res140, _mm512_sub_ps(pos140, neg140));
                res150 = _mm512_add_ps(res150, _mm512_sub_ps(pos150, neg150));
                res160 = _mm512_add_ps(res160, _mm512_sub_ps(pos160, neg160));
                res170 = _mm512_add_ps(res170, _mm512_sub_ps(pos170, neg170));
                res180 = _mm512_add_ps(res180, _mm512_sub_ps(pos180, neg180));
                res190 = _mm512_add_ps(res190, _mm512_sub_ps(pos190, neg190));
                res200 = _mm512_add_ps(res200, _mm512_sub_ps(pos200, neg200));
                res210 = _mm512_add_ps(res210, _mm512_sub_ps(pos210, neg210));
                res220 = _mm512_add_ps(res220, _mm512_sub_ps(pos220, neg220));
                res230 = _mm512_add_ps(res230, _mm512_sub_ps(pos230, neg230));
                res240 = _mm512_add_ps(res240, _mm512_sub_ps(pos240, neg240));
                res250 = _mm512_add_ps(res250, _mm512_sub_ps(pos250, neg250));
                res260 = _mm512_add_ps(res260, _mm512_sub_ps(pos260, neg260));
                res270 = _mm512_add_ps(res270, _mm512_sub_ps(pos270, neg270));
                res280 = _mm512_add_ps(res280, _mm512_sub_ps(pos280, neg280));
                res290 = _mm512_add_ps(res290, _mm512_sub_ps(pos290, neg290));
                res300 = _mm512_add_ps(res300, _mm512_sub_ps(pos300, neg300));
                res310 = _mm512_add_ps(res310, _mm512_sub_ps(pos310, neg310));
                res01 = _mm512_add_ps(res01, _mm512_sub_ps(pos01, neg01));
                res11 = _mm512_add_ps(res11, _mm512_sub_ps(pos11, neg11));
                res21 = _mm512_add_ps(res21, _mm512_sub_ps(pos21, neg21));
                res31 = _mm512_add_ps(res31, _mm512_sub_ps(pos31, neg31));
                res41 = _mm512_add_ps(res41, _mm512_sub_ps(pos41, neg41));
                res51 = _mm512_add_ps(res51, _mm512_sub_ps(pos51, neg51));
                res61 = _mm512_add_ps(res61, _mm512_sub_ps(pos61, neg61));
                res71 = _mm512_add_ps(res71, _mm512_sub_ps(pos71, neg71));
                res81 = _mm512_add_ps(res81, _mm512_sub_ps(pos81, neg81));
                res91 = _mm512_add_ps(res91, _mm512_sub_ps(pos91, neg91));
                res101 = _mm512_add_ps(res101, _mm512_sub_ps(pos101, neg101));
                res111 = _mm512_add_ps(res111, _mm512_sub_ps(pos111, neg111));
                res121 = _mm512_add_ps(res121, _mm512_sub_ps(pos121, neg121));
                res131 = _mm512_add_ps(res131, _mm512_sub_ps(pos131, neg131));
                res141 = _mm512_add_ps(res141, _mm512_sub_ps(pos141, neg141));
                res151 = _mm512_add_ps(res151, _mm512_sub_ps(pos151, neg151));
                res161 = _mm512_add_ps(res161, _mm512_sub_ps(pos161, neg161));
                res171 = _mm512_add_ps(res171, _mm512_sub_ps(pos171, neg171));
                res181 = _mm512_add_ps(res181, _mm512_sub_ps(pos181, neg181));
                res191 = _mm512_add_ps(res191, _mm512_sub_ps(pos191, neg191));
                res201 = _mm512_add_ps(res201, _mm512_sub_ps(pos201, neg201));
                res211 = _mm512_add_ps(res211, _mm512_sub_ps(pos211, neg211));
                res221 = _mm512_add_ps(res221, _mm512_sub_ps(pos221, neg221));
                res231 = _mm512_add_ps(res231, _mm512_sub_ps(pos231, neg231));
                res241 = _mm512_add_ps(res241, _mm512_sub_ps(pos241, neg241));
                res251 = _mm512_add_ps(res251, _mm512_sub_ps(pos251, neg251));
                res261 = _mm512_add_ps(res261, _mm512_sub_ps(pos261, neg261));
                res271 = _mm512_add_ps(res271, _mm512_sub_ps(pos271, neg271));
                res281 = _mm512_add_ps(res281, _mm512_sub_ps(pos281, neg281));
                res291 = _mm512_add_ps(res291, _mm512_sub_ps(pos291, neg291));
                res301 = _mm512_add_ps(res301, _mm512_sub_ps(pos301, neg301));
                res311 = _mm512_add_ps(res311, _mm512_sub_ps(pos311, neg311));
            }
            _mm512_store_ps(result + (j * 32 + 0) * M_ROW + i + 0, res00);
            _mm512_store_ps(result + (j * 32 + 1) * M_ROW + i + 0, res10);
            _mm512_store_ps(result + (j * 32 + 2) * M_ROW + i + 0, res20);
            _mm512_store_ps(result + (j * 32 + 3) * M_ROW + i + 0, res30);
            _mm512_store_ps(result + (j * 32 + 4) * M_ROW + i + 0, res40);
            _mm512_store_ps(result + (j * 32 + 5) * M_ROW + i + 0, res50);
            _mm512_store_ps(result + (j * 32 + 6) * M_ROW + i + 0, res60);
            _mm512_store_ps(result + (j * 32 + 7) * M_ROW + i + 0, res70);
            _mm512_store_ps(result + (j * 32 + 8) * M_ROW + i + 0, res80);
            _mm512_store_ps(result + (j * 32 + 9) * M_ROW + i + 0, res90);
            _mm512_store_ps(result + (j * 32 + 10) * M_ROW + i + 0, res100);
            _mm512_store_ps(result + (j * 32 + 11) * M_ROW + i + 0, res110);
            _mm512_store_ps(result + (j * 32 + 12) * M_ROW + i + 0, res120);
            _mm512_store_ps(result + (j * 32 + 13) * M_ROW + i + 0, res130);
            _mm512_store_ps(result + (j * 32 + 14) * M_ROW + i + 0, res140);
            _mm512_store_ps(result + (j * 32 + 15) * M_ROW + i + 0, res150);
            _mm512_store_ps(result + (j * 32 + 16) * M_ROW + i + 0, res160);
            _mm512_store_ps(result + (j * 32 + 17) * M_ROW + i + 0, res170);
            _mm512_store_ps(result + (j * 32 + 18) * M_ROW + i + 0, res180);
            _mm512_store_ps(result + (j * 32 + 19) * M_ROW + i + 0, res190);
            _mm512_store_ps(result + (j * 32 + 20) * M_ROW + i + 0, res200);
            _mm512_store_ps(result + (j * 32 + 21) * M_ROW + i + 0, res210);
            _mm512_store_ps(result + (j * 32 + 22) * M_ROW + i + 0, res220);
            _mm512_store_ps(result + (j * 32 + 23) * M_ROW + i + 0, res230);
            _mm512_store_ps(result + (j * 32 + 24) * M_ROW + i + 0, res240);
            _mm512_store_ps(result + (j * 32 + 25) * M_ROW + i + 0, res250);
            _mm512_store_ps(result + (j * 32 + 26) * M_ROW + i + 0, res260);
            _mm512_store_ps(result + (j * 32 + 27) * M_ROW + i + 0, res270);
            _mm512_store_ps(result + (j * 32 + 28) * M_ROW + i + 0, res280);
            _mm512_store_ps(result + (j * 32 + 29) * M_ROW + i + 0, res290);
            _mm512_store_ps(result + (j * 32 + 30) * M_ROW + i + 0, res300);
            _mm512_store_ps(result + (j * 32 + 31) * M_ROW + i + 0, res310);
            _mm512_store_ps(result + (j * 32 + 0) * M_ROW + i + 16, res01);
            _mm512_store_ps(result + (j * 32 + 1) * M_ROW + i + 16, res11);
            _mm512_store_ps(result + (j * 32 + 2) * M_ROW + i + 16, res21);
            _mm512_store_ps(result + (j * 32 + 3) * M_ROW + i + 16, res31);
            _mm512_store_ps(result + (j * 32 + 4) * M_ROW + i + 16, res41);
            _mm512_store_ps(result + (j * 32 + 5) * M_ROW + i + 16, res51);
            _mm512_store_ps(result + (j * 32 + 6) * M_ROW + i + 16, res61);
            _mm512_store_ps(result + (j * 32 + 7) * M_ROW + i + 16, res71);
            _mm512_store_ps(result + (j * 32 + 8) * M_ROW + i + 16, res81);
            _mm512_store_ps(result + (j * 32 + 9) * M_ROW + i + 16, res91);
            _mm512_store_ps(result + (j * 32 + 10) * M_ROW + i + 16, res101);
            _mm512_store_ps(result + (j * 32 + 11) * M_ROW + i + 16, res111);
            _mm512_store_ps(result + (j * 32 + 12) * M_ROW + i + 16, res121);
            _mm512_store_ps(result + (j * 32 + 13) * M_ROW + i + 16, res131);
            _mm512_store_ps(result + (j * 32 + 14) * M_ROW + i + 16, res141);
            _mm512_store_ps(result + (j * 32 + 15) * M_ROW + i + 16, res151);
            _mm512_store_ps(result + (j * 32 + 16) * M_ROW + i + 16, res161);
            _mm512_store_ps(result + (j * 32 + 17) * M_ROW + i + 16, res171);
            _mm512_store_ps(result + (j * 32 + 18) * M_ROW + i + 16, res181);
            _mm512_store_ps(result + (j * 32 + 19) * M_ROW + i + 16, res191);
            _mm512_store_ps(result + (j * 32 + 20) * M_ROW + i + 16, res201);
            _mm512_store_ps(result + (j * 32 + 21) * M_ROW + i + 16, res211);
            _mm512_store_ps(result + (j * 32 + 22) * M_ROW + i + 16, res221);
            _mm512_store_ps(result + (j * 32 + 23) * M_ROW + i + 16, res231);
            _mm512_store_ps(result + (j * 32 + 24) * M_ROW + i + 16, res241);
            _mm512_store_ps(result + (j * 32 + 25) * M_ROW + i + 16, res251);
            _mm512_store_ps(result + (j * 32 + 26) * M_ROW + i + 16, res261);
            _mm512_store_ps(result + (j * 32 + 27) * M_ROW + i + 16, res271);
            _mm512_store_ps(result + (j * 32 + 28) * M_ROW + i + 16, res281);
            _mm512_store_ps(result + (j * 32 + 29) * M_ROW + i + 16, res291);
            _mm512_store_ps(result + (j * 32 + 30) * M_ROW + i + 16, res301);
            _mm512_store_ps(result + (j * 32 + 31) * M_ROW + i + 16, res311);
        }
    }
}

void GEMM_CPU_FP32_colMajor_TCSC_Uniform_64xG4_AVX512_OpenMP(const float* X, const int32_t NonZeroPerCol, const int16_t* row_index, float* result, const int M_ROW, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 4; j++) {
        for (int i = 0; i < M_ROW; i += 64) {
            __m512 res00 = _mm512_setzero_ps();
            __m512 res01 = _mm512_setzero_ps();
            __m512 res02 = _mm512_setzero_ps();
            __m512 res03 = _mm512_setzero_ps();
            __m512 res10 = _mm512_setzero_ps();
            __m512 res11 = _mm512_setzero_ps();
            __m512 res12 = _mm512_setzero_ps();
            __m512 res13 = _mm512_setzero_ps();
            __m512 res20 = _mm512_setzero_ps();
            __m512 res21 = _mm512_setzero_ps();
            __m512 res22 = _mm512_setzero_ps();
            __m512 res23 = _mm512_setzero_ps();
            __m512 res30 = _mm512_setzero_ps();
            __m512 res31 = _mm512_setzero_ps();
            __m512 res32 = _mm512_setzero_ps();
            __m512 res33 = _mm512_setzero_ps();
            for (int k = j * 4 * NonZeroPerCol; k < (j + 1) * 4 * NonZeroPerCol; k += 8) {
                __m512 pos00 = _mm512_load_ps(X + row_index[k + 0] * M_ROW + i + 0);
                __m512 neg00 = _mm512_load_ps(X + row_index[k + 1] * M_ROW + i + 0);
                __m512 pos10 = _mm512_load_ps(X + row_index[k + 2] * M_ROW + i + 0);
                __m512 neg10 = _mm512_load_ps(X + row_index[k + 3] * M_ROW + i + 0);
                __m512 pos20 = _mm512_load_ps(X + row_index[k + 4] * M_ROW + i + 0);
                __m512 neg20 = _mm512_load_ps(X + row_index[k + 5] * M_ROW + i + 0);
                __m512 pos30 = _mm512_load_ps(X + row_index[k + 6] * M_ROW + i + 0);
                __m512 neg30 = _mm512_load_ps(X + row_index[k + 7] * M_ROW + i + 0);
                __m512 pos01 = _mm512_load_ps(X + row_index[k + 0] * M_ROW + i + 16);
                __m512 neg01 = _mm512_load_ps(X + row_index[k + 1] * M_ROW + i + 16);
                __m512 pos11 = _mm512_load_ps(X + row_index[k + 2] * M_ROW + i + 16);
                __m512 neg11 = _mm512_load_ps(X + row_index[k + 3] * M_ROW + i + 16);
                __m512 pos21 = _mm512_load_ps(X + row_index[k + 4] * M_ROW + i + 16);
                __m512 neg21 = _mm512_load_ps(X + row_index[k + 5] * M_ROW + i + 16);
                __m512 pos31 = _mm512_load_ps(X + row_index[k + 6] * M_ROW + i + 16);
                __m512 neg31 = _mm512_load_ps(X + row_index[k + 7] * M_ROW + i + 16);
                __m512 pos02 = _mm512_load_ps(X + row_index[k + 0] * M_ROW + i + 32);
                __m512 neg02 = _mm512_load_ps(X + row_index[k + 1] * M_ROW + i + 32);
                __m512 pos12 = _mm512_load_ps(X + row_index[k + 2] * M_ROW + i + 32);
                __m512 neg12 = _mm512_load_ps(X + row_index[k + 3] * M_ROW + i + 32);
                __m512 pos22 = _mm512_load_ps(X + row_index[k + 4] * M_ROW + i + 32);
                __m512 neg22 = _mm512_load_ps(X + row_index[k + 5] * M_ROW + i + 32);
                __m512 pos32 = _mm512_load_ps(X + row_index[k + 6] * M_ROW + i + 32);
                __m512 neg32 = _mm512_load_ps(X + row_index[k + 7] * M_ROW + i + 32);
                __m512 pos03 = _mm512_load_ps(X + row_index[k + 0] * M_ROW + i + 48);
                __m512 neg03 = _mm512_load_ps(X + row_index[k + 1] * M_ROW + i + 48);
                __m512 pos13 = _mm512_load_ps(X + row_index[k + 2] * M_ROW + i + 48);
                __m512 neg13 = _mm512_load_ps(X + row_index[k + 3] * M_ROW + i + 48);
                __m512 pos23 = _mm512_load_ps(X + row_index[k + 4] * M_ROW + i + 48);
                __m512 neg23 = _mm512_load_ps(X + row_index[k + 5] * M_ROW + i + 48);
                __m512 pos33 = _mm512_load_ps(X + row_index[k + 6] * M_ROW + i + 48);
                __m512 neg33 = _mm512_load_ps(X + row_index[k + 7] * M_ROW + i + 48);
                res00 = _mm512_add_ps(res00, _mm512_sub_ps(pos00, neg00));
                res10 = _mm512_add_ps(res10, _mm512_sub_ps(pos10, neg10));
                res20 = _mm512_add_ps(res20, _mm512_sub_ps(pos20, neg20));
                res30 = _mm512_add_ps(res30, _mm512_sub_ps(pos30, neg30));
                res01 = _mm512_add_ps(res01, _mm512_sub_ps(pos01, neg01));
                res11 = _mm512_add_ps(res11, _mm512_sub_ps(pos11, neg11));
                res21 = _mm512_add_ps(res21, _mm512_sub_ps(pos21, neg21));
                res31 = _mm512_add_ps(res31, _mm512_sub_ps(pos31, neg31));
                res02 = _mm512_add_ps(res02, _mm512_sub_ps(pos02, neg02));
                res12 = _mm512_add_ps(res12, _mm512_sub_ps(pos12, neg12));
                res22 = _mm512_add_ps(res22, _mm512_sub_ps(pos22, neg22));
                res32 = _mm512_add_ps(res32, _mm512_sub_ps(pos32, neg32));
                res03 = _mm512_add_ps(res03, _mm512_sub_ps(pos03, neg03));
                res13 = _mm512_add_ps(res13, _mm512_sub_ps(pos13, neg13));
                res23 = _mm512_add_ps(res23, _mm512_sub_ps(pos23, neg23));
                res33 = _mm512_add_ps(res33, _mm512_sub_ps(pos33, neg33));
            }
            _mm512_store_ps(result + (j * 4 + 0) * M_ROW + i + 0, res00);
            _mm512_store_ps(result + (j * 4 + 1) * M_ROW + i + 0, res10);
            _mm512_store_ps(result + (j * 4 + 2) * M_ROW + i + 0, res20);
            _mm512_store_ps(result + (j * 4 + 3) * M_ROW + i + 0, res30);
            _mm512_store_ps(result + (j * 4 + 0) * M_ROW + i + 16, res01);
            _mm512_store_ps(result + (j * 4 + 1) * M_ROW + i + 16, res11);
            _mm512_store_ps(result + (j * 4 + 2) * M_ROW + i + 16, res21);
            _mm512_store_ps(result + (j * 4 + 3) * M_ROW + i + 16, res31);
            _mm512_store_ps(result + (j * 4 + 0) * M_ROW + i + 32, res02);
            _mm512_store_ps(result + (j * 4 + 1) * M_ROW + i + 32, res12);
            _mm512_store_ps(result + (j * 4 + 2) * M_ROW + i + 32, res22);
            _mm512_store_ps(result + (j * 4 + 3) * M_ROW + i + 32, res32);
            _mm512_store_ps(result + (j * 4 + 0) * M_ROW + i + 48, res03);
            _mm512_store_ps(result + (j * 4 + 1) * M_ROW + i + 48, res13);
            _mm512_store_ps(result + (j * 4 + 2) * M_ROW + i + 48, res23);
            _mm512_store_ps(result + (j * 4 + 3) * M_ROW + i + 48, res33);
        }
    }
}

//////////////////////////
//         Row          //
//////////////////////////

void GEMM_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_1xG8_AVX2_OpenMPijj(float* X, int32_t* metadata, int32_t* row_index, float* result, int M_ROW, int N_COL, int K) {
    for (int i = 0; i < M_ROW; i ++) {
    #pragma omp parallel for
        for (int j = 0; j < N_COL / 8; j++) {
            /* Pointer to where a column starts and ends
            int align_start  = metadata[j * 10 + 0];
            int align_end    = metadata[j * 10 + 1];
            int +remain_end0 = metadata[j * 10 + 2];
            int -remain_end0 = metadata[j * 10 + 3];
            int +remain_end1 = metadata[j * 10 + 4];
            int -remain_end1 = metadata[j * 10 + 5];
            ...
            */
            // Group # = j, metadata per group = 18
            const int* groupData  = &metadata[j * 18];        
            __m256 res0 = _mm256_setzero_ps();
            float* Xbase = X + i * K;
            float* Ybase = result + i * N_COL + j * 8;
            for (int k = groupData[0]; k < groupData[1]; k += 16) {
                __m256i indices_p = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
                __m256i indices_n = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
                //__m128i indices_p = _mm128_load_si128(reinterpret_cast<const __m128i*>(row_index + k + 0));
                //__m128i indices_n = _mm128_load_si128(reinterpret_cast<const __m128i*>(row_index + k + 8));
                __m256 pos0 = _mm256_i32gather_ps(Xbase, indices_p, 4);
                __m256 neg0 = _mm256_i32gather_ps(Xbase, indices_n, 4);
                res0 = _mm256_add_ps(res0, _mm256_sub_ps(pos0, neg0));
            }
            _mm256_store_ps(Ybase, res0);

            #pragma unroll(8)
            for (int g = 0; g < 8; g++) {
                float pos00 = 0;
                for (int k = groupData[2*g+1]; k < groupData[2 * g+2]; k++) {
                    pos00 += Xbase[row_index[k]];
                }
                Ybase[g]+=pos00;
                float neg00 = 0;
                for (int k = groupData[2 * g+2]; k < groupData[2 * g+3]; k++) {
                    neg00 += Xbase[row_index[k]];
                }
                Ybase[g] -= neg00;
            }          
        }
    }
}

void GEMM_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_1xG8_AVX2_OpenMPij(float* X, int32_t* metadata, int32_t* row_index, float* result, int M_ROW, int N_COL, int K) {
#pragma omp parallel for
    for (int i = 0; i < M_ROW; i++) {
        for (int j = 0; j < N_COL / 8; j++) {
            /* Pointer to where a column starts and ends
            int align_start  = metadata[j * 10 + 0];
            int align_end    = metadata[j * 10 + 1];
            int +remain_end0 = metadata[j * 10 + 2];
            int -remain_end0 = metadata[j * 10 + 3];
            int +remain_end1 = metadata[j * 10 + 4];
            int -remain_end1 = metadata[j * 10 + 5];
            ...
            */
            // Group # = j, metadata per group = 18
            const int* groupData  = &metadata[j * 18];
            __m256 res0 = _mm256_setzero_ps();
            float* Xbase = X + i * K;
            float* Ybase = result + i * N_COL + j * 8;
            for (int k = groupData[0]; k < groupData[1]; k += 16) {
                __m256i indices_p = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
                __m256i indices_n = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
                //__m128i indices_p = _mm128_load_si128(reinterpret_cast<const __m128i*>(row_index + k + 0));
                //__m128i indices_n = _mm128_load_si128(reinterpret_cast<const __m128i*>(row_index + k + 8));
                __m256 pos0 = _mm256_i32gather_ps(Xbase, indices_p, 4);
                __m256 neg0 = _mm256_i32gather_ps(Xbase, indices_n, 4);
                res0 = _mm256_add_ps(res0, _mm256_sub_ps(pos0, neg0));
            }
            _mm256_store_ps(Ybase, res0);

#pragma unroll(8)
            for (int g = 0; g < 8; g++) {
                float pos00 = 0;
                for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++) {
                    pos00 += Xbase[row_index[k]];
                }
                Ybase[g] += pos00;
                float neg00 = 0;
                for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++) {
                    neg00 += Xbase[row_index[k]];
                }
                Ybase[g] -= neg00;
            }
        }
    }
}

void GEMM_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_1xG8_AVX2_OpenMPji(float* X, int32_t* metadata, int32_t* row_index, float* result, int M_ROW, int N_COL, int K) {
#pragma omp parallel for
        for (int j = 0; j < N_COL / 8; j++) {
            for (int i = 0; i < M_ROW; i++) {

            /* Pointer to where a column starts and ends
            int align_start  = metadata[j * 10 + 0];
            int align_end    = metadata[j * 10 + 1];
            int +remain_end0 = metadata[j * 10 + 2];
            int -remain_end0 = metadata[j * 10 + 3];
            int +remain_end1 = metadata[j * 10 + 4];
            int -remain_end1 = metadata[j * 10 + 5];
            ...
            */
            // Group # = j, metadata per group = 18
            const int* groupData  = &metadata[j * 18];
            __m256 res0 = _mm256_setzero_ps();
            float* Xbase = X + i * K;
            float* Ybase = result + i * N_COL + j * 8;
            for (int k = groupData[0]; k < groupData[1]; k += 16) {
                __m256i indices_p = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
                __m256i indices_n = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
                //__m128i indices_p = _mm128_load_si128(reinterpret_cast<const __m128i*>(row_index + k + 0));
                //__m128i indices_n = _mm128_load_si128(reinterpret_cast<const __m128i*>(row_index + k + 8));
                __m256 pos0 = _mm256_i32gather_ps(Xbase, indices_p, 4);
                __m256 neg0 = _mm256_i32gather_ps(Xbase, indices_n, 4);
                res0 = _mm256_add_ps(res0, _mm256_sub_ps(pos0, neg0));
            }
            _mm256_store_ps(Ybase, res0);

#pragma unroll(8)
            for (int g = 0; g < 8; g++) {
                float pos00 = 0;
                for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++) {
                    pos00 += Xbase[row_index[k]];
                }
                Ybase[g] += pos00;
                float neg00 = 0;
                for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++) {
                    neg00 += Xbase[row_index[k]];
                }
                Ybase[g] -= neg00;
            }
        }
    }
}

void GEMM_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_1xG8_AVX2_OpenMPjii(float* X, int32_t* metadata, int32_t* row_index, float* result, int M_ROW, int N_COL, int K) {
    for (int j = 0; j < N_COL / 8; j++) {
#pragma omp parallel for
        for (int i = 0; i < M_ROW; i++) {

            /* Pointer to where a column starts and ends
            int align_start  = metadata[j * 10 + 0];
            int align_end    = metadata[j * 10 + 1];
            int +remain_end0 = metadata[j * 10 + 2];
            int -remain_end0 = metadata[j * 10 + 3];
            int +remain_end1 = metadata[j * 10 + 4];
            int -remain_end1 = metadata[j * 10 + 5];
            ...
            */
            // Group # = j, metadata per group = 18
            const int* groupData  = &metadata[j * 18];
            __m256 res0 = _mm256_setzero_ps();
            float* Xbase = X + i * K;
            float* Ybase = result + i * N_COL + j * 8;
            for (int k = groupData[0]; k < groupData[1]; k += 16) {
                __m256i indices_p = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
                __m256i indices_n = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
                //__m128i indices_p = _mm128_load_si128(reinterpret_cast<const __m128i*>(row_index + k + 0));
                //__m128i indices_n = _mm128_load_si128(reinterpret_cast<const __m128i*>(row_index + k + 8));
                __m256 pos0 = _mm256_i32gather_ps(Xbase, indices_p, 4);
                __m256 neg0 = _mm256_i32gather_ps(Xbase, indices_n, 4);
                res0 = _mm256_add_ps(res0, _mm256_sub_ps(pos0, neg0));
            }
            _mm256_store_ps(Ybase, res0);

#pragma unroll(8)
            for (int g = 0; g < 8; g++) {
                float pos00 = 0;
                for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++) {
                    pos00 += Xbase[row_index[k]];
                }
                Ybase[g] += pos00;
                float neg00 = 0;
                for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++) {
                    neg00 += Xbase[row_index[k]];
                }
                Ybase[g] -= neg00;
            }
        }
    }
}


void GEMM_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_1xG8_AVX2_OpenMP(float* X, int32_t* metadata, int32_t* row_index, float* result, int M_ROW, int N_COL, int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 8; j++) {
        for (int i = 0; i < M_ROW; i++) {

            /* Pointer to where a column starts and ends
            int align_start  = metadata[j * 10 + 0];
            int align_end    = metadata[j * 10 + 1];
            int +remain_end0 = metadata[j * 10 + 2];
            int -remain_end0 = metadata[j * 10 + 3];
            int +remain_end1 = metadata[j * 10 + 4];
            int -remain_end1 = metadata[j * 10 + 5];
            ...
            */
            // Group # = j, metadata per group = 18
            const int* groupData  = &metadata[j * 18];
            __m256 res0 = _mm256_setzero_ps();
            float* Xbase = X + i * K;
            float* Ybase = result + i * N_COL + j * 8;
            for (int k = groupData[0]; k < groupData[1]; k += 16) {
                __m256i indices_p = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
                __m256i indices_n = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
                //__m128i indices_p = _mm128_load_si128(reinterpret_cast<const __m128i*>(row_index + k + 0));
                //__m128i indices_n = _mm128_load_si128(reinterpret_cast<const __m128i*>(row_index + k + 8));
                __m256 pos0 = _mm256_i32gather_ps(Xbase, indices_p, 4);
                __m256 neg0 = _mm256_i32gather_ps(Xbase, indices_n, 4);
                res0 = _mm256_add_ps(res0, _mm256_sub_ps(pos0, neg0));
            }
            _mm256_store_ps(Ybase, res0);

#pragma unroll(8)
            for (int g = 0; g < 8; g++) {
                float pos00 = 0;
                for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++) {
                    pos00 += Xbase[row_index[k]];
                }
                Ybase[g] += pos00;
                float neg00 = 0;
                for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++) {
                    neg00 += Xbase[row_index[k]];
                }
                Ybase[g] -= neg00;
            }
        }
    }
}

void GEMM_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_2xG8_AVX2_OpenMP(float* X, int32_t* metadata, int32_t* row_index, float* result, int M_ROW, int N_COL, int K) {
#pragma omp parallel for
        for (int j = 0; j < N_COL / 8; j++) { 
            for (int i = 0; i < M_ROW; i += 2) {
            float* Xbase = X + i * K;

            const int* groupData  = &metadata[j * 18];
            __m256 res00 = _mm256_setzero_ps();
            __m256 res01 = _mm256_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 16) {
                __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
                __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
                __m256 pos00 = _mm256_i32gather_ps(Xbase + K * 0, indices_p0, 4);
                __m256 neg00 = _mm256_i32gather_ps(Xbase + K * 0, indices_n0, 4);
                __m256 pos01 = _mm256_i32gather_ps(Xbase + K * 1, indices_p0, 4);
                __m256 neg01 = _mm256_i32gather_ps(Xbase + K * 1, indices_n0, 4);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res01 = _mm256_add_ps(res01, _mm256_sub_ps(pos01, neg01));
            }
            float* Ybase = result + i * N_COL + j * 8;
            _mm256_store_ps(Ybase + N_COL * 0 + 0, res00);
            _mm256_store_ps(Ybase + N_COL * 1 + 0, res01);
            for (int k = groupData[1]; k < groupData[2]; k++) {
                Ybase[N_COL * 0 + 0] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 0] += Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[2]; k < groupData[3]; k++) {
                Ybase[N_COL * 0 + 0] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 0] -= Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[3]; k < groupData[4]; k++) {
                Ybase[N_COL * 0 + 1] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 1] += Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[4]; k < groupData[5]; k++) {
                Ybase[N_COL * 0 + 1] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 1] -= Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[5]; k < groupData[6]; k++) {
                Ybase[N_COL * 0 + 2] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 2] += Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[6]; k < groupData[7]; k++) {
                Ybase[N_COL * 0 + 2] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 2] -= Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[7]; k < groupData[8]; k++) {
                Ybase[N_COL * 0 + 3] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 3] += Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[8]; k < groupData[9]; k++) {
                Ybase[N_COL * 0 + 3] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 3] -= Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[9]; k < groupData[10]; k++) {
                Ybase[N_COL * 0 + 4] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 4] += Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[10]; k < groupData[11]; k++) {
                Ybase[N_COL * 0 + 4] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 4] -= Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[11]; k < groupData[12]; k++) {
                Ybase[N_COL * 0 + 5] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 5] += Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[12]; k < groupData[13]; k++) {
                Ybase[N_COL * 0 + 5] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 5] -= Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[13]; k < groupData[14]; k++) {
                Ybase[N_COL * 0 + 6] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 6] += Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[14]; k < groupData[15]; k++) {
                Ybase[N_COL * 0 + 6] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 6] -= Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[15]; k < groupData[16]; k++) {
                Ybase[N_COL * 0 + 7] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 7] += Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[16]; k < groupData[17]; k++) {
                Ybase[N_COL * 0 + 7] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 7] -= Xbase[K * 1 + row_index[k]];
            }
        }
    }
}

void GEMM_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_4xG8_AVX2_OpenMP(float* X, int32_t* metadata, int32_t* row_index, float* result, int M_ROW, int N_COL, int K) {
#pragma omp parallel for
        for (int j = 0; j < N_COL / 8; j++) {
            for (int i = 0; i < M_ROW; i += 4) {
            float* Xbase = X + i * K;

            const int* groupData  = &metadata[j * 18];
            __m256 res00 = _mm256_setzero_ps();
            __m256 res01 = _mm256_setzero_ps();
            __m256 res02 = _mm256_setzero_ps();
            __m256 res03 = _mm256_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 16) {
                __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
                __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
                __m256 pos00 = _mm256_i32gather_ps(Xbase + K * 0, indices_p0, 4);
                __m256 neg00 = _mm256_i32gather_ps(Xbase + K * 0, indices_n0, 4);
                __m256 pos01 = _mm256_i32gather_ps(Xbase + K * 1, indices_p0, 4);
                __m256 neg01 = _mm256_i32gather_ps(Xbase + K * 1, indices_n0, 4);
                __m256 pos02 = _mm256_i32gather_ps(Xbase + K * 2, indices_p0, 4);
                __m256 neg02 = _mm256_i32gather_ps(Xbase + K * 2, indices_n0, 4);
                __m256 pos03 = _mm256_i32gather_ps(Xbase + K * 3, indices_p0, 4);
                __m256 neg03 = _mm256_i32gather_ps(Xbase + K * 3, indices_n0, 4);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res01 = _mm256_add_ps(res01, _mm256_sub_ps(pos01, neg01));
                res02 = _mm256_add_ps(res02, _mm256_sub_ps(pos02, neg02));
                res03 = _mm256_add_ps(res03, _mm256_sub_ps(pos03, neg03));
            }
            float* Ybase = result + i * N_COL + j * 8;
            _mm256_store_ps(Ybase + N_COL * 0 + 0, res00);
            _mm256_store_ps(Ybase + N_COL * 1 + 0, res01);
            _mm256_store_ps(Ybase + N_COL * 2 + 0, res02);
            _mm256_store_ps(Ybase + N_COL * 3 + 0, res03);
            for (int k = groupData[1]; k < groupData[2]; k++) {
                Ybase[N_COL * 0 + 0] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 0] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 0] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 0] += Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[2]; k < groupData[3]; k++) {
                Ybase[N_COL * 0 + 0] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 0] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 0] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 0] -= Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[3]; k < groupData[4]; k++) {
                Ybase[N_COL * 0 + 1] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 1] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 1] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 1] += Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[4]; k < groupData[5]; k++) {
                Ybase[N_COL * 0 + 1] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 1] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 1] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 1] -= Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[5]; k < groupData[6]; k++) {
                Ybase[N_COL * 0 + 2] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 2] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 2] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 2] += Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[6]; k < groupData[7]; k++) {
                Ybase[N_COL * 0 + 2] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 2] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 2] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 2] -= Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[7]; k < groupData[8]; k++) {
                Ybase[N_COL * 0 + 3] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 3] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 3] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 3] += Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[8]; k < groupData[9]; k++) {
                Ybase[N_COL * 0 + 3] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 3] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 3] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 3] -= Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[9]; k < groupData[10]; k++) {
                Ybase[N_COL * 0 + 4] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 4] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 4] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 4] += Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[10]; k < groupData[11]; k++) {
                Ybase[N_COL * 0 + 4] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 4] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 4] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 4] -= Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[11]; k < groupData[12]; k++) {
                Ybase[N_COL * 0 + 5] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 5] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 5] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 5] += Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[12]; k < groupData[13]; k++) {
                Ybase[N_COL * 0 + 5] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 5] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 5] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 5] -= Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[13]; k < groupData[14]; k++) {
                Ybase[N_COL * 0 + 6] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 6] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 6] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 6] += Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[14]; k < groupData[15]; k++) {
                Ybase[N_COL * 0 + 6] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 6] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 6] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 6] -= Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[15]; k < groupData[16]; k++) {
                Ybase[N_COL * 0 + 7] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 7] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 7] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 7] += Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[16]; k < groupData[17]; k++) {
                Ybase[N_COL * 0 + 7] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 7] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 7] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 7] -= Xbase[K * 3 + row_index[k]];
            }
        }
    }
}

void GEMM_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_8xG8_AVX2_OpenMP(float* X, int32_t* metadata, int32_t* row_index, float* result, int M_ROW, int N_COL, int K) {
#pragma omp parallel for
        for (int j = 0; j < N_COL / 8; j++) {
            for (int i = 0; i < M_ROW; i += 8) {
            float* Xbase = X + i * K;

            const int* groupData  = &metadata[j * 18];
            __m256 res00 = _mm256_setzero_ps();
            __m256 res01 = _mm256_setzero_ps();
            __m256 res02 = _mm256_setzero_ps();
            __m256 res03 = _mm256_setzero_ps();
            __m256 res04 = _mm256_setzero_ps();
            __m256 res05 = _mm256_setzero_ps();
            __m256 res06 = _mm256_setzero_ps();
            __m256 res07 = _mm256_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 16) {
                __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
                __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
                __m256 pos00 = _mm256_i32gather_ps(Xbase + K * 0, indices_p0, 4);
                __m256 neg00 = _mm256_i32gather_ps(Xbase + K * 0, indices_n0, 4);
                __m256 pos01 = _mm256_i32gather_ps(Xbase + K * 1, indices_p0, 4);
                __m256 neg01 = _mm256_i32gather_ps(Xbase + K * 1, indices_n0, 4);
                __m256 pos02 = _mm256_i32gather_ps(Xbase + K * 2, indices_p0, 4);
                __m256 neg02 = _mm256_i32gather_ps(Xbase + K * 2, indices_n0, 4);
                __m256 pos03 = _mm256_i32gather_ps(Xbase + K * 3, indices_p0, 4);
                __m256 neg03 = _mm256_i32gather_ps(Xbase + K * 3, indices_n0, 4);
                __m256 pos04 = _mm256_i32gather_ps(Xbase + K * 4, indices_p0, 4);
                __m256 neg04 = _mm256_i32gather_ps(Xbase + K * 4, indices_n0, 4);
                __m256 pos05 = _mm256_i32gather_ps(Xbase + K * 5, indices_p0, 4);
                __m256 neg05 = _mm256_i32gather_ps(Xbase + K * 5, indices_n0, 4);
                __m256 pos06 = _mm256_i32gather_ps(Xbase + K * 6, indices_p0, 4);
                __m256 neg06 = _mm256_i32gather_ps(Xbase + K * 6, indices_n0, 4);
                __m256 pos07 = _mm256_i32gather_ps(Xbase + K * 7, indices_p0, 4);
                __m256 neg07 = _mm256_i32gather_ps(Xbase + K * 7, indices_n0, 4);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res01 = _mm256_add_ps(res01, _mm256_sub_ps(pos01, neg01));
                res02 = _mm256_add_ps(res02, _mm256_sub_ps(pos02, neg02));
                res03 = _mm256_add_ps(res03, _mm256_sub_ps(pos03, neg03));
                res04 = _mm256_add_ps(res04, _mm256_sub_ps(pos04, neg04));
                res05 = _mm256_add_ps(res05, _mm256_sub_ps(pos05, neg05));
                res06 = _mm256_add_ps(res06, _mm256_sub_ps(pos06, neg06));
                res07 = _mm256_add_ps(res07, _mm256_sub_ps(pos07, neg07));
            }
            float* Ybase = result + i * N_COL + j * 8;
            _mm256_store_ps(Ybase + N_COL * 0 + 0, res00);
            _mm256_store_ps(Ybase + N_COL * 1 + 0, res01);
            _mm256_store_ps(Ybase + N_COL * 2 + 0, res02);
            _mm256_store_ps(Ybase + N_COL * 3 + 0, res03);
            _mm256_store_ps(Ybase + N_COL * 4 + 0, res04);
            _mm256_store_ps(Ybase + N_COL * 5 + 0, res05);
            _mm256_store_ps(Ybase + N_COL * 6 + 0, res06);
            _mm256_store_ps(Ybase + N_COL * 7 + 0, res07);
            for (int k = groupData[1]; k < groupData[2]; k++) {
                Ybase[N_COL * 0 + 0] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 0] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 0] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 0] += Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 0] += Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 0] += Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 0] += Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 0] += Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[2]; k < groupData[3]; k++) {
                Ybase[N_COL * 0 + 0] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 0] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 0] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 0] -= Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 0] -= Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 0] -= Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 0] -= Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 0] -= Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[3]; k < groupData[4]; k++) {
                Ybase[N_COL * 0 + 1] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 1] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 1] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 1] += Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 1] += Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 1] += Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 1] += Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 1] += Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[4]; k < groupData[5]; k++) {
                Ybase[N_COL * 0 + 1] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 1] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 1] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 1] -= Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 1] -= Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 1] -= Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 1] -= Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 1] -= Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[5]; k < groupData[6]; k++) {
                Ybase[N_COL * 0 + 2] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 2] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 2] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 2] += Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 2] += Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 2] += Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 2] += Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 2] += Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[6]; k < groupData[7]; k++) {
                Ybase[N_COL * 0 + 2] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 2] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 2] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 2] -= Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 2] -= Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 2] -= Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 2] -= Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 2] -= Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[7]; k < groupData[8]; k++) {
                Ybase[N_COL * 0 + 3] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 3] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 3] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 3] += Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 3] += Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 3] += Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 3] += Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 3] += Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[8]; k < groupData[9]; k++) {
                Ybase[N_COL * 0 + 3] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 3] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 3] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 3] -= Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 3] -= Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 3] -= Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 3] -= Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 3] -= Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[9]; k < groupData[10]; k++) {
                Ybase[N_COL * 0 + 4] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 4] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 4] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 4] += Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 4] += Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 4] += Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 4] += Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 4] += Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[10]; k < groupData[11]; k++) {
                Ybase[N_COL * 0 + 4] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 4] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 4] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 4] -= Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 4] -= Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 4] -= Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 4] -= Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 4] -= Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[11]; k < groupData[12]; k++) {
                Ybase[N_COL * 0 + 5] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 5] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 5] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 5] += Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 5] += Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 5] += Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 5] += Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 5] += Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[12]; k < groupData[13]; k++) {
                Ybase[N_COL * 0 + 5] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 5] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 5] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 5] -= Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 5] -= Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 5] -= Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 5] -= Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 5] -= Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[13]; k < groupData[14]; k++) {
                Ybase[N_COL * 0 + 6] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 6] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 6] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 6] += Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 6] += Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 6] += Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 6] += Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 6] += Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[14]; k < groupData[15]; k++) {
                Ybase[N_COL * 0 + 6] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 6] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 6] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 6] -= Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 6] -= Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 6] -= Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 6] -= Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 6] -= Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[15]; k < groupData[16]; k++) {
                Ybase[N_COL * 0 + 7] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 7] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 7] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 7] += Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 7] += Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 7] += Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 7] += Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 7] += Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[16]; k < groupData[17]; k++) {
                Ybase[N_COL * 0 + 7] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 7] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 7] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 7] -= Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 7] -= Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 7] -= Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 7] -= Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 7] -= Xbase[K * 7 + row_index[k]];
            }
        }
    }
}

void GEMM_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_16xG8_AVX2_OpenMP(float* X, int32_t* metadata, int32_t* row_index, float* result, int M_ROW, int N_COL, int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 8; j++) {
        for (int i = 0; i < M_ROW; i += 16) {
            float* Xbase = X + i * K;
            const int* groupData  = &metadata[j * 18];
            __m256 res00 = _mm256_setzero_ps();
            __m256 res01 = _mm256_setzero_ps();
            __m256 res02 = _mm256_setzero_ps();
            __m256 res03 = _mm256_setzero_ps();
            __m256 res04 = _mm256_setzero_ps();
            __m256 res05 = _mm256_setzero_ps();
            __m256 res06 = _mm256_setzero_ps();
            __m256 res07 = _mm256_setzero_ps();
            __m256 res08 = _mm256_setzero_ps();
            __m256 res09 = _mm256_setzero_ps();
            __m256 res010 = _mm256_setzero_ps();
            __m256 res011 = _mm256_setzero_ps();
            __m256 res012 = _mm256_setzero_ps();
            __m256 res013 = _mm256_setzero_ps();
            __m256 res014 = _mm256_setzero_ps();
            __m256 res015 = _mm256_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 16) {
                __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
                __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
                __m256 pos00 = _mm256_i32gather_ps(Xbase + K * 0, indices_p0, 4);
                __m256 pos01 = _mm256_i32gather_ps(Xbase + K * 1, indices_p0, 4);
                __m256 pos02 = _mm256_i32gather_ps(Xbase + K * 2, indices_p0, 4);
                __m256 pos03 = _mm256_i32gather_ps(Xbase + K * 3, indices_p0, 4);
                __m256 pos04 = _mm256_i32gather_ps(Xbase + K * 4, indices_p0, 4);
                __m256 pos05 = _mm256_i32gather_ps(Xbase + K * 5, indices_p0, 4);
                __m256 pos06 = _mm256_i32gather_ps(Xbase + K * 6, indices_p0, 4);
                __m256 pos07 = _mm256_i32gather_ps(Xbase + K * 7, indices_p0, 4);
                __m256 pos08 = _mm256_i32gather_ps(Xbase + K * 8, indices_p0, 4);
                __m256 pos09 = _mm256_i32gather_ps(Xbase + K * 9, indices_p0, 4);
                __m256 pos010 = _mm256_i32gather_ps(Xbase + K * 10, indices_p0, 4);
                __m256 pos011 = _mm256_i32gather_ps(Xbase + K * 11, indices_p0, 4);
                __m256 pos012 = _mm256_i32gather_ps(Xbase + K * 12, indices_p0, 4);
                __m256 pos013 = _mm256_i32gather_ps(Xbase + K * 13, indices_p0, 4);
                __m256 pos014 = _mm256_i32gather_ps(Xbase + K * 14, indices_p0, 4);
                __m256 pos015 = _mm256_i32gather_ps(Xbase + K * 15, indices_p0, 4);
                __m256 neg00 = _mm256_i32gather_ps(Xbase + K * 0, indices_n0, 4);
                __m256 neg01 = _mm256_i32gather_ps(Xbase + K * 1, indices_n0, 4);
                __m256 neg02 = _mm256_i32gather_ps(Xbase + K * 2, indices_n0, 4);
                __m256 neg03 = _mm256_i32gather_ps(Xbase + K * 3, indices_n0, 4);
                __m256 neg04 = _mm256_i32gather_ps(Xbase + K * 4, indices_n0, 4);
                __m256 neg05 = _mm256_i32gather_ps(Xbase + K * 5, indices_n0, 4);
                __m256 neg06 = _mm256_i32gather_ps(Xbase + K * 6, indices_n0, 4);
                __m256 neg07 = _mm256_i32gather_ps(Xbase + K * 7, indices_n0, 4);
                __m256 neg08 = _mm256_i32gather_ps(Xbase + K * 8, indices_n0, 4);
                __m256 neg09 = _mm256_i32gather_ps(Xbase + K * 9, indices_n0, 4);
                __m256 neg010 = _mm256_i32gather_ps(Xbase + K * 10, indices_n0, 4);
                __m256 neg011 = _mm256_i32gather_ps(Xbase + K * 11, indices_n0, 4);
                __m256 neg012 = _mm256_i32gather_ps(Xbase + K * 12, indices_n0, 4);
                __m256 neg013 = _mm256_i32gather_ps(Xbase + K * 13, indices_n0, 4);
                __m256 neg014 = _mm256_i32gather_ps(Xbase + K * 14, indices_n0, 4);
                __m256 neg015 = _mm256_i32gather_ps(Xbase + K * 15, indices_n0, 4);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res01 = _mm256_add_ps(res01, _mm256_sub_ps(pos01, neg01));
                res02 = _mm256_add_ps(res02, _mm256_sub_ps(pos02, neg02));
                res03 = _mm256_add_ps(res03, _mm256_sub_ps(pos03, neg03));
                res04 = _mm256_add_ps(res04, _mm256_sub_ps(pos04, neg04));
                res05 = _mm256_add_ps(res05, _mm256_sub_ps(pos05, neg05));
                res06 = _mm256_add_ps(res06, _mm256_sub_ps(pos06, neg06));
                res07 = _mm256_add_ps(res07, _mm256_sub_ps(pos07, neg07));
                res08 = _mm256_add_ps(res08, _mm256_sub_ps(pos08, neg08));
                res09 = _mm256_add_ps(res09, _mm256_sub_ps(pos09, neg09));
                res010 = _mm256_add_ps(res010, _mm256_sub_ps(pos010, neg010));
                res011 = _mm256_add_ps(res011, _mm256_sub_ps(pos011, neg011));
                res012 = _mm256_add_ps(res012, _mm256_sub_ps(pos012, neg012));
                res013 = _mm256_add_ps(res013, _mm256_sub_ps(pos013, neg013));
                res014 = _mm256_add_ps(res014, _mm256_sub_ps(pos014, neg014));
                res015 = _mm256_add_ps(res015, _mm256_sub_ps(pos015, neg015));
            }
            float* Ybase = result + i * N_COL + j * 8;
            _mm256_store_ps(Ybase + N_COL * 0 + 0, res00);
            _mm256_store_ps(Ybase + N_COL * 1 + 0, res01);
            _mm256_store_ps(Ybase + N_COL * 2 + 0, res02);
            _mm256_store_ps(Ybase + N_COL * 3 + 0, res03);
            _mm256_store_ps(Ybase + N_COL * 4 + 0, res04);
            _mm256_store_ps(Ybase + N_COL * 5 + 0, res05);
            _mm256_store_ps(Ybase + N_COL * 6 + 0, res06);
            _mm256_store_ps(Ybase + N_COL * 7 + 0, res07);
            _mm256_store_ps(Ybase + N_COL * 8 + 0, res08);
            _mm256_store_ps(Ybase + N_COL * 9 + 0, res09);
            _mm256_store_ps(Ybase + N_COL * 10 + 0, res010);
            _mm256_store_ps(Ybase + N_COL * 11 + 0, res011);
            _mm256_store_ps(Ybase + N_COL * 12 + 0, res012);
            _mm256_store_ps(Ybase + N_COL * 13 + 0, res013);
            _mm256_store_ps(Ybase + N_COL * 14 + 0, res014);
            _mm256_store_ps(Ybase + N_COL * 15 + 0, res015);
            for (int k = groupData[1]; k < groupData[2]; k++) {
                Ybase[N_COL * 0 + 0] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 0] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 0] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 0] += Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 0] += Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 0] += Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 0] += Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 0] += Xbase[K * 7 + row_index[k]];
                Ybase[N_COL * 8 + 0] += Xbase[K * 8 + row_index[k]];
                Ybase[N_COL * 9 + 0] += Xbase[K * 9 + row_index[k]];
                Ybase[N_COL * 10 + 0] += Xbase[K * 10 + row_index[k]];
                Ybase[N_COL * 11 + 0] += Xbase[K * 11 + row_index[k]];
                Ybase[N_COL * 12 + 0] += Xbase[K * 12 + row_index[k]];
                Ybase[N_COL * 13 + 0] += Xbase[K * 13 + row_index[k]];
                Ybase[N_COL * 14 + 0] += Xbase[K * 14 + row_index[k]];
                Ybase[N_COL * 15 + 0] += Xbase[K * 15 + row_index[k]];
            }
            for (int k = groupData[2]; k < groupData[3]; k++) {
                Ybase[N_COL * 0 + 0] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 0] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 0] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 0] -= Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 0] -= Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 0] -= Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 0] -= Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 0] -= Xbase[K * 7 + row_index[k]];
                Ybase[N_COL * 8 + 0] -= Xbase[K * 8 + row_index[k]];
                Ybase[N_COL * 9 + 0] -= Xbase[K * 9 + row_index[k]];
                Ybase[N_COL * 10 + 0] -= Xbase[K * 10 + row_index[k]];
                Ybase[N_COL * 11 + 0] -= Xbase[K * 11 + row_index[k]];
                Ybase[N_COL * 12 + 0] -= Xbase[K * 12 + row_index[k]];
                Ybase[N_COL * 13 + 0] -= Xbase[K * 13 + row_index[k]];
                Ybase[N_COL * 14 + 0] -= Xbase[K * 14 + row_index[k]];
                Ybase[N_COL * 15 + 0] -= Xbase[K * 15 + row_index[k]];
            }
            for (int k = groupData[3]; k < groupData[4]; k++) {
                Ybase[N_COL * 0 + 1] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 1] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 1] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 1] += Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 1] += Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 1] += Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 1] += Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 1] += Xbase[K * 7 + row_index[k]];
                Ybase[N_COL * 8 + 1] += Xbase[K * 8 + row_index[k]];
                Ybase[N_COL * 9 + 1] += Xbase[K * 9 + row_index[k]];
                Ybase[N_COL * 10 + 1] += Xbase[K * 10 + row_index[k]];
                Ybase[N_COL * 11 + 1] += Xbase[K * 11 + row_index[k]];
                Ybase[N_COL * 12 + 1] += Xbase[K * 12 + row_index[k]];
                Ybase[N_COL * 13 + 1] += Xbase[K * 13 + row_index[k]];
                Ybase[N_COL * 14 + 1] += Xbase[K * 14 + row_index[k]];
                Ybase[N_COL * 15 + 1] += Xbase[K * 15 + row_index[k]];
            }
            for (int k = groupData[4]; k < groupData[5]; k++) {
                Ybase[N_COL * 0 + 1] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 1] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 1] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 1] -= Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 1] -= Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 1] -= Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 1] -= Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 1] -= Xbase[K * 7 + row_index[k]];
                Ybase[N_COL * 8 + 1] -= Xbase[K * 8 + row_index[k]];
                Ybase[N_COL * 9 + 1] -= Xbase[K * 9 + row_index[k]];
                Ybase[N_COL * 10 + 1] -= Xbase[K * 10 + row_index[k]];
                Ybase[N_COL * 11 + 1] -= Xbase[K * 11 + row_index[k]];
                Ybase[N_COL * 12 + 1] -= Xbase[K * 12 + row_index[k]];
                Ybase[N_COL * 13 + 1] -= Xbase[K * 13 + row_index[k]];
                Ybase[N_COL * 14 + 1] -= Xbase[K * 14 + row_index[k]];
                Ybase[N_COL * 15 + 1] -= Xbase[K * 15 + row_index[k]];
            }
            for (int k = groupData[5]; k < groupData[6]; k++) {
                Ybase[N_COL * 0 + 2] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 2] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 2] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 2] += Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 2] += Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 2] += Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 2] += Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 2] += Xbase[K * 7 + row_index[k]];
                Ybase[N_COL * 8 + 2] += Xbase[K * 8 + row_index[k]];
                Ybase[N_COL * 9 + 2] += Xbase[K * 9 + row_index[k]];
                Ybase[N_COL * 10 + 2] += Xbase[K * 10 + row_index[k]];
                Ybase[N_COL * 11 + 2] += Xbase[K * 11 + row_index[k]];
                Ybase[N_COL * 12 + 2] += Xbase[K * 12 + row_index[k]];
                Ybase[N_COL * 13 + 2] += Xbase[K * 13 + row_index[k]];
                Ybase[N_COL * 14 + 2] += Xbase[K * 14 + row_index[k]];
                Ybase[N_COL * 15 + 2] += Xbase[K * 15 + row_index[k]];
            }
            for (int k = groupData[6]; k < groupData[7]; k++) {
                Ybase[N_COL * 0 + 2] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 2] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 2] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 2] -= Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 2] -= Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 2] -= Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 2] -= Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 2] -= Xbase[K * 7 + row_index[k]];
                Ybase[N_COL * 8 + 2] -= Xbase[K * 8 + row_index[k]];
                Ybase[N_COL * 9 + 2] -= Xbase[K * 9 + row_index[k]];
                Ybase[N_COL * 10 + 2] -= Xbase[K * 10 + row_index[k]];
                Ybase[N_COL * 11 + 2] -= Xbase[K * 11 + row_index[k]];
                Ybase[N_COL * 12 + 2] -= Xbase[K * 12 + row_index[k]];
                Ybase[N_COL * 13 + 2] -= Xbase[K * 13 + row_index[k]];
                Ybase[N_COL * 14 + 2] -= Xbase[K * 14 + row_index[k]];
                Ybase[N_COL * 15 + 2] -= Xbase[K * 15 + row_index[k]];
            }
            for (int k = groupData[7]; k < groupData[8]; k++) {
                Ybase[N_COL * 0 + 3] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 3] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 3] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 3] += Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 3] += Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 3] += Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 3] += Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 3] += Xbase[K * 7 + row_index[k]];
                Ybase[N_COL * 8 + 3] += Xbase[K * 8 + row_index[k]];
                Ybase[N_COL * 9 + 3] += Xbase[K * 9 + row_index[k]];
                Ybase[N_COL * 10 + 3] += Xbase[K * 10 + row_index[k]];
                Ybase[N_COL * 11 + 3] += Xbase[K * 11 + row_index[k]];
                Ybase[N_COL * 12 + 3] += Xbase[K * 12 + row_index[k]];
                Ybase[N_COL * 13 + 3] += Xbase[K * 13 + row_index[k]];
                Ybase[N_COL * 14 + 3] += Xbase[K * 14 + row_index[k]];
                Ybase[N_COL * 15 + 3] += Xbase[K * 15 + row_index[k]];
            }
            for (int k = groupData[8]; k < groupData[9]; k++) {
                Ybase[N_COL * 0 + 3] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 3] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 3] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 3] -= Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 3] -= Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 3] -= Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 3] -= Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 3] -= Xbase[K * 7 + row_index[k]];
                Ybase[N_COL * 8 + 3] -= Xbase[K * 8 + row_index[k]];
                Ybase[N_COL * 9 + 3] -= Xbase[K * 9 + row_index[k]];
                Ybase[N_COL * 10 + 3] -= Xbase[K * 10 + row_index[k]];
                Ybase[N_COL * 11 + 3] -= Xbase[K * 11 + row_index[k]];
                Ybase[N_COL * 12 + 3] -= Xbase[K * 12 + row_index[k]];
                Ybase[N_COL * 13 + 3] -= Xbase[K * 13 + row_index[k]];
                Ybase[N_COL * 14 + 3] -= Xbase[K * 14 + row_index[k]];
                Ybase[N_COL * 15 + 3] -= Xbase[K * 15 + row_index[k]];
            }
            for (int k = groupData[9]; k < groupData[10]; k++) {
                Ybase[N_COL * 0 + 4] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 4] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 4] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 4] += Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 4] += Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 4] += Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 4] += Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 4] += Xbase[K * 7 + row_index[k]];
                Ybase[N_COL * 8 + 4] += Xbase[K * 8 + row_index[k]];
                Ybase[N_COL * 9 + 4] += Xbase[K * 9 + row_index[k]];
                Ybase[N_COL * 10 + 4] += Xbase[K * 10 + row_index[k]];
                Ybase[N_COL * 11 + 4] += Xbase[K * 11 + row_index[k]];
                Ybase[N_COL * 12 + 4] += Xbase[K * 12 + row_index[k]];
                Ybase[N_COL * 13 + 4] += Xbase[K * 13 + row_index[k]];
                Ybase[N_COL * 14 + 4] += Xbase[K * 14 + row_index[k]];
                Ybase[N_COL * 15 + 4] += Xbase[K * 15 + row_index[k]];
            }
            for (int k = groupData[10]; k < groupData[11]; k++) {
                Ybase[N_COL * 0 + 4] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 4] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 4] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 4] -= Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 4] -= Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 4] -= Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 4] -= Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 4] -= Xbase[K * 7 + row_index[k]];
                Ybase[N_COL * 8 + 4] -= Xbase[K * 8 + row_index[k]];
                Ybase[N_COL * 9 + 4] -= Xbase[K * 9 + row_index[k]];
                Ybase[N_COL * 10 + 4] -= Xbase[K * 10 + row_index[k]];
                Ybase[N_COL * 11 + 4] -= Xbase[K * 11 + row_index[k]];
                Ybase[N_COL * 12 + 4] -= Xbase[K * 12 + row_index[k]];
                Ybase[N_COL * 13 + 4] -= Xbase[K * 13 + row_index[k]];
                Ybase[N_COL * 14 + 4] -= Xbase[K * 14 + row_index[k]];
                Ybase[N_COL * 15 + 4] -= Xbase[K * 15 + row_index[k]];
            }
            for (int k = groupData[11]; k < groupData[12]; k++) {
                Ybase[N_COL * 0 + 5] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 5] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 5] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 5] += Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 5] += Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 5] += Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 5] += Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 5] += Xbase[K * 7 + row_index[k]];
                Ybase[N_COL * 8 + 5] += Xbase[K * 8 + row_index[k]];
                Ybase[N_COL * 9 + 5] += Xbase[K * 9 + row_index[k]];
                Ybase[N_COL * 10 + 5] += Xbase[K * 10 + row_index[k]];
                Ybase[N_COL * 11 + 5] += Xbase[K * 11 + row_index[k]];
                Ybase[N_COL * 12 + 5] += Xbase[K * 12 + row_index[k]];
                Ybase[N_COL * 13 + 5] += Xbase[K * 13 + row_index[k]];
                Ybase[N_COL * 14 + 5] += Xbase[K * 14 + row_index[k]];
                Ybase[N_COL * 15 + 5] += Xbase[K * 15 + row_index[k]];
            }
            for (int k = groupData[12]; k < groupData[13]; k++) {
                Ybase[N_COL * 0 + 5] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 5] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 5] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 5] -= Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 5] -= Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 5] -= Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 5] -= Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 5] -= Xbase[K * 7 + row_index[k]];
                Ybase[N_COL * 8 + 5] -= Xbase[K * 8 + row_index[k]];
                Ybase[N_COL * 9 + 5] -= Xbase[K * 9 + row_index[k]];
                Ybase[N_COL * 10 + 5] -= Xbase[K * 10 + row_index[k]];
                Ybase[N_COL * 11 + 5] -= Xbase[K * 11 + row_index[k]];
                Ybase[N_COL * 12 + 5] -= Xbase[K * 12 + row_index[k]];
                Ybase[N_COL * 13 + 5] -= Xbase[K * 13 + row_index[k]];
                Ybase[N_COL * 14 + 5] -= Xbase[K * 14 + row_index[k]];
                Ybase[N_COL * 15 + 5] -= Xbase[K * 15 + row_index[k]];
            }
            for (int k = groupData[13]; k < groupData[14]; k++) {
                Ybase[N_COL * 0 + 6] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 6] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 6] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 6] += Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 6] += Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 6] += Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 6] += Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 6] += Xbase[K * 7 + row_index[k]];
                Ybase[N_COL * 8 + 6] += Xbase[K * 8 + row_index[k]];
                Ybase[N_COL * 9 + 6] += Xbase[K * 9 + row_index[k]];
                Ybase[N_COL * 10 + 6] += Xbase[K * 10 + row_index[k]];
                Ybase[N_COL * 11 + 6] += Xbase[K * 11 + row_index[k]];
                Ybase[N_COL * 12 + 6] += Xbase[K * 12 + row_index[k]];
                Ybase[N_COL * 13 + 6] += Xbase[K * 13 + row_index[k]];
                Ybase[N_COL * 14 + 6] += Xbase[K * 14 + row_index[k]];
                Ybase[N_COL * 15 + 6] += Xbase[K * 15 + row_index[k]];
            }
            for (int k = groupData[14]; k < groupData[15]; k++) {
                Ybase[N_COL * 0 + 6] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 6] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 6] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 6] -= Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 6] -= Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 6] -= Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 6] -= Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 6] -= Xbase[K * 7 + row_index[k]];
                Ybase[N_COL * 8 + 6] -= Xbase[K * 8 + row_index[k]];
                Ybase[N_COL * 9 + 6] -= Xbase[K * 9 + row_index[k]];
                Ybase[N_COL * 10 + 6] -= Xbase[K * 10 + row_index[k]];
                Ybase[N_COL * 11 + 6] -= Xbase[K * 11 + row_index[k]];
                Ybase[N_COL * 12 + 6] -= Xbase[K * 12 + row_index[k]];
                Ybase[N_COL * 13 + 6] -= Xbase[K * 13 + row_index[k]];
                Ybase[N_COL * 14 + 6] -= Xbase[K * 14 + row_index[k]];
                Ybase[N_COL * 15 + 6] -= Xbase[K * 15 + row_index[k]];
            }
            for (int k = groupData[15]; k < groupData[16]; k++) {
                Ybase[N_COL * 0 + 7] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 7] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 7] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 7] += Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 7] += Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 7] += Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 7] += Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 7] += Xbase[K * 7 + row_index[k]];
                Ybase[N_COL * 8 + 7] += Xbase[K * 8 + row_index[k]];
                Ybase[N_COL * 9 + 7] += Xbase[K * 9 + row_index[k]];
                Ybase[N_COL * 10 + 7] += Xbase[K * 10 + row_index[k]];
                Ybase[N_COL * 11 + 7] += Xbase[K * 11 + row_index[k]];
                Ybase[N_COL * 12 + 7] += Xbase[K * 12 + row_index[k]];
                Ybase[N_COL * 13 + 7] += Xbase[K * 13 + row_index[k]];
                Ybase[N_COL * 14 + 7] += Xbase[K * 14 + row_index[k]];
                Ybase[N_COL * 15 + 7] += Xbase[K * 15 + row_index[k]];
            }
            for (int k = groupData[16]; k < groupData[17]; k++) {
                Ybase[N_COL * 0 + 7] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 7] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 7] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 7] -= Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 7] -= Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 7] -= Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 7] -= Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 7] -= Xbase[K * 7 + row_index[k]];
                Ybase[N_COL * 8 + 7] -= Xbase[K * 8 + row_index[k]];
                Ybase[N_COL * 9 + 7] -= Xbase[K * 9 + row_index[k]];
                Ybase[N_COL * 10 + 7] -= Xbase[K * 10 + row_index[k]];
                Ybase[N_COL * 11 + 7] -= Xbase[K * 11 + row_index[k]];
                Ybase[N_COL * 12 + 7] -= Xbase[K * 12 + row_index[k]];
                Ybase[N_COL * 13 + 7] -= Xbase[K * 13 + row_index[k]];
                Ybase[N_COL * 14 + 7] -= Xbase[K * 14 + row_index[k]];
                Ybase[N_COL * 15 + 7] -= Xbase[K * 15 + row_index[k]];
            }
        }
    }
}

void GEMM_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_1xG16_AVX2_OpenMP(float* X, int32_t* metadata, int32_t* row_index, float* result, int M_ROW, int N_COL, int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 16; j++) {
        for (int i = 0; i < M_ROW; i += 1) {
            float* Xbase = X + i * K;
            const int* groupData  = &metadata[j * 34];
            __m256 res00 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 32) {
                __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
                __m256i indices_p1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
                __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 16));
                __m256i indices_n1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 24));
                __m256 pos00 = _mm256_i32gather_ps(Xbase + K * 0, indices_p0, 4);
                __m256 pos10 = _mm256_i32gather_ps(Xbase + K * 0, indices_p1, 4);
                __m256 neg00 = _mm256_i32gather_ps(Xbase + K * 0, indices_n0, 4);
                __m256 neg10 = _mm256_i32gather_ps(Xbase + K * 0, indices_n1, 4);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
            }
            float* Ybase = result + i * N_COL + j * 16;
            _mm256_store_ps(Ybase + N_COL * 0 + 0, res00);
            _mm256_store_ps(Ybase + N_COL * 0 + 8, res10);
            for (int k = groupData[1]; k < groupData[2]; k++) {
                Ybase[N_COL * 0 + 0] += Xbase[K * 0 + row_index[k]];
            }
            for (int k = groupData[2]; k < groupData[3]; k++) {
                Ybase[N_COL * 0 + 0] -= Xbase[K * 0 + row_index[k]];
            }
            for (int k = groupData[3]; k < groupData[4]; k++) {
                Ybase[N_COL * 0 + 1] += Xbase[K * 0 + row_index[k]];
            }
            for (int k = groupData[4]; k < groupData[5]; k++) {
                Ybase[N_COL * 0 + 1] -= Xbase[K * 0 + row_index[k]];
            }
            for (int k = groupData[5]; k < groupData[6]; k++) {
                Ybase[N_COL * 0 + 2] += Xbase[K * 0 + row_index[k]];
            }
            for (int k = groupData[6]; k < groupData[7]; k++) {
                Ybase[N_COL * 0 + 2] -= Xbase[K * 0 + row_index[k]];
            }
            for (int k = groupData[7]; k < groupData[8]; k++) {
                Ybase[N_COL * 0 + 3] += Xbase[K * 0 + row_index[k]];
            }
            for (int k = groupData[8]; k < groupData[9]; k++) {
                Ybase[N_COL * 0 + 3] -= Xbase[K * 0 + row_index[k]];
            }
            for (int k = groupData[9]; k < groupData[10]; k++) {
                Ybase[N_COL * 0 + 4] += Xbase[K * 0 + row_index[k]];
            }
            for (int k = groupData[10]; k < groupData[11]; k++) {
                Ybase[N_COL * 0 + 4] -= Xbase[K * 0 + row_index[k]];
            }
            for (int k = groupData[11]; k < groupData[12]; k++) {
                Ybase[N_COL * 0 + 5] += Xbase[K * 0 + row_index[k]];
            }
            for (int k = groupData[12]; k < groupData[13]; k++) {
                Ybase[N_COL * 0 + 5] -= Xbase[K * 0 + row_index[k]];
            }
            for (int k = groupData[13]; k < groupData[14]; k++) {
                Ybase[N_COL * 0 + 6] += Xbase[K * 0 + row_index[k]];
            }
            for (int k = groupData[14]; k < groupData[15]; k++) {
                Ybase[N_COL * 0 + 6] -= Xbase[K * 0 + row_index[k]];
            }
            for (int k = groupData[15]; k < groupData[16]; k++) {
                Ybase[N_COL * 0 + 7] += Xbase[K * 0 + row_index[k]];
            }
            for (int k = groupData[16]; k < groupData[17]; k++) {
                Ybase[N_COL * 0 + 7] -= Xbase[K * 0 + row_index[k]];
            }
            for (int k = groupData[17]; k < groupData[18]; k++) {
                Ybase[N_COL * 0 + 8] += Xbase[K * 0 + row_index[k]];
            }
            for (int k = groupData[18]; k < groupData[19]; k++) {
                Ybase[N_COL * 0 + 8] -= Xbase[K * 0 + row_index[k]];
            }
            for (int k = groupData[19]; k < groupData[20]; k++) {
                Ybase[N_COL * 0 + 9] += Xbase[K * 0 + row_index[k]];
            }
            for (int k = groupData[20]; k < groupData[21]; k++) {
                Ybase[N_COL * 0 + 9] -= Xbase[K * 0 + row_index[k]];
            }
            for (int k = groupData[21]; k < groupData[22]; k++) {
                Ybase[N_COL * 0 + 10] += Xbase[K * 0 + row_index[k]];
            }
            for (int k = groupData[22]; k < groupData[23]; k++) {
                Ybase[N_COL * 0 + 10] -= Xbase[K * 0 + row_index[k]];
            }
            for (int k = groupData[23]; k < groupData[24]; k++) {
                Ybase[N_COL * 0 + 11] += Xbase[K * 0 + row_index[k]];
            }
            for (int k = groupData[24]; k < groupData[25]; k++) {
                Ybase[N_COL * 0 + 11] -= Xbase[K * 0 + row_index[k]];
            }
            for (int k = groupData[25]; k < groupData[26]; k++) {
                Ybase[N_COL * 0 + 12] += Xbase[K * 0 + row_index[k]];
            }
            for (int k = groupData[26]; k < groupData[27]; k++) {
                Ybase[N_COL * 0 + 12] -= Xbase[K * 0 + row_index[k]];
            }
            for (int k = groupData[27]; k < groupData[28]; k++) {
                Ybase[N_COL * 0 + 13] += Xbase[K * 0 + row_index[k]];
            }
            for (int k = groupData[28]; k < groupData[29]; k++) {
                Ybase[N_COL * 0 + 13] -= Xbase[K * 0 + row_index[k]];
            }
            for (int k = groupData[29]; k < groupData[30]; k++) {
                Ybase[N_COL * 0 + 14] += Xbase[K * 0 + row_index[k]];
            }
            for (int k = groupData[30]; k < groupData[31]; k++) {
                Ybase[N_COL * 0 + 14] -= Xbase[K * 0 + row_index[k]];
            }
            for (int k = groupData[31]; k < groupData[32]; k++) {
                Ybase[N_COL * 0 + 15] += Xbase[K * 0 + row_index[k]];
            }
            for (int k = groupData[32]; k < groupData[33]; k++) {
                Ybase[N_COL * 0 + 15] -= Xbase[K * 0 + row_index[k]];
            }
        }
    }
}

void GEMM_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_2xG16_AVX2_OpenMP(float* X, int32_t* metadata, int32_t* row_index, float* result, int M_ROW, int N_COL, int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 16; j++) {
        for (int i = 0; i < M_ROW; i += 2) {
            float* Xbase = X + i * K;
            const int* groupData  = &metadata[j * 34];
            __m256 res00 = _mm256_setzero_ps();
            __m256 res01 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res11 = _mm256_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 32) {
                __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
                __m256i indices_p1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
                __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 16));
                __m256i indices_n1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 24));
                __m256 pos00 = _mm256_i32gather_ps(Xbase + K * 0, indices_p0, 4);
                __m256 pos10 = _mm256_i32gather_ps(Xbase + K * 0, indices_p1, 4);
                __m256 pos01 = _mm256_i32gather_ps(Xbase + K * 1, indices_p0, 4);
                __m256 pos11 = _mm256_i32gather_ps(Xbase + K * 1, indices_p1, 4);
                __m256 neg00 = _mm256_i32gather_ps(Xbase + K * 0, indices_n0, 4);
                __m256 neg10 = _mm256_i32gather_ps(Xbase + K * 0, indices_n1, 4);
                __m256 neg01 = _mm256_i32gather_ps(Xbase + K * 1, indices_n0, 4);
                __m256 neg11 = _mm256_i32gather_ps(Xbase + K * 1, indices_n1, 4);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res01 = _mm256_add_ps(res01, _mm256_sub_ps(pos01, neg01));
                res11 = _mm256_add_ps(res11, _mm256_sub_ps(pos11, neg11));
            }
            float* Ybase = result + i * N_COL + j * 16;
            _mm256_store_ps(Ybase + N_COL * 0 + 0, res00);
            _mm256_store_ps(Ybase + N_COL * 0 + 8, res10);
            _mm256_store_ps(Ybase + N_COL * 1 + 0, res01);
            _mm256_store_ps(Ybase + N_COL * 1 + 8, res11);
            for (int k = groupData[1]; k < groupData[2]; k++) {
                Ybase[N_COL * 0 + 0] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 0] += Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[2]; k < groupData[3]; k++) {
                Ybase[N_COL * 0 + 0] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 0] -= Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[3]; k < groupData[4]; k++) {
                Ybase[N_COL * 0 + 1] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 1] += Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[4]; k < groupData[5]; k++) {
                Ybase[N_COL * 0 + 1] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 1] -= Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[5]; k < groupData[6]; k++) {
                Ybase[N_COL * 0 + 2] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 2] += Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[6]; k < groupData[7]; k++) {
                Ybase[N_COL * 0 + 2] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 2] -= Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[7]; k < groupData[8]; k++) {
                Ybase[N_COL * 0 + 3] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 3] += Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[8]; k < groupData[9]; k++) {
                Ybase[N_COL * 0 + 3] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 3] -= Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[9]; k < groupData[10]; k++) {
                Ybase[N_COL * 0 + 4] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 4] += Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[10]; k < groupData[11]; k++) {
                Ybase[N_COL * 0 + 4] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 4] -= Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[11]; k < groupData[12]; k++) {
                Ybase[N_COL * 0 + 5] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 5] += Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[12]; k < groupData[13]; k++) {
                Ybase[N_COL * 0 + 5] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 5] -= Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[13]; k < groupData[14]; k++) {
                Ybase[N_COL * 0 + 6] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 6] += Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[14]; k < groupData[15]; k++) {
                Ybase[N_COL * 0 + 6] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 6] -= Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[15]; k < groupData[16]; k++) {
                Ybase[N_COL * 0 + 7] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 7] += Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[16]; k < groupData[17]; k++) {
                Ybase[N_COL * 0 + 7] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 7] -= Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[17]; k < groupData[18]; k++) {
                Ybase[N_COL * 0 + 8] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 8] += Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[18]; k < groupData[19]; k++) {
                Ybase[N_COL * 0 + 8] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 8] -= Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[19]; k < groupData[20]; k++) {
                Ybase[N_COL * 0 + 9] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 9] += Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[20]; k < groupData[21]; k++) {
                Ybase[N_COL * 0 + 9] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 9] -= Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[21]; k < groupData[22]; k++) {
                Ybase[N_COL * 0 + 10] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 10] += Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[22]; k < groupData[23]; k++) {
                Ybase[N_COL * 0 + 10] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 10] -= Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[23]; k < groupData[24]; k++) {
                Ybase[N_COL * 0 + 11] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 11] += Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[24]; k < groupData[25]; k++) {
                Ybase[N_COL * 0 + 11] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 11] -= Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[25]; k < groupData[26]; k++) {
                Ybase[N_COL * 0 + 12] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 12] += Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[26]; k < groupData[27]; k++) {
                Ybase[N_COL * 0 + 12] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 12] -= Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[27]; k < groupData[28]; k++) {
                Ybase[N_COL * 0 + 13] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 13] += Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[28]; k < groupData[29]; k++) {
                Ybase[N_COL * 0 + 13] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 13] -= Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[29]; k < groupData[30]; k++) {
                Ybase[N_COL * 0 + 14] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 14] += Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[30]; k < groupData[31]; k++) {
                Ybase[N_COL * 0 + 14] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 14] -= Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[31]; k < groupData[32]; k++) {
                Ybase[N_COL * 0 + 15] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 15] += Xbase[K * 1 + row_index[k]];
            }
            for (int k = groupData[32]; k < groupData[33]; k++) {
                Ybase[N_COL * 0 + 15] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 15] -= Xbase[K * 1 + row_index[k]];
            }
        }
    }
}

void GEMM_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_4xG16_AVX2_OpenMP(float* X, int32_t* metadata, int32_t* row_index, float* result, int M_ROW, int N_COL, int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 16; j++) {
        for (int i = 0; i < M_ROW; i += 4) {
            float* Xbase = X + i * K;
            const int* groupData  = &metadata[j * 34];
            __m256 res00 = _mm256_setzero_ps();
            __m256 res01 = _mm256_setzero_ps();
            __m256 res02 = _mm256_setzero_ps();
            __m256 res03 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res11 = _mm256_setzero_ps();
            __m256 res12 = _mm256_setzero_ps();
            __m256 res13 = _mm256_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 32) {
                __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
                __m256i indices_p1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
                __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 16));
                __m256i indices_n1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 24));
                __m256 pos00 = _mm256_i32gather_ps(Xbase + K * 0, indices_p0, 4);
                __m256 pos10 = _mm256_i32gather_ps(Xbase + K * 0, indices_p1, 4);
                __m256 pos01 = _mm256_i32gather_ps(Xbase + K * 1, indices_p0, 4);
                __m256 pos11 = _mm256_i32gather_ps(Xbase + K * 1, indices_p1, 4);
                __m256 pos02 = _mm256_i32gather_ps(Xbase + K * 2, indices_p0, 4);
                __m256 pos12 = _mm256_i32gather_ps(Xbase + K * 2, indices_p1, 4);
                __m256 pos03 = _mm256_i32gather_ps(Xbase + K * 3, indices_p0, 4);
                __m256 pos13 = _mm256_i32gather_ps(Xbase + K * 3, indices_p1, 4);
                __m256 neg00 = _mm256_i32gather_ps(Xbase + K * 0, indices_n0, 4);
                __m256 neg10 = _mm256_i32gather_ps(Xbase + K * 0, indices_n1, 4);
                __m256 neg01 = _mm256_i32gather_ps(Xbase + K * 1, indices_n0, 4);
                __m256 neg11 = _mm256_i32gather_ps(Xbase + K * 1, indices_n1, 4);
                __m256 neg02 = _mm256_i32gather_ps(Xbase + K * 2, indices_n0, 4);
                __m256 neg12 = _mm256_i32gather_ps(Xbase + K * 2, indices_n1, 4);
                __m256 neg03 = _mm256_i32gather_ps(Xbase + K * 3, indices_n0, 4);
                __m256 neg13 = _mm256_i32gather_ps(Xbase + K * 3, indices_n1, 4);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res01 = _mm256_add_ps(res01, _mm256_sub_ps(pos01, neg01));
                res11 = _mm256_add_ps(res11, _mm256_sub_ps(pos11, neg11));
                res02 = _mm256_add_ps(res02, _mm256_sub_ps(pos02, neg02));
                res12 = _mm256_add_ps(res12, _mm256_sub_ps(pos12, neg12));
                res03 = _mm256_add_ps(res03, _mm256_sub_ps(pos03, neg03));
                res13 = _mm256_add_ps(res13, _mm256_sub_ps(pos13, neg13));
            }
            float* Ybase = result + i * N_COL + j * 16;
            _mm256_store_ps(Ybase + N_COL * 0 + 0, res00);
            _mm256_store_ps(Ybase + N_COL * 0 + 8, res10);
            _mm256_store_ps(Ybase + N_COL * 1 + 0, res01);
            _mm256_store_ps(Ybase + N_COL * 1 + 8, res11);
            _mm256_store_ps(Ybase + N_COL * 2 + 0, res02);
            _mm256_store_ps(Ybase + N_COL * 2 + 8, res12);
            _mm256_store_ps(Ybase + N_COL * 3 + 0, res03);
            _mm256_store_ps(Ybase + N_COL * 3 + 8, res13);
            for (int k = groupData[1]; k < groupData[2]; k++) {
                Ybase[N_COL * 0 + 0] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 0] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 0] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 0] += Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[2]; k < groupData[3]; k++) {
                Ybase[N_COL * 0 + 0] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 0] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 0] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 0] -= Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[3]; k < groupData[4]; k++) {
                Ybase[N_COL * 0 + 1] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 1] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 1] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 1] += Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[4]; k < groupData[5]; k++) {
                Ybase[N_COL * 0 + 1] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 1] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 1] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 1] -= Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[5]; k < groupData[6]; k++) {
                Ybase[N_COL * 0 + 2] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 2] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 2] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 2] += Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[6]; k < groupData[7]; k++) {
                Ybase[N_COL * 0 + 2] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 2] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 2] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 2] -= Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[7]; k < groupData[8]; k++) {
                Ybase[N_COL * 0 + 3] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 3] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 3] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 3] += Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[8]; k < groupData[9]; k++) {
                Ybase[N_COL * 0 + 3] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 3] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 3] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 3] -= Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[9]; k < groupData[10]; k++) {
                Ybase[N_COL * 0 + 4] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 4] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 4] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 4] += Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[10]; k < groupData[11]; k++) {
                Ybase[N_COL * 0 + 4] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 4] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 4] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 4] -= Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[11]; k < groupData[12]; k++) {
                Ybase[N_COL * 0 + 5] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 5] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 5] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 5] += Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[12]; k < groupData[13]; k++) {
                Ybase[N_COL * 0 + 5] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 5] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 5] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 5] -= Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[13]; k < groupData[14]; k++) {
                Ybase[N_COL * 0 + 6] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 6] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 6] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 6] += Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[14]; k < groupData[15]; k++) {
                Ybase[N_COL * 0 + 6] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 6] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 6] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 6] -= Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[15]; k < groupData[16]; k++) {
                Ybase[N_COL * 0 + 7] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 7] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 7] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 7] += Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[16]; k < groupData[17]; k++) {
                Ybase[N_COL * 0 + 7] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 7] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 7] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 7] -= Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[17]; k < groupData[18]; k++) {
                Ybase[N_COL * 0 + 8] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 8] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 8] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 8] += Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[18]; k < groupData[19]; k++) {
                Ybase[N_COL * 0 + 8] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 8] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 8] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 8] -= Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[19]; k < groupData[20]; k++) {
                Ybase[N_COL * 0 + 9] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 9] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 9] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 9] += Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[20]; k < groupData[21]; k++) {
                Ybase[N_COL * 0 + 9] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 9] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 9] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 9] -= Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[21]; k < groupData[22]; k++) {
                Ybase[N_COL * 0 + 10] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 10] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 10] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 10] += Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[22]; k < groupData[23]; k++) {
                Ybase[N_COL * 0 + 10] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 10] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 10] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 10] -= Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[23]; k < groupData[24]; k++) {
                Ybase[N_COL * 0 + 11] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 11] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 11] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 11] += Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[24]; k < groupData[25]; k++) {
                Ybase[N_COL * 0 + 11] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 11] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 11] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 11] -= Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[25]; k < groupData[26]; k++) {
                Ybase[N_COL * 0 + 12] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 12] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 12] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 12] += Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[26]; k < groupData[27]; k++) {
                Ybase[N_COL * 0 + 12] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 12] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 12] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 12] -= Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[27]; k < groupData[28]; k++) {
                Ybase[N_COL * 0 + 13] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 13] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 13] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 13] += Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[28]; k < groupData[29]; k++) {
                Ybase[N_COL * 0 + 13] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 13] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 13] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 13] -= Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[29]; k < groupData[30]; k++) {
                Ybase[N_COL * 0 + 14] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 14] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 14] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 14] += Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[30]; k < groupData[31]; k++) {
                Ybase[N_COL * 0 + 14] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 14] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 14] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 14] -= Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[31]; k < groupData[32]; k++) {
                Ybase[N_COL * 0 + 15] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 15] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 15] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 15] += Xbase[K * 3 + row_index[k]];
            }
            for (int k = groupData[32]; k < groupData[33]; k++) {
                Ybase[N_COL * 0 + 15] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 15] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 15] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 15] -= Xbase[K * 3 + row_index[k]];
            }
        }
    }
}

void GEMM_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_8xG16_AVX2_OpenMP(float* X, int32_t* metadata, int32_t* row_index, float* result, int M_ROW, int N_COL, int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 16; j++) {
        for (int i = 0; i < M_ROW; i += 8) {
            float* Xbase = X + i * K;
            const int* groupData  = &metadata[j * 34];
            __m256 res00 = _mm256_setzero_ps();
            __m256 res01 = _mm256_setzero_ps();
            __m256 res02 = _mm256_setzero_ps();
            __m256 res03 = _mm256_setzero_ps();
            __m256 res04 = _mm256_setzero_ps();
            __m256 res05 = _mm256_setzero_ps();
            __m256 res06 = _mm256_setzero_ps();
            __m256 res07 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res11 = _mm256_setzero_ps();
            __m256 res12 = _mm256_setzero_ps();
            __m256 res13 = _mm256_setzero_ps();
            __m256 res14 = _mm256_setzero_ps();
            __m256 res15 = _mm256_setzero_ps();
            __m256 res16 = _mm256_setzero_ps();
            __m256 res17 = _mm256_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 32) {
                __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
                __m256i indices_p1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
                __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 16));
                __m256i indices_n1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 24));
                __m256 pos00 = _mm256_i32gather_ps(Xbase + K * 0, indices_p0, 4);
                __m256 pos10 = _mm256_i32gather_ps(Xbase + K * 0, indices_p1, 4);
                __m256 pos01 = _mm256_i32gather_ps(Xbase + K * 1, indices_p0, 4);
                __m256 pos11 = _mm256_i32gather_ps(Xbase + K * 1, indices_p1, 4);
                __m256 pos02 = _mm256_i32gather_ps(Xbase + K * 2, indices_p0, 4);
                __m256 pos12 = _mm256_i32gather_ps(Xbase + K * 2, indices_p1, 4);
                __m256 pos03 = _mm256_i32gather_ps(Xbase + K * 3, indices_p0, 4);
                __m256 pos13 = _mm256_i32gather_ps(Xbase + K * 3, indices_p1, 4);
                __m256 pos04 = _mm256_i32gather_ps(Xbase + K * 4, indices_p0, 4);
                __m256 pos14 = _mm256_i32gather_ps(Xbase + K * 4, indices_p1, 4);
                __m256 pos05 = _mm256_i32gather_ps(Xbase + K * 5, indices_p0, 4);
                __m256 pos15 = _mm256_i32gather_ps(Xbase + K * 5, indices_p1, 4);
                __m256 pos06 = _mm256_i32gather_ps(Xbase + K * 6, indices_p0, 4);
                __m256 pos16 = _mm256_i32gather_ps(Xbase + K * 6, indices_p1, 4);
                __m256 pos07 = _mm256_i32gather_ps(Xbase + K * 7, indices_p0, 4);
                __m256 pos17 = _mm256_i32gather_ps(Xbase + K * 7, indices_p1, 4);
                __m256 neg00 = _mm256_i32gather_ps(Xbase + K * 0, indices_n0, 4);
                __m256 neg10 = _mm256_i32gather_ps(Xbase + K * 0, indices_n1, 4);
                __m256 neg01 = _mm256_i32gather_ps(Xbase + K * 1, indices_n0, 4);
                __m256 neg11 = _mm256_i32gather_ps(Xbase + K * 1, indices_n1, 4);
                __m256 neg02 = _mm256_i32gather_ps(Xbase + K * 2, indices_n0, 4);
                __m256 neg12 = _mm256_i32gather_ps(Xbase + K * 2, indices_n1, 4);
                __m256 neg03 = _mm256_i32gather_ps(Xbase + K * 3, indices_n0, 4);
                __m256 neg13 = _mm256_i32gather_ps(Xbase + K * 3, indices_n1, 4);
                __m256 neg04 = _mm256_i32gather_ps(Xbase + K * 4, indices_n0, 4);
                __m256 neg14 = _mm256_i32gather_ps(Xbase + K * 4, indices_n1, 4);
                __m256 neg05 = _mm256_i32gather_ps(Xbase + K * 5, indices_n0, 4);
                __m256 neg15 = _mm256_i32gather_ps(Xbase + K * 5, indices_n1, 4);
                __m256 neg06 = _mm256_i32gather_ps(Xbase + K * 6, indices_n0, 4);
                __m256 neg16 = _mm256_i32gather_ps(Xbase + K * 6, indices_n1, 4);
                __m256 neg07 = _mm256_i32gather_ps(Xbase + K * 7, indices_n0, 4);
                __m256 neg17 = _mm256_i32gather_ps(Xbase + K * 7, indices_n1, 4);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res01 = _mm256_add_ps(res01, _mm256_sub_ps(pos01, neg01));
                res11 = _mm256_add_ps(res11, _mm256_sub_ps(pos11, neg11));
                res02 = _mm256_add_ps(res02, _mm256_sub_ps(pos02, neg02));
                res12 = _mm256_add_ps(res12, _mm256_sub_ps(pos12, neg12));
                res03 = _mm256_add_ps(res03, _mm256_sub_ps(pos03, neg03));
                res13 = _mm256_add_ps(res13, _mm256_sub_ps(pos13, neg13));
                res04 = _mm256_add_ps(res04, _mm256_sub_ps(pos04, neg04));
                res14 = _mm256_add_ps(res14, _mm256_sub_ps(pos14, neg14));
                res05 = _mm256_add_ps(res05, _mm256_sub_ps(pos05, neg05));
                res15 = _mm256_add_ps(res15, _mm256_sub_ps(pos15, neg15));
                res06 = _mm256_add_ps(res06, _mm256_sub_ps(pos06, neg06));
                res16 = _mm256_add_ps(res16, _mm256_sub_ps(pos16, neg16));
                res07 = _mm256_add_ps(res07, _mm256_sub_ps(pos07, neg07));
                res17 = _mm256_add_ps(res17, _mm256_sub_ps(pos17, neg17));
            }
            float* Ybase = result + i * N_COL + j * 16;
            _mm256_store_ps(Ybase + N_COL * 0 + 0, res00);
            _mm256_store_ps(Ybase + N_COL * 0 + 8, res10);
            _mm256_store_ps(Ybase + N_COL * 1 + 0, res01);
            _mm256_store_ps(Ybase + N_COL * 1 + 8, res11);
            _mm256_store_ps(Ybase + N_COL * 2 + 0, res02);
            _mm256_store_ps(Ybase + N_COL * 2 + 8, res12);
            _mm256_store_ps(Ybase + N_COL * 3 + 0, res03);
            _mm256_store_ps(Ybase + N_COL * 3 + 8, res13);
            _mm256_store_ps(Ybase + N_COL * 4 + 0, res04);
            _mm256_store_ps(Ybase + N_COL * 4 + 8, res14);
            _mm256_store_ps(Ybase + N_COL * 5 + 0, res05);
            _mm256_store_ps(Ybase + N_COL * 5 + 8, res15);
            _mm256_store_ps(Ybase + N_COL * 6 + 0, res06);
            _mm256_store_ps(Ybase + N_COL * 6 + 8, res16);
            _mm256_store_ps(Ybase + N_COL * 7 + 0, res07);
            _mm256_store_ps(Ybase + N_COL * 7 + 8, res17);
            for (int k = groupData[1]; k < groupData[2]; k++) {
                Ybase[N_COL * 0 + 0] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 0] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 0] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 0] += Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 0] += Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 0] += Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 0] += Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 0] += Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[2]; k < groupData[3]; k++) {
                Ybase[N_COL * 0 + 0] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 0] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 0] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 0] -= Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 0] -= Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 0] -= Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 0] -= Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 0] -= Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[3]; k < groupData[4]; k++) {
                Ybase[N_COL * 0 + 1] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 1] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 1] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 1] += Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 1] += Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 1] += Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 1] += Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 1] += Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[4]; k < groupData[5]; k++) {
                Ybase[N_COL * 0 + 1] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 1] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 1] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 1] -= Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 1] -= Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 1] -= Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 1] -= Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 1] -= Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[5]; k < groupData[6]; k++) {
                Ybase[N_COL * 0 + 2] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 2] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 2] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 2] += Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 2] += Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 2] += Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 2] += Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 2] += Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[6]; k < groupData[7]; k++) {
                Ybase[N_COL * 0 + 2] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 2] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 2] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 2] -= Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 2] -= Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 2] -= Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 2] -= Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 2] -= Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[7]; k < groupData[8]; k++) {
                Ybase[N_COL * 0 + 3] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 3] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 3] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 3] += Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 3] += Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 3] += Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 3] += Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 3] += Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[8]; k < groupData[9]; k++) {
                Ybase[N_COL * 0 + 3] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 3] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 3] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 3] -= Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 3] -= Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 3] -= Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 3] -= Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 3] -= Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[9]; k < groupData[10]; k++) {
                Ybase[N_COL * 0 + 4] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 4] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 4] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 4] += Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 4] += Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 4] += Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 4] += Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 4] += Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[10]; k < groupData[11]; k++) {
                Ybase[N_COL * 0 + 4] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 4] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 4] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 4] -= Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 4] -= Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 4] -= Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 4] -= Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 4] -= Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[11]; k < groupData[12]; k++) {
                Ybase[N_COL * 0 + 5] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 5] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 5] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 5] += Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 5] += Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 5] += Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 5] += Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 5] += Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[12]; k < groupData[13]; k++) {
                Ybase[N_COL * 0 + 5] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 5] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 5] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 5] -= Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 5] -= Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 5] -= Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 5] -= Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 5] -= Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[13]; k < groupData[14]; k++) {
                Ybase[N_COL * 0 + 6] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 6] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 6] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 6] += Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 6] += Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 6] += Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 6] += Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 6] += Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[14]; k < groupData[15]; k++) {
                Ybase[N_COL * 0 + 6] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 6] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 6] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 6] -= Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 6] -= Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 6] -= Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 6] -= Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 6] -= Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[15]; k < groupData[16]; k++) {
                Ybase[N_COL * 0 + 7] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 7] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 7] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 7] += Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 7] += Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 7] += Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 7] += Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 7] += Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[16]; k < groupData[17]; k++) {
                Ybase[N_COL * 0 + 7] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 7] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 7] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 7] -= Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 7] -= Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 7] -= Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 7] -= Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 7] -= Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[17]; k < groupData[18]; k++) {
                Ybase[N_COL * 0 + 8] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 8] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 8] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 8] += Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 8] += Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 8] += Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 8] += Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 8] += Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[18]; k < groupData[19]; k++) {
                Ybase[N_COL * 0 + 8] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 8] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 8] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 8] -= Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 8] -= Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 8] -= Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 8] -= Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 8] -= Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[19]; k < groupData[20]; k++) {
                Ybase[N_COL * 0 + 9] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 9] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 9] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 9] += Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 9] += Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 9] += Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 9] += Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 9] += Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[20]; k < groupData[21]; k++) {
                Ybase[N_COL * 0 + 9] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 9] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 9] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 9] -= Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 9] -= Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 9] -= Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 9] -= Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 9] -= Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[21]; k < groupData[22]; k++) {
                Ybase[N_COL * 0 + 10] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 10] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 10] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 10] += Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 10] += Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 10] += Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 10] += Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 10] += Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[22]; k < groupData[23]; k++) {
                Ybase[N_COL * 0 + 10] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 10] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 10] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 10] -= Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 10] -= Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 10] -= Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 10] -= Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 10] -= Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[23]; k < groupData[24]; k++) {
                Ybase[N_COL * 0 + 11] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 11] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 11] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 11] += Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 11] += Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 11] += Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 11] += Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 11] += Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[24]; k < groupData[25]; k++) {
                Ybase[N_COL * 0 + 11] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 11] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 11] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 11] -= Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 11] -= Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 11] -= Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 11] -= Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 11] -= Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[25]; k < groupData[26]; k++) {
                Ybase[N_COL * 0 + 12] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 12] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 12] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 12] += Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 12] += Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 12] += Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 12] += Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 12] += Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[26]; k < groupData[27]; k++) {
                Ybase[N_COL * 0 + 12] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 12] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 12] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 12] -= Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 12] -= Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 12] -= Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 12] -= Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 12] -= Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[27]; k < groupData[28]; k++) {
                Ybase[N_COL * 0 + 13] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 13] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 13] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 13] += Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 13] += Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 13] += Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 13] += Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 13] += Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[28]; k < groupData[29]; k++) {
                Ybase[N_COL * 0 + 13] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 13] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 13] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 13] -= Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 13] -= Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 13] -= Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 13] -= Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 13] -= Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[29]; k < groupData[30]; k++) {
                Ybase[N_COL * 0 + 14] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 14] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 14] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 14] += Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 14] += Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 14] += Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 14] += Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 14] += Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[30]; k < groupData[31]; k++) {
                Ybase[N_COL * 0 + 14] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 14] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 14] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 14] -= Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 14] -= Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 14] -= Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 14] -= Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 14] -= Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[31]; k < groupData[32]; k++) {
                Ybase[N_COL * 0 + 15] += Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 15] += Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 15] += Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 15] += Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 15] += Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 15] += Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 15] += Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 15] += Xbase[K * 7 + row_index[k]];
            }
            for (int k = groupData[32]; k < groupData[33]; k++) {
                Ybase[N_COL * 0 + 15] -= Xbase[K * 0 + row_index[k]];
                Ybase[N_COL * 1 + 15] -= Xbase[K * 1 + row_index[k]];
                Ybase[N_COL * 2 + 15] -= Xbase[K * 2 + row_index[k]];
                Ybase[N_COL * 3 + 15] -= Xbase[K * 3 + row_index[k]];
                Ybase[N_COL * 4 + 15] -= Xbase[K * 4 + row_index[k]];
                Ybase[N_COL * 5 + 15] -= Xbase[K * 5 + row_index[k]];
                Ybase[N_COL * 6 + 15] -= Xbase[K * 6 + row_index[k]];
                Ybase[N_COL * 7 + 15] -= Xbase[K * 7 + row_index[k]];
            }
        }
    }
}

void GEMM_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_16xG16_AVX2_OpenMP(float* X, int32_t* metadata, int32_t* row_index, float* result, int M_ROW, int N_COL, int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 16; j++) {
        for (int i = 0; i < M_ROW; i += 16) {
            float* Xbase = X + i * K;
            const int* groupData  = &metadata[j * 34];
            __m256 res00 = _mm256_setzero_ps();
            __m256 res01 = _mm256_setzero_ps();
            __m256 res02 = _mm256_setzero_ps();
            __m256 res03 = _mm256_setzero_ps();
            __m256 res04 = _mm256_setzero_ps();
            __m256 res05 = _mm256_setzero_ps();
            __m256 res06 = _mm256_setzero_ps();
            __m256 res07 = _mm256_setzero_ps();
            __m256 res08 = _mm256_setzero_ps();
            __m256 res09 = _mm256_setzero_ps();
            __m256 res010 = _mm256_setzero_ps();
            __m256 res011 = _mm256_setzero_ps();
            __m256 res012 = _mm256_setzero_ps();
            __m256 res013 = _mm256_setzero_ps();
            __m256 res014 = _mm256_setzero_ps();
            __m256 res015 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res11 = _mm256_setzero_ps();
            __m256 res12 = _mm256_setzero_ps();
            __m256 res13 = _mm256_setzero_ps();
            __m256 res14 = _mm256_setzero_ps();
            __m256 res15 = _mm256_setzero_ps();
            __m256 res16 = _mm256_setzero_ps();
            __m256 res17 = _mm256_setzero_ps();
            __m256 res18 = _mm256_setzero_ps();
            __m256 res19 = _mm256_setzero_ps();
            __m256 res110 = _mm256_setzero_ps();
            __m256 res111 = _mm256_setzero_ps();
            __m256 res112 = _mm256_setzero_ps();
            __m256 res113 = _mm256_setzero_ps();
            __m256 res114 = _mm256_setzero_ps();
            __m256 res115 = _mm256_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 32) {
                __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
                __m256i indices_p1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
                __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 16));
                __m256i indices_n1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 24));
                __m256 pos00 = _mm256_i32gather_ps(Xbase + K * 0, indices_p0, 4);
                __m256 pos10 = _mm256_i32gather_ps(Xbase + K * 0, indices_p1, 4);
                __m256 pos01 = _mm256_i32gather_ps(Xbase + K * 1, indices_p0, 4);
                __m256 pos11 = _mm256_i32gather_ps(Xbase + K * 1, indices_p1, 4);
                __m256 pos02 = _mm256_i32gather_ps(Xbase + K * 2, indices_p0, 4);
                __m256 pos12 = _mm256_i32gather_ps(Xbase + K * 2, indices_p1, 4);
                __m256 pos03 = _mm256_i32gather_ps(Xbase + K * 3, indices_p0, 4);
                __m256 pos13 = _mm256_i32gather_ps(Xbase + K * 3, indices_p1, 4);
                __m256 pos04 = _mm256_i32gather_ps(Xbase + K * 4, indices_p0, 4);
                __m256 pos14 = _mm256_i32gather_ps(Xbase + K * 4, indices_p1, 4);
                __m256 pos05 = _mm256_i32gather_ps(Xbase + K * 5, indices_p0, 4);
                __m256 pos15 = _mm256_i32gather_ps(Xbase + K * 5, indices_p1, 4);
                __m256 pos06 = _mm256_i32gather_ps(Xbase + K * 6, indices_p0, 4);
                __m256 pos16 = _mm256_i32gather_ps(Xbase + K * 6, indices_p1, 4);
                __m256 pos07 = _mm256_i32gather_ps(Xbase + K * 7, indices_p0, 4);
                __m256 pos17 = _mm256_i32gather_ps(Xbase + K * 7, indices_p1, 4);
                __m256 pos08 = _mm256_i32gather_ps(Xbase + K * 8, indices_p0, 4);
                __m256 pos18 = _mm256_i32gather_ps(Xbase + K * 8, indices_p1, 4);
                __m256 pos09 = _mm256_i32gather_ps(Xbase + K * 9, indices_p0, 4);
                __m256 pos19 = _mm256_i32gather_ps(Xbase + K * 9, indices_p1, 4);
                __m256 pos010 = _mm256_i32gather_ps(Xbase + K * 10, indices_p0, 4);
                __m256 pos110 = _mm256_i32gather_ps(Xbase + K * 10, indices_p1, 4);
                __m256 pos011 = _mm256_i32gather_ps(Xbase + K * 11, indices_p0, 4);
                __m256 pos111 = _mm256_i32gather_ps(Xbase + K * 11, indices_p1, 4);
                __m256 pos012 = _mm256_i32gather_ps(Xbase + K * 12, indices_p0, 4);
                __m256 pos112 = _mm256_i32gather_ps(Xbase + K * 12, indices_p1, 4);
                __m256 pos013 = _mm256_i32gather_ps(Xbase + K * 13, indices_p0, 4);
                __m256 pos113 = _mm256_i32gather_ps(Xbase + K * 13, indices_p1, 4);
                __m256 pos014 = _mm256_i32gather_ps(Xbase + K * 14, indices_p0, 4);
                __m256 pos114 = _mm256_i32gather_ps(Xbase + K * 14, indices_p1, 4);
                __m256 pos015 = _mm256_i32gather_ps(Xbase + K * 15, indices_p0, 4);
                __m256 pos115 = _mm256_i32gather_ps(Xbase + K * 15, indices_p1, 4);
                __m256 neg00 = _mm256_i32gather_ps(Xbase + K * 0, indices_n0, 4);
                __m256 neg10 = _mm256_i32gather_ps(Xbase + K * 0, indices_n1, 4);
                __m256 neg01 = _mm256_i32gather_ps(Xbase + K * 1, indices_n0, 4);
                __m256 neg11 = _mm256_i32gather_ps(Xbase + K * 1, indices_n1, 4);
                __m256 neg02 = _mm256_i32gather_ps(Xbase + K * 2, indices_n0, 4);
                __m256 neg12 = _mm256_i32gather_ps(Xbase + K * 2, indices_n1, 4);
                __m256 neg03 = _mm256_i32gather_ps(Xbase + K * 3, indices_n0, 4);
                __m256 neg13 = _mm256_i32gather_ps(Xbase + K * 3, indices_n1, 4);
                __m256 neg04 = _mm256_i32gather_ps(Xbase + K * 4, indices_n0, 4);
                __m256 neg14 = _mm256_i32gather_ps(Xbase + K * 4, indices_n1, 4);
                __m256 neg05 = _mm256_i32gather_ps(Xbase + K * 5, indices_n0, 4);
                __m256 neg15 = _mm256_i32gather_ps(Xbase + K * 5, indices_n1, 4);
                __m256 neg06 = _mm256_i32gather_ps(Xbase + K * 6, indices_n0, 4);
                __m256 neg16 = _mm256_i32gather_ps(Xbase + K * 6, indices_n1, 4);
                __m256 neg07 = _mm256_i32gather_ps(Xbase + K * 7, indices_n0, 4);
                __m256 neg17 = _mm256_i32gather_ps(Xbase + K * 7, indices_n1, 4);
                __m256 neg08 = _mm256_i32gather_ps(Xbase + K * 8, indices_n0, 4);
                __m256 neg18 = _mm256_i32gather_ps(Xbase + K * 8, indices_n1, 4);
                __m256 neg09 = _mm256_i32gather_ps(Xbase + K * 9, indices_n0, 4);
                __m256 neg19 = _mm256_i32gather_ps(Xbase + K * 9, indices_n1, 4);
                __m256 neg010 = _mm256_i32gather_ps(Xbase + K * 10, indices_n0, 4);
                __m256 neg110 = _mm256_i32gather_ps(Xbase + K * 10, indices_n1, 4);
                __m256 neg011 = _mm256_i32gather_ps(Xbase + K * 11, indices_n0, 4);
                __m256 neg111 = _mm256_i32gather_ps(Xbase + K * 11, indices_n1, 4);
                __m256 neg012 = _mm256_i32gather_ps(Xbase + K * 12, indices_n0, 4);
                __m256 neg112 = _mm256_i32gather_ps(Xbase + K * 12, indices_n1, 4);
                __m256 neg013 = _mm256_i32gather_ps(Xbase + K * 13, indices_n0, 4);
                __m256 neg113 = _mm256_i32gather_ps(Xbase + K * 13, indices_n1, 4);
                __m256 neg014 = _mm256_i32gather_ps(Xbase + K * 14, indices_n0, 4);
                __m256 neg114 = _mm256_i32gather_ps(Xbase + K * 14, indices_n1, 4);
                __m256 neg015 = _mm256_i32gather_ps(Xbase + K * 15, indices_n0, 4);
                __m256 neg115 = _mm256_i32gather_ps(Xbase + K * 15, indices_n1, 4);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res01 = _mm256_add_ps(res01, _mm256_sub_ps(pos01, neg01));
                res11 = _mm256_add_ps(res11, _mm256_sub_ps(pos11, neg11));
                res02 = _mm256_add_ps(res02, _mm256_sub_ps(pos02, neg02));
                res12 = _mm256_add_ps(res12, _mm256_sub_ps(pos12, neg12));
                res03 = _mm256_add_ps(res03, _mm256_sub_ps(pos03, neg03));
                res13 = _mm256_add_ps(res13, _mm256_sub_ps(pos13, neg13));
                res04 = _mm256_add_ps(res04, _mm256_sub_ps(pos04, neg04));
                res14 = _mm256_add_ps(res14, _mm256_sub_ps(pos14, neg14));
                res05 = _mm256_add_ps(res05, _mm256_sub_ps(pos05, neg05));
                res15 = _mm256_add_ps(res15, _mm256_sub_ps(pos15, neg15));
                res06 = _mm256_add_ps(res06, _mm256_sub_ps(pos06, neg06));
                res16 = _mm256_add_ps(res16, _mm256_sub_ps(pos16, neg16));
                res07 = _mm256_add_ps(res07, _mm256_sub_ps(pos07, neg07));
                res17 = _mm256_add_ps(res17, _mm256_sub_ps(pos17, neg17));
                res08 = _mm256_add_ps(res08, _mm256_sub_ps(pos08, neg08));
                res18 = _mm256_add_ps(res18, _mm256_sub_ps(pos18, neg18));
                res09 = _mm256_add_ps(res09, _mm256_sub_ps(pos09, neg09));
                res19 = _mm256_add_ps(res19, _mm256_sub_ps(pos19, neg19));
                res010 = _mm256_add_ps(res010, _mm256_sub_ps(pos010, neg010));
                res110 = _mm256_add_ps(res110, _mm256_sub_ps(pos110, neg110));
                res011 = _mm256_add_ps(res011, _mm256_sub_ps(pos011, neg011));
                res111 = _mm256_add_ps(res111, _mm256_sub_ps(pos111, neg111));
                res012 = _mm256_add_ps(res012, _mm256_sub_ps(pos012, neg012));
                res112 = _mm256_add_ps(res112, _mm256_sub_ps(pos112, neg112));
                res013 = _mm256_add_ps(res013, _mm256_sub_ps(pos013, neg013));
                res113 = _mm256_add_ps(res113, _mm256_sub_ps(pos113, neg113));
                res014 = _mm256_add_ps(res014, _mm256_sub_ps(pos014, neg014));
                res114 = _mm256_add_ps(res114, _mm256_sub_ps(pos114, neg114));
                res015 = _mm256_add_ps(res015, _mm256_sub_ps(pos015, neg015));
                res115 = _mm256_add_ps(res115, _mm256_sub_ps(pos115, neg115));
            }
            float* Ybase = result + i * N_COL + j * 16;
            _mm256_store_ps(Ybase + N_COL * 0 + 0, res00);
            _mm256_store_ps(Ybase + N_COL * 0 + 8, res10);
            _mm256_store_ps(Ybase + N_COL * 1 + 0, res01);
            _mm256_store_ps(Ybase + N_COL * 1 + 8, res11);
            _mm256_store_ps(Ybase + N_COL * 2 + 0, res02);
            _mm256_store_ps(Ybase + N_COL * 2 + 8, res12);
            _mm256_store_ps(Ybase + N_COL * 3 + 0, res03);
            _mm256_store_ps(Ybase + N_COL * 3 + 8, res13);
            _mm256_store_ps(Ybase + N_COL * 4 + 0, res04);
            _mm256_store_ps(Ybase + N_COL * 4 + 8, res14);
            _mm256_store_ps(Ybase + N_COL * 5 + 0, res05);
            _mm256_store_ps(Ybase + N_COL * 5 + 8, res15);
            _mm256_store_ps(Ybase + N_COL * 6 + 0, res06);
            _mm256_store_ps(Ybase + N_COL * 6 + 8, res16);
            _mm256_store_ps(Ybase + N_COL * 7 + 0, res07);
            _mm256_store_ps(Ybase + N_COL * 7 + 8, res17);
            _mm256_store_ps(Ybase + N_COL * 8 + 0, res08);
            _mm256_store_ps(Ybase + N_COL * 8 + 8, res18);
            _mm256_store_ps(Ybase + N_COL * 9 + 0, res09);
            _mm256_store_ps(Ybase + N_COL * 9 + 8, res19);
            _mm256_store_ps(Ybase + N_COL * 10 + 0, res010);
            _mm256_store_ps(Ybase + N_COL * 10 + 8, res110);
            _mm256_store_ps(Ybase + N_COL * 11 + 0, res011);
            _mm256_store_ps(Ybase + N_COL * 11 + 8, res111);
            _mm256_store_ps(Ybase + N_COL * 12 + 0, res012);
            _mm256_store_ps(Ybase + N_COL * 12 + 8, res112);
            _mm256_store_ps(Ybase + N_COL * 13 + 0, res013);
            _mm256_store_ps(Ybase + N_COL * 13 + 8, res113);
            _mm256_store_ps(Ybase + N_COL * 14 + 0, res014);
            _mm256_store_ps(Ybase + N_COL * 14 + 8, res114);
            _mm256_store_ps(Ybase + N_COL * 15 + 0, res015);
            _mm256_store_ps(Ybase + N_COL * 15 + 8, res115);
            for (int g = 0; g < 16; g++) {
                for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++) {
                    Ybase[N_COL * 0 + g] += Xbase[K * 0 + row_index[k]];
                    Ybase[N_COL * 1 + g] += Xbase[K * 1 + row_index[k]];
                    Ybase[N_COL * 2 + g] += Xbase[K * 2 + row_index[k]];
                    Ybase[N_COL * 3 + g] += Xbase[K * 3 + row_index[k]];
                    Ybase[N_COL * 4 + g] += Xbase[K * 4 + row_index[k]];
                    Ybase[N_COL * 5 + g] += Xbase[K * 5 + row_index[k]];
                    Ybase[N_COL * 6 + g] += Xbase[K * 6 + row_index[k]];
                    Ybase[N_COL * 7 + g] += Xbase[K * 7 + row_index[k]];
                    Ybase[N_COL * 8 + g] += Xbase[K * 8 + row_index[k]];
                    Ybase[N_COL * 9 + g] += Xbase[K * 9 + row_index[k]];
                    Ybase[N_COL * 10 + g] += Xbase[K * 10 + row_index[k]];
                    Ybase[N_COL * 11 + g] += Xbase[K * 11 + row_index[k]];
                    Ybase[N_COL * 12 + g] += Xbase[K * 12 + row_index[k]];
                    Ybase[N_COL * 13 + g] += Xbase[K * 13 + row_index[k]];
                    Ybase[N_COL * 14 + g] += Xbase[K * 14 + row_index[k]];
                    Ybase[N_COL * 15 + g] += Xbase[K * 15 + row_index[k]];
                }
                for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++) {
                    Ybase[N_COL * 0 + g] -= Xbase[K * 0 + row_index[k]];
                    Ybase[N_COL * 1 + g] -= Xbase[K * 1 + row_index[k]];
                    Ybase[N_COL * 2 + g] -= Xbase[K * 2 + row_index[k]];
                    Ybase[N_COL * 3 + g] -= Xbase[K * 3 + row_index[k]];
                    Ybase[N_COL * 4 + g] -= Xbase[K * 4 + row_index[k]];
                    Ybase[N_COL * 5 + g] -= Xbase[K * 5 + row_index[k]];
                    Ybase[N_COL * 6 + g] -= Xbase[K * 6 + row_index[k]];
                    Ybase[N_COL * 7 + g] -= Xbase[K * 7 + row_index[k]];
                    Ybase[N_COL * 8 + g] -= Xbase[K * 8 + row_index[k]];
                    Ybase[N_COL * 9 + g] -= Xbase[K * 9 + row_index[k]];
                    Ybase[N_COL * 10 + g] -= Xbase[K * 10 + row_index[k]];
                    Ybase[N_COL * 11 + g] -= Xbase[K * 11 + row_index[k]];
                    Ybase[N_COL * 12 + g] -= Xbase[K * 12 + row_index[k]];
                    Ybase[N_COL * 13 + g] -= Xbase[K * 13 + row_index[k]];
                    Ybase[N_COL * 14 + g] -= Xbase[K * 14 + row_index[k]];
                    Ybase[N_COL * 15 + g] -= Xbase[K * 15 + row_index[k]];
                }
            }
        }
    }
}

void GEMM_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_1xG32_AVX2_OpenMP(float* X, int32_t* metadata, int32_t* row_index, float* result, int M_ROW, int N_COL, int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 32; j++) {
        for (int i = 0; i < M_ROW; i += 1) {
            float* Xbase = X + i * K;
            const int* groupData  = &metadata[j * 66];
            __m256 res00 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res20 = _mm256_setzero_ps();
            __m256 res30 = _mm256_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 64) {
                __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
                __m256i indices_p1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
                __m256i indices_p2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 16));
                __m256i indices_p3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 24));
                __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 32));
                __m256i indices_n1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 40));
                __m256i indices_n2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 48));
                __m256i indices_n3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 56));
                __m256 pos00 = _mm256_i32gather_ps(Xbase + K * 0, indices_p0, 4);
                __m256 pos10 = _mm256_i32gather_ps(Xbase + K * 0, indices_p1, 4);
                __m256 pos20 = _mm256_i32gather_ps(Xbase + K * 0, indices_p2, 4);
                __m256 pos30 = _mm256_i32gather_ps(Xbase + K * 0, indices_p3, 4);
                __m256 neg00 = _mm256_i32gather_ps(Xbase + K * 0, indices_n0, 4);
                __m256 neg10 = _mm256_i32gather_ps(Xbase + K * 0, indices_n1, 4);
                __m256 neg20 = _mm256_i32gather_ps(Xbase + K * 0, indices_n2, 4);
                __m256 neg30 = _mm256_i32gather_ps(Xbase + K * 0, indices_n3, 4);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res20 = _mm256_add_ps(res20, _mm256_sub_ps(pos20, neg20));
                res30 = _mm256_add_ps(res30, _mm256_sub_ps(pos30, neg30));
            }
            float* Ybase = result + i * N_COL + j * 32;
            _mm256_store_ps(Ybase + N_COL * 0 + 0, res00);
            _mm256_store_ps(Ybase + N_COL * 0 + 8, res10);
            _mm256_store_ps(Ybase + N_COL * 0 + 16, res20);
            _mm256_store_ps(Ybase + N_COL * 0 + 24, res30);
            for (int g = 0; g < 32; g++) {
                for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++) {
                    Ybase[N_COL * 0 + g] += Xbase[K * 0 + row_index[k]];
                }
                for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++) {
                    Ybase[N_COL * 0 + g] -= Xbase[K * 0 + row_index[k]];
                }
            }
        }
    }
}

void GEMM_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_2xG32_AVX2_OpenMP(float* X, int32_t* metadata, int32_t* row_index, float* result, int M_ROW, int N_COL, int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 32; j++) {
        for (int i = 0; i < M_ROW; i += 2) {
            float* Xbase = X + i * K;
            const int* groupData  = &metadata[j * 66];
            __m256 res00 = _mm256_setzero_ps();
            __m256 res01 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res11 = _mm256_setzero_ps();
            __m256 res20 = _mm256_setzero_ps();
            __m256 res21 = _mm256_setzero_ps();
            __m256 res30 = _mm256_setzero_ps();
            __m256 res31 = _mm256_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 64) {
                __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
                __m256i indices_p1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
                __m256i indices_p2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 16));
                __m256i indices_p3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 24));
                __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 32));
                __m256i indices_n1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 40));
                __m256i indices_n2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 48));
                __m256i indices_n3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 56));
                __m256 pos00 = _mm256_i32gather_ps(Xbase + K * 0, indices_p0, 4);
                __m256 pos10 = _mm256_i32gather_ps(Xbase + K * 0, indices_p1, 4);
                __m256 pos20 = _mm256_i32gather_ps(Xbase + K * 0, indices_p2, 4);
                __m256 pos30 = _mm256_i32gather_ps(Xbase + K * 0, indices_p3, 4);
                __m256 pos01 = _mm256_i32gather_ps(Xbase + K * 1, indices_p0, 4);
                __m256 pos11 = _mm256_i32gather_ps(Xbase + K * 1, indices_p1, 4);
                __m256 pos21 = _mm256_i32gather_ps(Xbase + K * 1, indices_p2, 4);
                __m256 pos31 = _mm256_i32gather_ps(Xbase + K * 1, indices_p3, 4);
                __m256 neg00 = _mm256_i32gather_ps(Xbase + K * 0, indices_n0, 4);
                __m256 neg10 = _mm256_i32gather_ps(Xbase + K * 0, indices_n1, 4);
                __m256 neg20 = _mm256_i32gather_ps(Xbase + K * 0, indices_n2, 4);
                __m256 neg30 = _mm256_i32gather_ps(Xbase + K * 0, indices_n3, 4);
                __m256 neg01 = _mm256_i32gather_ps(Xbase + K * 1, indices_n0, 4);
                __m256 neg11 = _mm256_i32gather_ps(Xbase + K * 1, indices_n1, 4);
                __m256 neg21 = _mm256_i32gather_ps(Xbase + K * 1, indices_n2, 4);
                __m256 neg31 = _mm256_i32gather_ps(Xbase + K * 1, indices_n3, 4);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res20 = _mm256_add_ps(res20, _mm256_sub_ps(pos20, neg20));
                res30 = _mm256_add_ps(res30, _mm256_sub_ps(pos30, neg30));
                res01 = _mm256_add_ps(res01, _mm256_sub_ps(pos01, neg01));
                res11 = _mm256_add_ps(res11, _mm256_sub_ps(pos11, neg11));
                res21 = _mm256_add_ps(res21, _mm256_sub_ps(pos21, neg21));
                res31 = _mm256_add_ps(res31, _mm256_sub_ps(pos31, neg31));
            }
            float* Ybase = result + i * N_COL + j * 32;
            _mm256_store_ps(Ybase + N_COL * 0 + 0, res00);
            _mm256_store_ps(Ybase + N_COL * 0 + 8, res10);
            _mm256_store_ps(Ybase + N_COL * 0 + 16, res20);
            _mm256_store_ps(Ybase + N_COL * 0 + 24, res30);
            _mm256_store_ps(Ybase + N_COL * 1 + 0, res01);
            _mm256_store_ps(Ybase + N_COL * 1 + 8, res11);
            _mm256_store_ps(Ybase + N_COL * 1 + 16, res21);
            _mm256_store_ps(Ybase + N_COL * 1 + 24, res31);
            for (int g = 0; g < 32; g++) {
                for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++) {
                    Ybase[N_COL * 0 + g] += Xbase[K * 0 + row_index[k]];
                    Ybase[N_COL * 1 + g] += Xbase[K * 1 + row_index[k]];
                }
                for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++) {
                    Ybase[N_COL * 0 + g] -= Xbase[K * 0 + row_index[k]];
                    Ybase[N_COL * 1 + g] -= Xbase[K * 1 + row_index[k]];
                }
            }
        }
    }
}

void GEMM_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_4xG32_AVX2_OpenMP(float* X, int32_t* metadata, int32_t* row_index, float* result, int M_ROW, int N_COL, int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 32; j++) {
        for (int i = 0; i < M_ROW; i += 4) {
            float* Xbase = X + i * K;
            const int* groupData  = &metadata[j * 66];
            __m256 res00 = _mm256_setzero_ps();
            __m256 res01 = _mm256_setzero_ps();
            __m256 res02 = _mm256_setzero_ps();
            __m256 res03 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res11 = _mm256_setzero_ps();
            __m256 res12 = _mm256_setzero_ps();
            __m256 res13 = _mm256_setzero_ps();
            __m256 res20 = _mm256_setzero_ps();
            __m256 res21 = _mm256_setzero_ps();
            __m256 res22 = _mm256_setzero_ps();
            __m256 res23 = _mm256_setzero_ps();
            __m256 res30 = _mm256_setzero_ps();
            __m256 res31 = _mm256_setzero_ps();
            __m256 res32 = _mm256_setzero_ps();
            __m256 res33 = _mm256_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 64) {
                __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
                __m256i indices_p1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
                __m256i indices_p2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 16));
                __m256i indices_p3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 24));
                __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 32));
                __m256i indices_n1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 40));
                __m256i indices_n2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 48));
                __m256i indices_n3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 56));
                __m256 pos00 = _mm256_i32gather_ps(Xbase + K * 0, indices_p0, 4);
                __m256 pos10 = _mm256_i32gather_ps(Xbase + K * 0, indices_p1, 4);
                __m256 pos20 = _mm256_i32gather_ps(Xbase + K * 0, indices_p2, 4);
                __m256 pos30 = _mm256_i32gather_ps(Xbase + K * 0, indices_p3, 4);
                __m256 pos01 = _mm256_i32gather_ps(Xbase + K * 1, indices_p0, 4);
                __m256 pos11 = _mm256_i32gather_ps(Xbase + K * 1, indices_p1, 4);
                __m256 pos21 = _mm256_i32gather_ps(Xbase + K * 1, indices_p2, 4);
                __m256 pos31 = _mm256_i32gather_ps(Xbase + K * 1, indices_p3, 4);
                __m256 pos02 = _mm256_i32gather_ps(Xbase + K * 2, indices_p0, 4);
                __m256 pos12 = _mm256_i32gather_ps(Xbase + K * 2, indices_p1, 4);
                __m256 pos22 = _mm256_i32gather_ps(Xbase + K * 2, indices_p2, 4);
                __m256 pos32 = _mm256_i32gather_ps(Xbase + K * 2, indices_p3, 4);
                __m256 pos03 = _mm256_i32gather_ps(Xbase + K * 3, indices_p0, 4);
                __m256 pos13 = _mm256_i32gather_ps(Xbase + K * 3, indices_p1, 4);
                __m256 pos23 = _mm256_i32gather_ps(Xbase + K * 3, indices_p2, 4);
                __m256 pos33 = _mm256_i32gather_ps(Xbase + K * 3, indices_p3, 4);
                __m256 neg00 = _mm256_i32gather_ps(Xbase + K * 0, indices_n0, 4);
                __m256 neg10 = _mm256_i32gather_ps(Xbase + K * 0, indices_n1, 4);
                __m256 neg20 = _mm256_i32gather_ps(Xbase + K * 0, indices_n2, 4);
                __m256 neg30 = _mm256_i32gather_ps(Xbase + K * 0, indices_n3, 4);
                __m256 neg01 = _mm256_i32gather_ps(Xbase + K * 1, indices_n0, 4);
                __m256 neg11 = _mm256_i32gather_ps(Xbase + K * 1, indices_n1, 4);
                __m256 neg21 = _mm256_i32gather_ps(Xbase + K * 1, indices_n2, 4);
                __m256 neg31 = _mm256_i32gather_ps(Xbase + K * 1, indices_n3, 4);
                __m256 neg02 = _mm256_i32gather_ps(Xbase + K * 2, indices_n0, 4);
                __m256 neg12 = _mm256_i32gather_ps(Xbase + K * 2, indices_n1, 4);
                __m256 neg22 = _mm256_i32gather_ps(Xbase + K * 2, indices_n2, 4);
                __m256 neg32 = _mm256_i32gather_ps(Xbase + K * 2, indices_n3, 4);
                __m256 neg03 = _mm256_i32gather_ps(Xbase + K * 3, indices_n0, 4);
                __m256 neg13 = _mm256_i32gather_ps(Xbase + K * 3, indices_n1, 4);
                __m256 neg23 = _mm256_i32gather_ps(Xbase + K * 3, indices_n2, 4);
                __m256 neg33 = _mm256_i32gather_ps(Xbase + K * 3, indices_n3, 4);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res20 = _mm256_add_ps(res20, _mm256_sub_ps(pos20, neg20));
                res30 = _mm256_add_ps(res30, _mm256_sub_ps(pos30, neg30));
                res01 = _mm256_add_ps(res01, _mm256_sub_ps(pos01, neg01));
                res11 = _mm256_add_ps(res11, _mm256_sub_ps(pos11, neg11));
                res21 = _mm256_add_ps(res21, _mm256_sub_ps(pos21, neg21));
                res31 = _mm256_add_ps(res31, _mm256_sub_ps(pos31, neg31));
                res02 = _mm256_add_ps(res02, _mm256_sub_ps(pos02, neg02));
                res12 = _mm256_add_ps(res12, _mm256_sub_ps(pos12, neg12));
                res22 = _mm256_add_ps(res22, _mm256_sub_ps(pos22, neg22));
                res32 = _mm256_add_ps(res32, _mm256_sub_ps(pos32, neg32));
                res03 = _mm256_add_ps(res03, _mm256_sub_ps(pos03, neg03));
                res13 = _mm256_add_ps(res13, _mm256_sub_ps(pos13, neg13));
                res23 = _mm256_add_ps(res23, _mm256_sub_ps(pos23, neg23));
                res33 = _mm256_add_ps(res33, _mm256_sub_ps(pos33, neg33));
            }
            float* Ybase = result + i * N_COL + j * 32;
            _mm256_store_ps(Ybase + N_COL * 0 + 0, res00);
            _mm256_store_ps(Ybase + N_COL * 0 + 8, res10);
            _mm256_store_ps(Ybase + N_COL * 0 + 16, res20);
            _mm256_store_ps(Ybase + N_COL * 0 + 24, res30);
            _mm256_store_ps(Ybase + N_COL * 1 + 0, res01);
            _mm256_store_ps(Ybase + N_COL * 1 + 8, res11);
            _mm256_store_ps(Ybase + N_COL * 1 + 16, res21);
            _mm256_store_ps(Ybase + N_COL * 1 + 24, res31);
            _mm256_store_ps(Ybase + N_COL * 2 + 0, res02);
            _mm256_store_ps(Ybase + N_COL * 2 + 8, res12);
            _mm256_store_ps(Ybase + N_COL * 2 + 16, res22);
            _mm256_store_ps(Ybase + N_COL * 2 + 24, res32);
            _mm256_store_ps(Ybase + N_COL * 3 + 0, res03);
            _mm256_store_ps(Ybase + N_COL * 3 + 8, res13);
            _mm256_store_ps(Ybase + N_COL * 3 + 16, res23);
            _mm256_store_ps(Ybase + N_COL * 3 + 24, res33);
            for (int g = 0; g < 32; g++) {
                for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++) {
                    Ybase[N_COL * 0 + g] += Xbase[K * 0 + row_index[k]];
                    Ybase[N_COL * 1 + g] += Xbase[K * 1 + row_index[k]];
                    Ybase[N_COL * 2 + g] += Xbase[K * 2 + row_index[k]];
                    Ybase[N_COL * 3 + g] += Xbase[K * 3 + row_index[k]];
                }
                for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++) {
                    Ybase[N_COL * 0 + g] -= Xbase[K * 0 + row_index[k]];
                    Ybase[N_COL * 1 + g] -= Xbase[K * 1 + row_index[k]];
                    Ybase[N_COL * 2 + g] -= Xbase[K * 2 + row_index[k]];
                    Ybase[N_COL * 3 + g] -= Xbase[K * 3 + row_index[k]];
                }
            }
        }
    }
}

void GEMM_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_8xG32_AVX2_OpenMP(float* X, int32_t* metadata, int32_t* row_index, float* result, int M_ROW, int N_COL, int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 32; j++) {
        for (int i = 0; i < M_ROW; i += 8) {
            float* Xbase = X + i * K;
            const int* groupData  = &metadata[j * 66];
            __m256 res00 = _mm256_setzero_ps();
            __m256 res01 = _mm256_setzero_ps();
            __m256 res02 = _mm256_setzero_ps();
            __m256 res03 = _mm256_setzero_ps();
            __m256 res04 = _mm256_setzero_ps();
            __m256 res05 = _mm256_setzero_ps();
            __m256 res06 = _mm256_setzero_ps();
            __m256 res07 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res11 = _mm256_setzero_ps();
            __m256 res12 = _mm256_setzero_ps();
            __m256 res13 = _mm256_setzero_ps();
            __m256 res14 = _mm256_setzero_ps();
            __m256 res15 = _mm256_setzero_ps();
            __m256 res16 = _mm256_setzero_ps();
            __m256 res17 = _mm256_setzero_ps();
            __m256 res20 = _mm256_setzero_ps();
            __m256 res21 = _mm256_setzero_ps();
            __m256 res22 = _mm256_setzero_ps();
            __m256 res23 = _mm256_setzero_ps();
            __m256 res24 = _mm256_setzero_ps();
            __m256 res25 = _mm256_setzero_ps();
            __m256 res26 = _mm256_setzero_ps();
            __m256 res27 = _mm256_setzero_ps();
            __m256 res30 = _mm256_setzero_ps();
            __m256 res31 = _mm256_setzero_ps();
            __m256 res32 = _mm256_setzero_ps();
            __m256 res33 = _mm256_setzero_ps();
            __m256 res34 = _mm256_setzero_ps();
            __m256 res35 = _mm256_setzero_ps();
            __m256 res36 = _mm256_setzero_ps();
            __m256 res37 = _mm256_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 64) {
                __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
                __m256i indices_p1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
                __m256i indices_p2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 16));
                __m256i indices_p3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 24));
                __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 32));
                __m256i indices_n1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 40));
                __m256i indices_n2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 48));
                __m256i indices_n3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 56));
                __m256 pos00 = _mm256_i32gather_ps(Xbase + K * 0, indices_p0, 4);
                __m256 pos10 = _mm256_i32gather_ps(Xbase + K * 0, indices_p1, 4);
                __m256 pos20 = _mm256_i32gather_ps(Xbase + K * 0, indices_p2, 4);
                __m256 pos30 = _mm256_i32gather_ps(Xbase + K * 0, indices_p3, 4);
                __m256 pos01 = _mm256_i32gather_ps(Xbase + K * 1, indices_p0, 4);
                __m256 pos11 = _mm256_i32gather_ps(Xbase + K * 1, indices_p1, 4);
                __m256 pos21 = _mm256_i32gather_ps(Xbase + K * 1, indices_p2, 4);
                __m256 pos31 = _mm256_i32gather_ps(Xbase + K * 1, indices_p3, 4);
                __m256 pos02 = _mm256_i32gather_ps(Xbase + K * 2, indices_p0, 4);
                __m256 pos12 = _mm256_i32gather_ps(Xbase + K * 2, indices_p1, 4);
                __m256 pos22 = _mm256_i32gather_ps(Xbase + K * 2, indices_p2, 4);
                __m256 pos32 = _mm256_i32gather_ps(Xbase + K * 2, indices_p3, 4);
                __m256 pos03 = _mm256_i32gather_ps(Xbase + K * 3, indices_p0, 4);
                __m256 pos13 = _mm256_i32gather_ps(Xbase + K * 3, indices_p1, 4);
                __m256 pos23 = _mm256_i32gather_ps(Xbase + K * 3, indices_p2, 4);
                __m256 pos33 = _mm256_i32gather_ps(Xbase + K * 3, indices_p3, 4);
                __m256 pos04 = _mm256_i32gather_ps(Xbase + K * 4, indices_p0, 4);
                __m256 pos14 = _mm256_i32gather_ps(Xbase + K * 4, indices_p1, 4);
                __m256 pos24 = _mm256_i32gather_ps(Xbase + K * 4, indices_p2, 4);
                __m256 pos34 = _mm256_i32gather_ps(Xbase + K * 4, indices_p3, 4);
                __m256 pos05 = _mm256_i32gather_ps(Xbase + K * 5, indices_p0, 4);
                __m256 pos15 = _mm256_i32gather_ps(Xbase + K * 5, indices_p1, 4);
                __m256 pos25 = _mm256_i32gather_ps(Xbase + K * 5, indices_p2, 4);
                __m256 pos35 = _mm256_i32gather_ps(Xbase + K * 5, indices_p3, 4);
                __m256 pos06 = _mm256_i32gather_ps(Xbase + K * 6, indices_p0, 4);
                __m256 pos16 = _mm256_i32gather_ps(Xbase + K * 6, indices_p1, 4);
                __m256 pos26 = _mm256_i32gather_ps(Xbase + K * 6, indices_p2, 4);
                __m256 pos36 = _mm256_i32gather_ps(Xbase + K * 6, indices_p3, 4);
                __m256 pos07 = _mm256_i32gather_ps(Xbase + K * 7, indices_p0, 4);
                __m256 pos17 = _mm256_i32gather_ps(Xbase + K * 7, indices_p1, 4);
                __m256 pos27 = _mm256_i32gather_ps(Xbase + K * 7, indices_p2, 4);
                __m256 pos37 = _mm256_i32gather_ps(Xbase + K * 7, indices_p3, 4);
                __m256 neg00 = _mm256_i32gather_ps(Xbase + K * 0, indices_n0, 4);
                __m256 neg10 = _mm256_i32gather_ps(Xbase + K * 0, indices_n1, 4);
                __m256 neg20 = _mm256_i32gather_ps(Xbase + K * 0, indices_n2, 4);
                __m256 neg30 = _mm256_i32gather_ps(Xbase + K * 0, indices_n3, 4);
                __m256 neg01 = _mm256_i32gather_ps(Xbase + K * 1, indices_n0, 4);
                __m256 neg11 = _mm256_i32gather_ps(Xbase + K * 1, indices_n1, 4);
                __m256 neg21 = _mm256_i32gather_ps(Xbase + K * 1, indices_n2, 4);
                __m256 neg31 = _mm256_i32gather_ps(Xbase + K * 1, indices_n3, 4);
                __m256 neg02 = _mm256_i32gather_ps(Xbase + K * 2, indices_n0, 4);
                __m256 neg12 = _mm256_i32gather_ps(Xbase + K * 2, indices_n1, 4);
                __m256 neg22 = _mm256_i32gather_ps(Xbase + K * 2, indices_n2, 4);
                __m256 neg32 = _mm256_i32gather_ps(Xbase + K * 2, indices_n3, 4);
                __m256 neg03 = _mm256_i32gather_ps(Xbase + K * 3, indices_n0, 4);
                __m256 neg13 = _mm256_i32gather_ps(Xbase + K * 3, indices_n1, 4);
                __m256 neg23 = _mm256_i32gather_ps(Xbase + K * 3, indices_n2, 4);
                __m256 neg33 = _mm256_i32gather_ps(Xbase + K * 3, indices_n3, 4);
                __m256 neg04 = _mm256_i32gather_ps(Xbase + K * 4, indices_n0, 4);
                __m256 neg14 = _mm256_i32gather_ps(Xbase + K * 4, indices_n1, 4);
                __m256 neg24 = _mm256_i32gather_ps(Xbase + K * 4, indices_n2, 4);
                __m256 neg34 = _mm256_i32gather_ps(Xbase + K * 4, indices_n3, 4);
                __m256 neg05 = _mm256_i32gather_ps(Xbase + K * 5, indices_n0, 4);
                __m256 neg15 = _mm256_i32gather_ps(Xbase + K * 5, indices_n1, 4);
                __m256 neg25 = _mm256_i32gather_ps(Xbase + K * 5, indices_n2, 4);
                __m256 neg35 = _mm256_i32gather_ps(Xbase + K * 5, indices_n3, 4);
                __m256 neg06 = _mm256_i32gather_ps(Xbase + K * 6, indices_n0, 4);
                __m256 neg16 = _mm256_i32gather_ps(Xbase + K * 6, indices_n1, 4);
                __m256 neg26 = _mm256_i32gather_ps(Xbase + K * 6, indices_n2, 4);
                __m256 neg36 = _mm256_i32gather_ps(Xbase + K * 6, indices_n3, 4);
                __m256 neg07 = _mm256_i32gather_ps(Xbase + K * 7, indices_n0, 4);
                __m256 neg17 = _mm256_i32gather_ps(Xbase + K * 7, indices_n1, 4);
                __m256 neg27 = _mm256_i32gather_ps(Xbase + K * 7, indices_n2, 4);
                __m256 neg37 = _mm256_i32gather_ps(Xbase + K * 7, indices_n3, 4);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res20 = _mm256_add_ps(res20, _mm256_sub_ps(pos20, neg20));
                res30 = _mm256_add_ps(res30, _mm256_sub_ps(pos30, neg30));
                res01 = _mm256_add_ps(res01, _mm256_sub_ps(pos01, neg01));
                res11 = _mm256_add_ps(res11, _mm256_sub_ps(pos11, neg11));
                res21 = _mm256_add_ps(res21, _mm256_sub_ps(pos21, neg21));
                res31 = _mm256_add_ps(res31, _mm256_sub_ps(pos31, neg31));
                res02 = _mm256_add_ps(res02, _mm256_sub_ps(pos02, neg02));
                res12 = _mm256_add_ps(res12, _mm256_sub_ps(pos12, neg12));
                res22 = _mm256_add_ps(res22, _mm256_sub_ps(pos22, neg22));
                res32 = _mm256_add_ps(res32, _mm256_sub_ps(pos32, neg32));
                res03 = _mm256_add_ps(res03, _mm256_sub_ps(pos03, neg03));
                res13 = _mm256_add_ps(res13, _mm256_sub_ps(pos13, neg13));
                res23 = _mm256_add_ps(res23, _mm256_sub_ps(pos23, neg23));
                res33 = _mm256_add_ps(res33, _mm256_sub_ps(pos33, neg33));
                res04 = _mm256_add_ps(res04, _mm256_sub_ps(pos04, neg04));
                res14 = _mm256_add_ps(res14, _mm256_sub_ps(pos14, neg14));
                res24 = _mm256_add_ps(res24, _mm256_sub_ps(pos24, neg24));
                res34 = _mm256_add_ps(res34, _mm256_sub_ps(pos34, neg34));
                res05 = _mm256_add_ps(res05, _mm256_sub_ps(pos05, neg05));
                res15 = _mm256_add_ps(res15, _mm256_sub_ps(pos15, neg15));
                res25 = _mm256_add_ps(res25, _mm256_sub_ps(pos25, neg25));
                res35 = _mm256_add_ps(res35, _mm256_sub_ps(pos35, neg35));
                res06 = _mm256_add_ps(res06, _mm256_sub_ps(pos06, neg06));
                res16 = _mm256_add_ps(res16, _mm256_sub_ps(pos16, neg16));
                res26 = _mm256_add_ps(res26, _mm256_sub_ps(pos26, neg26));
                res36 = _mm256_add_ps(res36, _mm256_sub_ps(pos36, neg36));
                res07 = _mm256_add_ps(res07, _mm256_sub_ps(pos07, neg07));
                res17 = _mm256_add_ps(res17, _mm256_sub_ps(pos17, neg17));
                res27 = _mm256_add_ps(res27, _mm256_sub_ps(pos27, neg27));
                res37 = _mm256_add_ps(res37, _mm256_sub_ps(pos37, neg37));
            }
            float* Ybase = result + i * N_COL + j * 32;
            _mm256_store_ps(Ybase + N_COL * 0 + 0, res00);
            _mm256_store_ps(Ybase + N_COL * 0 + 8, res10);
            _mm256_store_ps(Ybase + N_COL * 0 + 16, res20);
            _mm256_store_ps(Ybase + N_COL * 0 + 24, res30);
            _mm256_store_ps(Ybase + N_COL * 1 + 0, res01);
            _mm256_store_ps(Ybase + N_COL * 1 + 8, res11);
            _mm256_store_ps(Ybase + N_COL * 1 + 16, res21);
            _mm256_store_ps(Ybase + N_COL * 1 + 24, res31);
            _mm256_store_ps(Ybase + N_COL * 2 + 0, res02);
            _mm256_store_ps(Ybase + N_COL * 2 + 8, res12);
            _mm256_store_ps(Ybase + N_COL * 2 + 16, res22);
            _mm256_store_ps(Ybase + N_COL * 2 + 24, res32);
            _mm256_store_ps(Ybase + N_COL * 3 + 0, res03);
            _mm256_store_ps(Ybase + N_COL * 3 + 8, res13);
            _mm256_store_ps(Ybase + N_COL * 3 + 16, res23);
            _mm256_store_ps(Ybase + N_COL * 3 + 24, res33);
            _mm256_store_ps(Ybase + N_COL * 4 + 0, res04);
            _mm256_store_ps(Ybase + N_COL * 4 + 8, res14);
            _mm256_store_ps(Ybase + N_COL * 4 + 16, res24);
            _mm256_store_ps(Ybase + N_COL * 4 + 24, res34);
            _mm256_store_ps(Ybase + N_COL * 5 + 0, res05);
            _mm256_store_ps(Ybase + N_COL * 5 + 8, res15);
            _mm256_store_ps(Ybase + N_COL * 5 + 16, res25);
            _mm256_store_ps(Ybase + N_COL * 5 + 24, res35);
            _mm256_store_ps(Ybase + N_COL * 6 + 0, res06);
            _mm256_store_ps(Ybase + N_COL * 6 + 8, res16);
            _mm256_store_ps(Ybase + N_COL * 6 + 16, res26);
            _mm256_store_ps(Ybase + N_COL * 6 + 24, res36);
            _mm256_store_ps(Ybase + N_COL * 7 + 0, res07);
            _mm256_store_ps(Ybase + N_COL * 7 + 8, res17);
            _mm256_store_ps(Ybase + N_COL * 7 + 16, res27);
            _mm256_store_ps(Ybase + N_COL * 7 + 24, res37);
            for (int g = 0; g < 32; g++) {
                for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++) {
                    Ybase[N_COL * 0 + g] += Xbase[K * 0 + row_index[k]];
                    Ybase[N_COL * 1 + g] += Xbase[K * 1 + row_index[k]];
                    Ybase[N_COL * 2 + g] += Xbase[K * 2 + row_index[k]];
                    Ybase[N_COL * 3 + g] += Xbase[K * 3 + row_index[k]];
                    Ybase[N_COL * 4 + g] += Xbase[K * 4 + row_index[k]];
                    Ybase[N_COL * 5 + g] += Xbase[K * 5 + row_index[k]];
                    Ybase[N_COL * 6 + g] += Xbase[K * 6 + row_index[k]];
                    Ybase[N_COL * 7 + g] += Xbase[K * 7 + row_index[k]];
                }
                for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++) {
                    Ybase[N_COL * 0 + g] -= Xbase[K * 0 + row_index[k]];
                    Ybase[N_COL * 1 + g] -= Xbase[K * 1 + row_index[k]];
                    Ybase[N_COL * 2 + g] -= Xbase[K * 2 + row_index[k]];
                    Ybase[N_COL * 3 + g] -= Xbase[K * 3 + row_index[k]];
                    Ybase[N_COL * 4 + g] -= Xbase[K * 4 + row_index[k]];
                    Ybase[N_COL * 5 + g] -= Xbase[K * 5 + row_index[k]];
                    Ybase[N_COL * 6 + g] -= Xbase[K * 6 + row_index[k]];
                    Ybase[N_COL * 7 + g] -= Xbase[K * 7 + row_index[k]];
                }
            }
        }
    }
}

void GEMM_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_16xG32_AVX2_OpenMP(float* X, int32_t* metadata, int32_t* row_index, float* result, int M_ROW, int N_COL, int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 32; j++) {
        for (int i = 0; i < M_ROW; i += 16) {
            float* Xbase = X + i * K;
            const int* groupData  = &metadata[j * 66];
            __m256 res00 = _mm256_setzero_ps();
            __m256 res01 = _mm256_setzero_ps();
            __m256 res02 = _mm256_setzero_ps();
            __m256 res03 = _mm256_setzero_ps();
            __m256 res04 = _mm256_setzero_ps();
            __m256 res05 = _mm256_setzero_ps();
            __m256 res06 = _mm256_setzero_ps();
            __m256 res07 = _mm256_setzero_ps();
            __m256 res08 = _mm256_setzero_ps();
            __m256 res09 = _mm256_setzero_ps();
            __m256 res010 = _mm256_setzero_ps();
            __m256 res011 = _mm256_setzero_ps();
            __m256 res012 = _mm256_setzero_ps();
            __m256 res013 = _mm256_setzero_ps();
            __m256 res014 = _mm256_setzero_ps();
            __m256 res015 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res11 = _mm256_setzero_ps();
            __m256 res12 = _mm256_setzero_ps();
            __m256 res13 = _mm256_setzero_ps();
            __m256 res14 = _mm256_setzero_ps();
            __m256 res15 = _mm256_setzero_ps();
            __m256 res16 = _mm256_setzero_ps();
            __m256 res17 = _mm256_setzero_ps();
            __m256 res18 = _mm256_setzero_ps();
            __m256 res19 = _mm256_setzero_ps();
            __m256 res110 = _mm256_setzero_ps();
            __m256 res111 = _mm256_setzero_ps();
            __m256 res112 = _mm256_setzero_ps();
            __m256 res113 = _mm256_setzero_ps();
            __m256 res114 = _mm256_setzero_ps();
            __m256 res115 = _mm256_setzero_ps();
            __m256 res20 = _mm256_setzero_ps();
            __m256 res21 = _mm256_setzero_ps();
            __m256 res22 = _mm256_setzero_ps();
            __m256 res23 = _mm256_setzero_ps();
            __m256 res24 = _mm256_setzero_ps();
            __m256 res25 = _mm256_setzero_ps();
            __m256 res26 = _mm256_setzero_ps();
            __m256 res27 = _mm256_setzero_ps();
            __m256 res28 = _mm256_setzero_ps();
            __m256 res29 = _mm256_setzero_ps();
            __m256 res210 = _mm256_setzero_ps();
            __m256 res211 = _mm256_setzero_ps();
            __m256 res212 = _mm256_setzero_ps();
            __m256 res213 = _mm256_setzero_ps();
            __m256 res214 = _mm256_setzero_ps();
            __m256 res215 = _mm256_setzero_ps();
            __m256 res30 = _mm256_setzero_ps();
            __m256 res31 = _mm256_setzero_ps();
            __m256 res32 = _mm256_setzero_ps();
            __m256 res33 = _mm256_setzero_ps();
            __m256 res34 = _mm256_setzero_ps();
            __m256 res35 = _mm256_setzero_ps();
            __m256 res36 = _mm256_setzero_ps();
            __m256 res37 = _mm256_setzero_ps();
            __m256 res38 = _mm256_setzero_ps();
            __m256 res39 = _mm256_setzero_ps();
            __m256 res310 = _mm256_setzero_ps();
            __m256 res311 = _mm256_setzero_ps();
            __m256 res312 = _mm256_setzero_ps();
            __m256 res313 = _mm256_setzero_ps();
            __m256 res314 = _mm256_setzero_ps();
            __m256 res315 = _mm256_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 64) {
                __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
                __m256i indices_p1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
                __m256i indices_p2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 16));
                __m256i indices_p3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 24));
                __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 32));
                __m256i indices_n1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 40));
                __m256i indices_n2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 48));
                __m256i indices_n3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 56));
                __m256 pos00 = _mm256_i32gather_ps(Xbase + K * 0, indices_p0, 4);
                __m256 pos10 = _mm256_i32gather_ps(Xbase + K * 0, indices_p1, 4);
                __m256 pos20 = _mm256_i32gather_ps(Xbase + K * 0, indices_p2, 4);
                __m256 pos30 = _mm256_i32gather_ps(Xbase + K * 0, indices_p3, 4);
                __m256 pos01 = _mm256_i32gather_ps(Xbase + K * 1, indices_p0, 4);
                __m256 pos11 = _mm256_i32gather_ps(Xbase + K * 1, indices_p1, 4);
                __m256 pos21 = _mm256_i32gather_ps(Xbase + K * 1, indices_p2, 4);
                __m256 pos31 = _mm256_i32gather_ps(Xbase + K * 1, indices_p3, 4);
                __m256 pos02 = _mm256_i32gather_ps(Xbase + K * 2, indices_p0, 4);
                __m256 pos12 = _mm256_i32gather_ps(Xbase + K * 2, indices_p1, 4);
                __m256 pos22 = _mm256_i32gather_ps(Xbase + K * 2, indices_p2, 4);
                __m256 pos32 = _mm256_i32gather_ps(Xbase + K * 2, indices_p3, 4);
                __m256 pos03 = _mm256_i32gather_ps(Xbase + K * 3, indices_p0, 4);
                __m256 pos13 = _mm256_i32gather_ps(Xbase + K * 3, indices_p1, 4);
                __m256 pos23 = _mm256_i32gather_ps(Xbase + K * 3, indices_p2, 4);
                __m256 pos33 = _mm256_i32gather_ps(Xbase + K * 3, indices_p3, 4);
                __m256 pos04 = _mm256_i32gather_ps(Xbase + K * 4, indices_p0, 4);
                __m256 pos14 = _mm256_i32gather_ps(Xbase + K * 4, indices_p1, 4);
                __m256 pos24 = _mm256_i32gather_ps(Xbase + K * 4, indices_p2, 4);
                __m256 pos34 = _mm256_i32gather_ps(Xbase + K * 4, indices_p3, 4);
                __m256 pos05 = _mm256_i32gather_ps(Xbase + K * 5, indices_p0, 4);
                __m256 pos15 = _mm256_i32gather_ps(Xbase + K * 5, indices_p1, 4);
                __m256 pos25 = _mm256_i32gather_ps(Xbase + K * 5, indices_p2, 4);
                __m256 pos35 = _mm256_i32gather_ps(Xbase + K * 5, indices_p3, 4);
                __m256 pos06 = _mm256_i32gather_ps(Xbase + K * 6, indices_p0, 4);
                __m256 pos16 = _mm256_i32gather_ps(Xbase + K * 6, indices_p1, 4);
                __m256 pos26 = _mm256_i32gather_ps(Xbase + K * 6, indices_p2, 4);
                __m256 pos36 = _mm256_i32gather_ps(Xbase + K * 6, indices_p3, 4);
                __m256 pos07 = _mm256_i32gather_ps(Xbase + K * 7, indices_p0, 4);
                __m256 pos17 = _mm256_i32gather_ps(Xbase + K * 7, indices_p1, 4);
                __m256 pos27 = _mm256_i32gather_ps(Xbase + K * 7, indices_p2, 4);
                __m256 pos37 = _mm256_i32gather_ps(Xbase + K * 7, indices_p3, 4);
                __m256 pos08 = _mm256_i32gather_ps(Xbase + K * 8, indices_p0, 4);
                __m256 pos18 = _mm256_i32gather_ps(Xbase + K * 8, indices_p1, 4);
                __m256 pos28 = _mm256_i32gather_ps(Xbase + K * 8, indices_p2, 4);
                __m256 pos38 = _mm256_i32gather_ps(Xbase + K * 8, indices_p3, 4);
                __m256 pos09 = _mm256_i32gather_ps(Xbase + K * 9, indices_p0, 4);
                __m256 pos19 = _mm256_i32gather_ps(Xbase + K * 9, indices_p1, 4);
                __m256 pos29 = _mm256_i32gather_ps(Xbase + K * 9, indices_p2, 4);
                __m256 pos39 = _mm256_i32gather_ps(Xbase + K * 9, indices_p3, 4);
                __m256 pos010 = _mm256_i32gather_ps(Xbase + K * 10, indices_p0, 4);
                __m256 pos110 = _mm256_i32gather_ps(Xbase + K * 10, indices_p1, 4);
                __m256 pos210 = _mm256_i32gather_ps(Xbase + K * 10, indices_p2, 4);
                __m256 pos310 = _mm256_i32gather_ps(Xbase + K * 10, indices_p3, 4);
                __m256 pos011 = _mm256_i32gather_ps(Xbase + K * 11, indices_p0, 4);
                __m256 pos111 = _mm256_i32gather_ps(Xbase + K * 11, indices_p1, 4);
                __m256 pos211 = _mm256_i32gather_ps(Xbase + K * 11, indices_p2, 4);
                __m256 pos311 = _mm256_i32gather_ps(Xbase + K * 11, indices_p3, 4);
                __m256 pos012 = _mm256_i32gather_ps(Xbase + K * 12, indices_p0, 4);
                __m256 pos112 = _mm256_i32gather_ps(Xbase + K * 12, indices_p1, 4);
                __m256 pos212 = _mm256_i32gather_ps(Xbase + K * 12, indices_p2, 4);
                __m256 pos312 = _mm256_i32gather_ps(Xbase + K * 12, indices_p3, 4);
                __m256 pos013 = _mm256_i32gather_ps(Xbase + K * 13, indices_p0, 4);
                __m256 pos113 = _mm256_i32gather_ps(Xbase + K * 13, indices_p1, 4);
                __m256 pos213 = _mm256_i32gather_ps(Xbase + K * 13, indices_p2, 4);
                __m256 pos313 = _mm256_i32gather_ps(Xbase + K * 13, indices_p3, 4);
                __m256 pos014 = _mm256_i32gather_ps(Xbase + K * 14, indices_p0, 4);
                __m256 pos114 = _mm256_i32gather_ps(Xbase + K * 14, indices_p1, 4);
                __m256 pos214 = _mm256_i32gather_ps(Xbase + K * 14, indices_p2, 4);
                __m256 pos314 = _mm256_i32gather_ps(Xbase + K * 14, indices_p3, 4);
                __m256 pos015 = _mm256_i32gather_ps(Xbase + K * 15, indices_p0, 4);
                __m256 pos115 = _mm256_i32gather_ps(Xbase + K * 15, indices_p1, 4);
                __m256 pos215 = _mm256_i32gather_ps(Xbase + K * 15, indices_p2, 4);
                __m256 pos315 = _mm256_i32gather_ps(Xbase + K * 15, indices_p3, 4);
                __m256 neg00 = _mm256_i32gather_ps(Xbase + K * 0, indices_n0, 4);
                __m256 neg10 = _mm256_i32gather_ps(Xbase + K * 0, indices_n1, 4);
                __m256 neg20 = _mm256_i32gather_ps(Xbase + K * 0, indices_n2, 4);
                __m256 neg30 = _mm256_i32gather_ps(Xbase + K * 0, indices_n3, 4);
                __m256 neg01 = _mm256_i32gather_ps(Xbase + K * 1, indices_n0, 4);
                __m256 neg11 = _mm256_i32gather_ps(Xbase + K * 1, indices_n1, 4);
                __m256 neg21 = _mm256_i32gather_ps(Xbase + K * 1, indices_n2, 4);
                __m256 neg31 = _mm256_i32gather_ps(Xbase + K * 1, indices_n3, 4);
                __m256 neg02 = _mm256_i32gather_ps(Xbase + K * 2, indices_n0, 4);
                __m256 neg12 = _mm256_i32gather_ps(Xbase + K * 2, indices_n1, 4);
                __m256 neg22 = _mm256_i32gather_ps(Xbase + K * 2, indices_n2, 4);
                __m256 neg32 = _mm256_i32gather_ps(Xbase + K * 2, indices_n3, 4);
                __m256 neg03 = _mm256_i32gather_ps(Xbase + K * 3, indices_n0, 4);
                __m256 neg13 = _mm256_i32gather_ps(Xbase + K * 3, indices_n1, 4);
                __m256 neg23 = _mm256_i32gather_ps(Xbase + K * 3, indices_n2, 4);
                __m256 neg33 = _mm256_i32gather_ps(Xbase + K * 3, indices_n3, 4);
                __m256 neg04 = _mm256_i32gather_ps(Xbase + K * 4, indices_n0, 4);
                __m256 neg14 = _mm256_i32gather_ps(Xbase + K * 4, indices_n1, 4);
                __m256 neg24 = _mm256_i32gather_ps(Xbase + K * 4, indices_n2, 4);
                __m256 neg34 = _mm256_i32gather_ps(Xbase + K * 4, indices_n3, 4);
                __m256 neg05 = _mm256_i32gather_ps(Xbase + K * 5, indices_n0, 4);
                __m256 neg15 = _mm256_i32gather_ps(Xbase + K * 5, indices_n1, 4);
                __m256 neg25 = _mm256_i32gather_ps(Xbase + K * 5, indices_n2, 4);
                __m256 neg35 = _mm256_i32gather_ps(Xbase + K * 5, indices_n3, 4);
                __m256 neg06 = _mm256_i32gather_ps(Xbase + K * 6, indices_n0, 4);
                __m256 neg16 = _mm256_i32gather_ps(Xbase + K * 6, indices_n1, 4);
                __m256 neg26 = _mm256_i32gather_ps(Xbase + K * 6, indices_n2, 4);
                __m256 neg36 = _mm256_i32gather_ps(Xbase + K * 6, indices_n3, 4);
                __m256 neg07 = _mm256_i32gather_ps(Xbase + K * 7, indices_n0, 4);
                __m256 neg17 = _mm256_i32gather_ps(Xbase + K * 7, indices_n1, 4);
                __m256 neg27 = _mm256_i32gather_ps(Xbase + K * 7, indices_n2, 4);
                __m256 neg37 = _mm256_i32gather_ps(Xbase + K * 7, indices_n3, 4);
                __m256 neg08 = _mm256_i32gather_ps(Xbase + K * 8, indices_n0, 4);
                __m256 neg18 = _mm256_i32gather_ps(Xbase + K * 8, indices_n1, 4);
                __m256 neg28 = _mm256_i32gather_ps(Xbase + K * 8, indices_n2, 4);
                __m256 neg38 = _mm256_i32gather_ps(Xbase + K * 8, indices_n3, 4);
                __m256 neg09 = _mm256_i32gather_ps(Xbase + K * 9, indices_n0, 4);
                __m256 neg19 = _mm256_i32gather_ps(Xbase + K * 9, indices_n1, 4);
                __m256 neg29 = _mm256_i32gather_ps(Xbase + K * 9, indices_n2, 4);
                __m256 neg39 = _mm256_i32gather_ps(Xbase + K * 9, indices_n3, 4);
                __m256 neg010 = _mm256_i32gather_ps(Xbase + K * 10, indices_n0, 4);
                __m256 neg110 = _mm256_i32gather_ps(Xbase + K * 10, indices_n1, 4);
                __m256 neg210 = _mm256_i32gather_ps(Xbase + K * 10, indices_n2, 4);
                __m256 neg310 = _mm256_i32gather_ps(Xbase + K * 10, indices_n3, 4);
                __m256 neg011 = _mm256_i32gather_ps(Xbase + K * 11, indices_n0, 4);
                __m256 neg111 = _mm256_i32gather_ps(Xbase + K * 11, indices_n1, 4);
                __m256 neg211 = _mm256_i32gather_ps(Xbase + K * 11, indices_n2, 4);
                __m256 neg311 = _mm256_i32gather_ps(Xbase + K * 11, indices_n3, 4);
                __m256 neg012 = _mm256_i32gather_ps(Xbase + K * 12, indices_n0, 4);
                __m256 neg112 = _mm256_i32gather_ps(Xbase + K * 12, indices_n1, 4);
                __m256 neg212 = _mm256_i32gather_ps(Xbase + K * 12, indices_n2, 4);
                __m256 neg312 = _mm256_i32gather_ps(Xbase + K * 12, indices_n3, 4);
                __m256 neg013 = _mm256_i32gather_ps(Xbase + K * 13, indices_n0, 4);
                __m256 neg113 = _mm256_i32gather_ps(Xbase + K * 13, indices_n1, 4);
                __m256 neg213 = _mm256_i32gather_ps(Xbase + K * 13, indices_n2, 4);
                __m256 neg313 = _mm256_i32gather_ps(Xbase + K * 13, indices_n3, 4);
                __m256 neg014 = _mm256_i32gather_ps(Xbase + K * 14, indices_n0, 4);
                __m256 neg114 = _mm256_i32gather_ps(Xbase + K * 14, indices_n1, 4);
                __m256 neg214 = _mm256_i32gather_ps(Xbase + K * 14, indices_n2, 4);
                __m256 neg314 = _mm256_i32gather_ps(Xbase + K * 14, indices_n3, 4);
                __m256 neg015 = _mm256_i32gather_ps(Xbase + K * 15, indices_n0, 4);
                __m256 neg115 = _mm256_i32gather_ps(Xbase + K * 15, indices_n1, 4);
                __m256 neg215 = _mm256_i32gather_ps(Xbase + K * 15, indices_n2, 4);
                __m256 neg315 = _mm256_i32gather_ps(Xbase + K * 15, indices_n3, 4);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res20 = _mm256_add_ps(res20, _mm256_sub_ps(pos20, neg20));
                res30 = _mm256_add_ps(res30, _mm256_sub_ps(pos30, neg30));
                res01 = _mm256_add_ps(res01, _mm256_sub_ps(pos01, neg01));
                res11 = _mm256_add_ps(res11, _mm256_sub_ps(pos11, neg11));
                res21 = _mm256_add_ps(res21, _mm256_sub_ps(pos21, neg21));
                res31 = _mm256_add_ps(res31, _mm256_sub_ps(pos31, neg31));
                res02 = _mm256_add_ps(res02, _mm256_sub_ps(pos02, neg02));
                res12 = _mm256_add_ps(res12, _mm256_sub_ps(pos12, neg12));
                res22 = _mm256_add_ps(res22, _mm256_sub_ps(pos22, neg22));
                res32 = _mm256_add_ps(res32, _mm256_sub_ps(pos32, neg32));
                res03 = _mm256_add_ps(res03, _mm256_sub_ps(pos03, neg03));
                res13 = _mm256_add_ps(res13, _mm256_sub_ps(pos13, neg13));
                res23 = _mm256_add_ps(res23, _mm256_sub_ps(pos23, neg23));
                res33 = _mm256_add_ps(res33, _mm256_sub_ps(pos33, neg33));
                res04 = _mm256_add_ps(res04, _mm256_sub_ps(pos04, neg04));
                res14 = _mm256_add_ps(res14, _mm256_sub_ps(pos14, neg14));
                res24 = _mm256_add_ps(res24, _mm256_sub_ps(pos24, neg24));
                res34 = _mm256_add_ps(res34, _mm256_sub_ps(pos34, neg34));
                res05 = _mm256_add_ps(res05, _mm256_sub_ps(pos05, neg05));
                res15 = _mm256_add_ps(res15, _mm256_sub_ps(pos15, neg15));
                res25 = _mm256_add_ps(res25, _mm256_sub_ps(pos25, neg25));
                res35 = _mm256_add_ps(res35, _mm256_sub_ps(pos35, neg35));
                res06 = _mm256_add_ps(res06, _mm256_sub_ps(pos06, neg06));
                res16 = _mm256_add_ps(res16, _mm256_sub_ps(pos16, neg16));
                res26 = _mm256_add_ps(res26, _mm256_sub_ps(pos26, neg26));
                res36 = _mm256_add_ps(res36, _mm256_sub_ps(pos36, neg36));
                res07 = _mm256_add_ps(res07, _mm256_sub_ps(pos07, neg07));
                res17 = _mm256_add_ps(res17, _mm256_sub_ps(pos17, neg17));
                res27 = _mm256_add_ps(res27, _mm256_sub_ps(pos27, neg27));
                res37 = _mm256_add_ps(res37, _mm256_sub_ps(pos37, neg37));
                res08 = _mm256_add_ps(res08, _mm256_sub_ps(pos08, neg08));
                res18 = _mm256_add_ps(res18, _mm256_sub_ps(pos18, neg18));
                res28 = _mm256_add_ps(res28, _mm256_sub_ps(pos28, neg28));
                res38 = _mm256_add_ps(res38, _mm256_sub_ps(pos38, neg38));
                res09 = _mm256_add_ps(res09, _mm256_sub_ps(pos09, neg09));
                res19 = _mm256_add_ps(res19, _mm256_sub_ps(pos19, neg19));
                res29 = _mm256_add_ps(res29, _mm256_sub_ps(pos29, neg29));
                res39 = _mm256_add_ps(res39, _mm256_sub_ps(pos39, neg39));
                res010 = _mm256_add_ps(res010, _mm256_sub_ps(pos010, neg010));
                res110 = _mm256_add_ps(res110, _mm256_sub_ps(pos110, neg110));
                res210 = _mm256_add_ps(res210, _mm256_sub_ps(pos210, neg210));
                res310 = _mm256_add_ps(res310, _mm256_sub_ps(pos310, neg310));
                res011 = _mm256_add_ps(res011, _mm256_sub_ps(pos011, neg011));
                res111 = _mm256_add_ps(res111, _mm256_sub_ps(pos111, neg111));
                res211 = _mm256_add_ps(res211, _mm256_sub_ps(pos211, neg211));
                res311 = _mm256_add_ps(res311, _mm256_sub_ps(pos311, neg311));
                res012 = _mm256_add_ps(res012, _mm256_sub_ps(pos012, neg012));
                res112 = _mm256_add_ps(res112, _mm256_sub_ps(pos112, neg112));
                res212 = _mm256_add_ps(res212, _mm256_sub_ps(pos212, neg212));
                res312 = _mm256_add_ps(res312, _mm256_sub_ps(pos312, neg312));
                res013 = _mm256_add_ps(res013, _mm256_sub_ps(pos013, neg013));
                res113 = _mm256_add_ps(res113, _mm256_sub_ps(pos113, neg113));
                res213 = _mm256_add_ps(res213, _mm256_sub_ps(pos213, neg213));
                res313 = _mm256_add_ps(res313, _mm256_sub_ps(pos313, neg313));
                res014 = _mm256_add_ps(res014, _mm256_sub_ps(pos014, neg014));
                res114 = _mm256_add_ps(res114, _mm256_sub_ps(pos114, neg114));
                res214 = _mm256_add_ps(res214, _mm256_sub_ps(pos214, neg214));
                res314 = _mm256_add_ps(res314, _mm256_sub_ps(pos314, neg314));
                res015 = _mm256_add_ps(res015, _mm256_sub_ps(pos015, neg015));
                res115 = _mm256_add_ps(res115, _mm256_sub_ps(pos115, neg115));
                res215 = _mm256_add_ps(res215, _mm256_sub_ps(pos215, neg215));
                res315 = _mm256_add_ps(res315, _mm256_sub_ps(pos315, neg315));
            }
            float* Ybase = result + i * N_COL + j * 32;
            _mm256_store_ps(Ybase + N_COL * 0 + 0, res00);
            _mm256_store_ps(Ybase + N_COL * 0 + 8, res10);
            _mm256_store_ps(Ybase + N_COL * 0 + 16, res20);
            _mm256_store_ps(Ybase + N_COL * 0 + 24, res30);
            _mm256_store_ps(Ybase + N_COL * 1 + 0, res01);
            _mm256_store_ps(Ybase + N_COL * 1 + 8, res11);
            _mm256_store_ps(Ybase + N_COL * 1 + 16, res21);
            _mm256_store_ps(Ybase + N_COL * 1 + 24, res31);
            _mm256_store_ps(Ybase + N_COL * 2 + 0, res02);
            _mm256_store_ps(Ybase + N_COL * 2 + 8, res12);
            _mm256_store_ps(Ybase + N_COL * 2 + 16, res22);
            _mm256_store_ps(Ybase + N_COL * 2 + 24, res32);
            _mm256_store_ps(Ybase + N_COL * 3 + 0, res03);
            _mm256_store_ps(Ybase + N_COL * 3 + 8, res13);
            _mm256_store_ps(Ybase + N_COL * 3 + 16, res23);
            _mm256_store_ps(Ybase + N_COL * 3 + 24, res33);
            _mm256_store_ps(Ybase + N_COL * 4 + 0, res04);
            _mm256_store_ps(Ybase + N_COL * 4 + 8, res14);
            _mm256_store_ps(Ybase + N_COL * 4 + 16, res24);
            _mm256_store_ps(Ybase + N_COL * 4 + 24, res34);
            _mm256_store_ps(Ybase + N_COL * 5 + 0, res05);
            _mm256_store_ps(Ybase + N_COL * 5 + 8, res15);
            _mm256_store_ps(Ybase + N_COL * 5 + 16, res25);
            _mm256_store_ps(Ybase + N_COL * 5 + 24, res35);
            _mm256_store_ps(Ybase + N_COL * 6 + 0, res06);
            _mm256_store_ps(Ybase + N_COL * 6 + 8, res16);
            _mm256_store_ps(Ybase + N_COL * 6 + 16, res26);
            _mm256_store_ps(Ybase + N_COL * 6 + 24, res36);
            _mm256_store_ps(Ybase + N_COL * 7 + 0, res07);
            _mm256_store_ps(Ybase + N_COL * 7 + 8, res17);
            _mm256_store_ps(Ybase + N_COL * 7 + 16, res27);
            _mm256_store_ps(Ybase + N_COL * 7 + 24, res37);
            _mm256_store_ps(Ybase + N_COL * 8 + 0, res08);
            _mm256_store_ps(Ybase + N_COL * 8 + 8, res18);
            _mm256_store_ps(Ybase + N_COL * 8 + 16, res28);
            _mm256_store_ps(Ybase + N_COL * 8 + 24, res38);
            _mm256_store_ps(Ybase + N_COL * 9 + 0, res09);
            _mm256_store_ps(Ybase + N_COL * 9 + 8, res19);
            _mm256_store_ps(Ybase + N_COL * 9 + 16, res29);
            _mm256_store_ps(Ybase + N_COL * 9 + 24, res39);
            _mm256_store_ps(Ybase + N_COL * 10 + 0, res010);
            _mm256_store_ps(Ybase + N_COL * 10 + 8, res110);
            _mm256_store_ps(Ybase + N_COL * 10 + 16, res210);
            _mm256_store_ps(Ybase + N_COL * 10 + 24, res310);
            _mm256_store_ps(Ybase + N_COL * 11 + 0, res011);
            _mm256_store_ps(Ybase + N_COL * 11 + 8, res111);
            _mm256_store_ps(Ybase + N_COL * 11 + 16, res211);
            _mm256_store_ps(Ybase + N_COL * 11 + 24, res311);
            _mm256_store_ps(Ybase + N_COL * 12 + 0, res012);
            _mm256_store_ps(Ybase + N_COL * 12 + 8, res112);
            _mm256_store_ps(Ybase + N_COL * 12 + 16, res212);
            _mm256_store_ps(Ybase + N_COL * 12 + 24, res312);
            _mm256_store_ps(Ybase + N_COL * 13 + 0, res013);
            _mm256_store_ps(Ybase + N_COL * 13 + 8, res113);
            _mm256_store_ps(Ybase + N_COL * 13 + 16, res213);
            _mm256_store_ps(Ybase + N_COL * 13 + 24, res313);
            _mm256_store_ps(Ybase + N_COL * 14 + 0, res014);
            _mm256_store_ps(Ybase + N_COL * 14 + 8, res114);
            _mm256_store_ps(Ybase + N_COL * 14 + 16, res214);
            _mm256_store_ps(Ybase + N_COL * 14 + 24, res314);
            _mm256_store_ps(Ybase + N_COL * 15 + 0, res015);
            _mm256_store_ps(Ybase + N_COL * 15 + 8, res115);
            _mm256_store_ps(Ybase + N_COL * 15 + 16, res215);
            _mm256_store_ps(Ybase + N_COL * 15 + 24, res315);
            for (int g = 0; g < 32; g++) {
                for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++) {
                    Ybase[N_COL * 0 + g] += Xbase[K * 0 + row_index[k]];
                    Ybase[N_COL * 1 + g] += Xbase[K * 1 + row_index[k]];
                    Ybase[N_COL * 2 + g] += Xbase[K * 2 + row_index[k]];
                    Ybase[N_COL * 3 + g] += Xbase[K * 3 + row_index[k]];
                    Ybase[N_COL * 4 + g] += Xbase[K * 4 + row_index[k]];
                    Ybase[N_COL * 5 + g] += Xbase[K * 5 + row_index[k]];
                    Ybase[N_COL * 6 + g] += Xbase[K * 6 + row_index[k]];
                    Ybase[N_COL * 7 + g] += Xbase[K * 7 + row_index[k]];
                    Ybase[N_COL * 8 + g] += Xbase[K * 8 + row_index[k]];
                    Ybase[N_COL * 9 + g] += Xbase[K * 9 + row_index[k]];
                    Ybase[N_COL * 10 + g] += Xbase[K * 10 + row_index[k]];
                    Ybase[N_COL * 11 + g] += Xbase[K * 11 + row_index[k]];
                    Ybase[N_COL * 12 + g] += Xbase[K * 12 + row_index[k]];
                    Ybase[N_COL * 13 + g] += Xbase[K * 13 + row_index[k]];
                    Ybase[N_COL * 14 + g] += Xbase[K * 14 + row_index[k]];
                    Ybase[N_COL * 15 + g] += Xbase[K * 15 + row_index[k]];
                }
                for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++) {
                    Ybase[N_COL * 0 + g] -= Xbase[K * 0 + row_index[k]];
                    Ybase[N_COL * 1 + g] -= Xbase[K * 1 + row_index[k]];
                    Ybase[N_COL * 2 + g] -= Xbase[K * 2 + row_index[k]];
                    Ybase[N_COL * 3 + g] -= Xbase[K * 3 + row_index[k]];
                    Ybase[N_COL * 4 + g] -= Xbase[K * 4 + row_index[k]];
                    Ybase[N_COL * 5 + g] -= Xbase[K * 5 + row_index[k]];
                    Ybase[N_COL * 6 + g] -= Xbase[K * 6 + row_index[k]];
                    Ybase[N_COL * 7 + g] -= Xbase[K * 7 + row_index[k]];
                    Ybase[N_COL * 8 + g] -= Xbase[K * 8 + row_index[k]];
                    Ybase[N_COL * 9 + g] -= Xbase[K * 9 + row_index[k]];
                    Ybase[N_COL * 10 + g] -= Xbase[K * 10 + row_index[k]];
                    Ybase[N_COL * 11 + g] -= Xbase[K * 11 + row_index[k]];
                    Ybase[N_COL * 12 + g] -= Xbase[K * 12 + row_index[k]];
                    Ybase[N_COL * 13 + g] -= Xbase[K * 13 + row_index[k]];
                    Ybase[N_COL * 14 + g] -= Xbase[K * 14 + row_index[k]];
                    Ybase[N_COL * 15 + g] -= Xbase[K * 15 + row_index[k]];
                }
            }
        }
    }
}

void GEMM_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_1xG64_AVX2_OpenMP(float* X, int32_t* metadata, int32_t* row_index, float* result, int M_ROW, int N_COL, int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 64; j++) {
        for (int i = 0; i < M_ROW; i += 1) {
            float* Xbase = X + i * K;
            const int* groupData  = &metadata[j * 130];
            __m256 res00 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res20 = _mm256_setzero_ps();
            __m256 res30 = _mm256_setzero_ps();
            __m256 res40 = _mm256_setzero_ps();
            __m256 res50 = _mm256_setzero_ps();
            __m256 res60 = _mm256_setzero_ps();
            __m256 res70 = _mm256_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 128) {
                __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
                __m256i indices_p1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
                __m256i indices_p2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 16));
                __m256i indices_p3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 24));
                __m256i indices_p4 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 32));
                __m256i indices_p5 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 40));
                __m256i indices_p6 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 48));
                __m256i indices_p7 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 56));
                __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 64));
                __m256i indices_n1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 72));
                __m256i indices_n2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 80));
                __m256i indices_n3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 88));
                __m256i indices_n4 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 96));
                __m256i indices_n5 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 104));
                __m256i indices_n6 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 112));
                __m256i indices_n7 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 120));
                __m256 pos00 = _mm256_i32gather_ps(Xbase + K * 0, indices_p0, 4);
                __m256 pos10 = _mm256_i32gather_ps(Xbase + K * 0, indices_p1, 4);
                __m256 pos20 = _mm256_i32gather_ps(Xbase + K * 0, indices_p2, 4);
                __m256 pos30 = _mm256_i32gather_ps(Xbase + K * 0, indices_p3, 4);
                __m256 pos40 = _mm256_i32gather_ps(Xbase + K * 0, indices_p4, 4);
                __m256 pos50 = _mm256_i32gather_ps(Xbase + K * 0, indices_p5, 4);
                __m256 pos60 = _mm256_i32gather_ps(Xbase + K * 0, indices_p6, 4);
                __m256 pos70 = _mm256_i32gather_ps(Xbase + K * 0, indices_p7, 4);
                __m256 neg00 = _mm256_i32gather_ps(Xbase + K * 0, indices_n0, 4);
                __m256 neg10 = _mm256_i32gather_ps(Xbase + K * 0, indices_n1, 4);
                __m256 neg20 = _mm256_i32gather_ps(Xbase + K * 0, indices_n2, 4);
                __m256 neg30 = _mm256_i32gather_ps(Xbase + K * 0, indices_n3, 4);
                __m256 neg40 = _mm256_i32gather_ps(Xbase + K * 0, indices_n4, 4);
                __m256 neg50 = _mm256_i32gather_ps(Xbase + K * 0, indices_n5, 4);
                __m256 neg60 = _mm256_i32gather_ps(Xbase + K * 0, indices_n6, 4);
                __m256 neg70 = _mm256_i32gather_ps(Xbase + K * 0, indices_n7, 4);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res20 = _mm256_add_ps(res20, _mm256_sub_ps(pos20, neg20));
                res30 = _mm256_add_ps(res30, _mm256_sub_ps(pos30, neg30));
                res40 = _mm256_add_ps(res40, _mm256_sub_ps(pos40, neg40));
                res50 = _mm256_add_ps(res50, _mm256_sub_ps(pos50, neg50));
                res60 = _mm256_add_ps(res60, _mm256_sub_ps(pos60, neg60));
                res70 = _mm256_add_ps(res70, _mm256_sub_ps(pos70, neg70));
            }
            float* Ybase = result + i * N_COL + j * 64;
            _mm256_store_ps(Ybase + N_COL * 0 + 0, res00);
            _mm256_store_ps(Ybase + N_COL * 0 + 8, res10);
            _mm256_store_ps(Ybase + N_COL * 0 + 16, res20);
            _mm256_store_ps(Ybase + N_COL * 0 + 24, res30);
            _mm256_store_ps(Ybase + N_COL * 0 + 32, res40);
            _mm256_store_ps(Ybase + N_COL * 0 + 40, res50);
            _mm256_store_ps(Ybase + N_COL * 0 + 48, res60);
            _mm256_store_ps(Ybase + N_COL * 0 + 56, res70);
            for (int g = 0; g < 64; g++) {
                for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++) {
                    Ybase[N_COL * 0 + g] += Xbase[K * 0 + row_index[k]];
                }
                for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++) {
                    Ybase[N_COL * 0 + g] -= Xbase[K * 0 + row_index[k]];
                }
            }
        }
    }
}

void GEMM_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_2xG64_AVX2_OpenMP(float* X, int32_t* metadata, int32_t* row_index, float* result, int M_ROW, int N_COL, int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 64; j++) {
        for (int i = 0; i < M_ROW; i += 2) {
            float* Xbase = X + i * K;
            const int* groupData  = &metadata[j * 130];
            __m256 res00 = _mm256_setzero_ps();
            __m256 res01 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res11 = _mm256_setzero_ps();
            __m256 res20 = _mm256_setzero_ps();
            __m256 res21 = _mm256_setzero_ps();
            __m256 res30 = _mm256_setzero_ps();
            __m256 res31 = _mm256_setzero_ps();
            __m256 res40 = _mm256_setzero_ps();
            __m256 res41 = _mm256_setzero_ps();
            __m256 res50 = _mm256_setzero_ps();
            __m256 res51 = _mm256_setzero_ps();
            __m256 res60 = _mm256_setzero_ps();
            __m256 res61 = _mm256_setzero_ps();
            __m256 res70 = _mm256_setzero_ps();
            __m256 res71 = _mm256_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 128) {
                __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
                __m256i indices_p1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
                __m256i indices_p2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 16));
                __m256i indices_p3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 24));
                __m256i indices_p4 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 32));
                __m256i indices_p5 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 40));
                __m256i indices_p6 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 48));
                __m256i indices_p7 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 56));
                __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 64));
                __m256i indices_n1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 72));
                __m256i indices_n2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 80));
                __m256i indices_n3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 88));
                __m256i indices_n4 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 96));
                __m256i indices_n5 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 104));
                __m256i indices_n6 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 112));
                __m256i indices_n7 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 120));
                __m256 pos00 = _mm256_i32gather_ps(Xbase + K * 0, indices_p0, 4);
                __m256 pos10 = _mm256_i32gather_ps(Xbase + K * 0, indices_p1, 4);
                __m256 pos20 = _mm256_i32gather_ps(Xbase + K * 0, indices_p2, 4);
                __m256 pos30 = _mm256_i32gather_ps(Xbase + K * 0, indices_p3, 4);
                __m256 pos40 = _mm256_i32gather_ps(Xbase + K * 0, indices_p4, 4);
                __m256 pos50 = _mm256_i32gather_ps(Xbase + K * 0, indices_p5, 4);
                __m256 pos60 = _mm256_i32gather_ps(Xbase + K * 0, indices_p6, 4);
                __m256 pos70 = _mm256_i32gather_ps(Xbase + K * 0, indices_p7, 4);
                __m256 pos01 = _mm256_i32gather_ps(Xbase + K * 1, indices_p0, 4);
                __m256 pos11 = _mm256_i32gather_ps(Xbase + K * 1, indices_p1, 4);
                __m256 pos21 = _mm256_i32gather_ps(Xbase + K * 1, indices_p2, 4);
                __m256 pos31 = _mm256_i32gather_ps(Xbase + K * 1, indices_p3, 4);
                __m256 pos41 = _mm256_i32gather_ps(Xbase + K * 1, indices_p4, 4);
                __m256 pos51 = _mm256_i32gather_ps(Xbase + K * 1, indices_p5, 4);
                __m256 pos61 = _mm256_i32gather_ps(Xbase + K * 1, indices_p6, 4);
                __m256 pos71 = _mm256_i32gather_ps(Xbase + K * 1, indices_p7, 4);
                __m256 neg00 = _mm256_i32gather_ps(Xbase + K * 0, indices_n0, 4);
                __m256 neg10 = _mm256_i32gather_ps(Xbase + K * 0, indices_n1, 4);
                __m256 neg20 = _mm256_i32gather_ps(Xbase + K * 0, indices_n2, 4);
                __m256 neg30 = _mm256_i32gather_ps(Xbase + K * 0, indices_n3, 4);
                __m256 neg40 = _mm256_i32gather_ps(Xbase + K * 0, indices_n4, 4);
                __m256 neg50 = _mm256_i32gather_ps(Xbase + K * 0, indices_n5, 4);
                __m256 neg60 = _mm256_i32gather_ps(Xbase + K * 0, indices_n6, 4);
                __m256 neg70 = _mm256_i32gather_ps(Xbase + K * 0, indices_n7, 4);
                __m256 neg01 = _mm256_i32gather_ps(Xbase + K * 1, indices_n0, 4);
                __m256 neg11 = _mm256_i32gather_ps(Xbase + K * 1, indices_n1, 4);
                __m256 neg21 = _mm256_i32gather_ps(Xbase + K * 1, indices_n2, 4);
                __m256 neg31 = _mm256_i32gather_ps(Xbase + K * 1, indices_n3, 4);
                __m256 neg41 = _mm256_i32gather_ps(Xbase + K * 1, indices_n4, 4);
                __m256 neg51 = _mm256_i32gather_ps(Xbase + K * 1, indices_n5, 4);
                __m256 neg61 = _mm256_i32gather_ps(Xbase + K * 1, indices_n6, 4);
                __m256 neg71 = _mm256_i32gather_ps(Xbase + K * 1, indices_n7, 4);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res20 = _mm256_add_ps(res20, _mm256_sub_ps(pos20, neg20));
                res30 = _mm256_add_ps(res30, _mm256_sub_ps(pos30, neg30));
                res40 = _mm256_add_ps(res40, _mm256_sub_ps(pos40, neg40));
                res50 = _mm256_add_ps(res50, _mm256_sub_ps(pos50, neg50));
                res60 = _mm256_add_ps(res60, _mm256_sub_ps(pos60, neg60));
                res70 = _mm256_add_ps(res70, _mm256_sub_ps(pos70, neg70));
                res01 = _mm256_add_ps(res01, _mm256_sub_ps(pos01, neg01));
                res11 = _mm256_add_ps(res11, _mm256_sub_ps(pos11, neg11));
                res21 = _mm256_add_ps(res21, _mm256_sub_ps(pos21, neg21));
                res31 = _mm256_add_ps(res31, _mm256_sub_ps(pos31, neg31));
                res41 = _mm256_add_ps(res41, _mm256_sub_ps(pos41, neg41));
                res51 = _mm256_add_ps(res51, _mm256_sub_ps(pos51, neg51));
                res61 = _mm256_add_ps(res61, _mm256_sub_ps(pos61, neg61));
                res71 = _mm256_add_ps(res71, _mm256_sub_ps(pos71, neg71));
            }
            float* Ybase = result + i * N_COL + j * 64;
            _mm256_store_ps(Ybase + N_COL * 0 + 0, res00);
            _mm256_store_ps(Ybase + N_COL * 0 + 8, res10);
            _mm256_store_ps(Ybase + N_COL * 0 + 16, res20);
            _mm256_store_ps(Ybase + N_COL * 0 + 24, res30);
            _mm256_store_ps(Ybase + N_COL * 0 + 32, res40);
            _mm256_store_ps(Ybase + N_COL * 0 + 40, res50);
            _mm256_store_ps(Ybase + N_COL * 0 + 48, res60);
            _mm256_store_ps(Ybase + N_COL * 0 + 56, res70);
            _mm256_store_ps(Ybase + N_COL * 1 + 0, res01);
            _mm256_store_ps(Ybase + N_COL * 1 + 8, res11);
            _mm256_store_ps(Ybase + N_COL * 1 + 16, res21);
            _mm256_store_ps(Ybase + N_COL * 1 + 24, res31);
            _mm256_store_ps(Ybase + N_COL * 1 + 32, res41);
            _mm256_store_ps(Ybase + N_COL * 1 + 40, res51);
            _mm256_store_ps(Ybase + N_COL * 1 + 48, res61);
            _mm256_store_ps(Ybase + N_COL * 1 + 56, res71);
            for (int g = 0; g < 64; g++) {
                for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++) {
                    Ybase[N_COL * 0 + g] += Xbase[K * 0 + row_index[k]];
                    Ybase[N_COL * 1 + g] += Xbase[K * 1 + row_index[k]];
                }
                for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++) {
                    Ybase[N_COL * 0 + g] -= Xbase[K * 0 + row_index[k]];
                    Ybase[N_COL * 1 + g] -= Xbase[K * 1 + row_index[k]];
                }
            }
        }
    }
}

void GEMM_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_4xG64_AVX2_OpenMP(float* X, int32_t* metadata, int32_t* row_index, float* result, int M_ROW, int N_COL, int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 64; j++) {
        for (int i = 0; i < M_ROW; i += 4) {
            float* Xbase = X + i * K;
            const int* groupData  = &metadata[j * 130];
            __m256 res00 = _mm256_setzero_ps();
            __m256 res01 = _mm256_setzero_ps();
            __m256 res02 = _mm256_setzero_ps();
            __m256 res03 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res11 = _mm256_setzero_ps();
            __m256 res12 = _mm256_setzero_ps();
            __m256 res13 = _mm256_setzero_ps();
            __m256 res20 = _mm256_setzero_ps();
            __m256 res21 = _mm256_setzero_ps();
            __m256 res22 = _mm256_setzero_ps();
            __m256 res23 = _mm256_setzero_ps();
            __m256 res30 = _mm256_setzero_ps();
            __m256 res31 = _mm256_setzero_ps();
            __m256 res32 = _mm256_setzero_ps();
            __m256 res33 = _mm256_setzero_ps();
            __m256 res40 = _mm256_setzero_ps();
            __m256 res41 = _mm256_setzero_ps();
            __m256 res42 = _mm256_setzero_ps();
            __m256 res43 = _mm256_setzero_ps();
            __m256 res50 = _mm256_setzero_ps();
            __m256 res51 = _mm256_setzero_ps();
            __m256 res52 = _mm256_setzero_ps();
            __m256 res53 = _mm256_setzero_ps();
            __m256 res60 = _mm256_setzero_ps();
            __m256 res61 = _mm256_setzero_ps();
            __m256 res62 = _mm256_setzero_ps();
            __m256 res63 = _mm256_setzero_ps();
            __m256 res70 = _mm256_setzero_ps();
            __m256 res71 = _mm256_setzero_ps();
            __m256 res72 = _mm256_setzero_ps();
            __m256 res73 = _mm256_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 128) {
                __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
                __m256i indices_p1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
                __m256i indices_p2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 16));
                __m256i indices_p3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 24));
                __m256i indices_p4 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 32));
                __m256i indices_p5 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 40));
                __m256i indices_p6 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 48));
                __m256i indices_p7 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 56));
                __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 64));
                __m256i indices_n1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 72));
                __m256i indices_n2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 80));
                __m256i indices_n3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 88));
                __m256i indices_n4 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 96));
                __m256i indices_n5 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 104));
                __m256i indices_n6 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 112));
                __m256i indices_n7 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 120));
                __m256 pos00 = _mm256_i32gather_ps(Xbase + K * 0, indices_p0, 4);
                __m256 pos10 = _mm256_i32gather_ps(Xbase + K * 0, indices_p1, 4);
                __m256 pos20 = _mm256_i32gather_ps(Xbase + K * 0, indices_p2, 4);
                __m256 pos30 = _mm256_i32gather_ps(Xbase + K * 0, indices_p3, 4);
                __m256 pos40 = _mm256_i32gather_ps(Xbase + K * 0, indices_p4, 4);
                __m256 pos50 = _mm256_i32gather_ps(Xbase + K * 0, indices_p5, 4);
                __m256 pos60 = _mm256_i32gather_ps(Xbase + K * 0, indices_p6, 4);
                __m256 pos70 = _mm256_i32gather_ps(Xbase + K * 0, indices_p7, 4);
                __m256 pos01 = _mm256_i32gather_ps(Xbase + K * 1, indices_p0, 4);
                __m256 pos11 = _mm256_i32gather_ps(Xbase + K * 1, indices_p1, 4);
                __m256 pos21 = _mm256_i32gather_ps(Xbase + K * 1, indices_p2, 4);
                __m256 pos31 = _mm256_i32gather_ps(Xbase + K * 1, indices_p3, 4);
                __m256 pos41 = _mm256_i32gather_ps(Xbase + K * 1, indices_p4, 4);
                __m256 pos51 = _mm256_i32gather_ps(Xbase + K * 1, indices_p5, 4);
                __m256 pos61 = _mm256_i32gather_ps(Xbase + K * 1, indices_p6, 4);
                __m256 pos71 = _mm256_i32gather_ps(Xbase + K * 1, indices_p7, 4);
                __m256 pos02 = _mm256_i32gather_ps(Xbase + K * 2, indices_p0, 4);
                __m256 pos12 = _mm256_i32gather_ps(Xbase + K * 2, indices_p1, 4);
                __m256 pos22 = _mm256_i32gather_ps(Xbase + K * 2, indices_p2, 4);
                __m256 pos32 = _mm256_i32gather_ps(Xbase + K * 2, indices_p3, 4);
                __m256 pos42 = _mm256_i32gather_ps(Xbase + K * 2, indices_p4, 4);
                __m256 pos52 = _mm256_i32gather_ps(Xbase + K * 2, indices_p5, 4);
                __m256 pos62 = _mm256_i32gather_ps(Xbase + K * 2, indices_p6, 4);
                __m256 pos72 = _mm256_i32gather_ps(Xbase + K * 2, indices_p7, 4);
                __m256 pos03 = _mm256_i32gather_ps(Xbase + K * 3, indices_p0, 4);
                __m256 pos13 = _mm256_i32gather_ps(Xbase + K * 3, indices_p1, 4);
                __m256 pos23 = _mm256_i32gather_ps(Xbase + K * 3, indices_p2, 4);
                __m256 pos33 = _mm256_i32gather_ps(Xbase + K * 3, indices_p3, 4);
                __m256 pos43 = _mm256_i32gather_ps(Xbase + K * 3, indices_p4, 4);
                __m256 pos53 = _mm256_i32gather_ps(Xbase + K * 3, indices_p5, 4);
                __m256 pos63 = _mm256_i32gather_ps(Xbase + K * 3, indices_p6, 4);
                __m256 pos73 = _mm256_i32gather_ps(Xbase + K * 3, indices_p7, 4);
                __m256 neg00 = _mm256_i32gather_ps(Xbase + K * 0, indices_n0, 4);
                __m256 neg10 = _mm256_i32gather_ps(Xbase + K * 0, indices_n1, 4);
                __m256 neg20 = _mm256_i32gather_ps(Xbase + K * 0, indices_n2, 4);
                __m256 neg30 = _mm256_i32gather_ps(Xbase + K * 0, indices_n3, 4);
                __m256 neg40 = _mm256_i32gather_ps(Xbase + K * 0, indices_n4, 4);
                __m256 neg50 = _mm256_i32gather_ps(Xbase + K * 0, indices_n5, 4);
                __m256 neg60 = _mm256_i32gather_ps(Xbase + K * 0, indices_n6, 4);
                __m256 neg70 = _mm256_i32gather_ps(Xbase + K * 0, indices_n7, 4);
                __m256 neg01 = _mm256_i32gather_ps(Xbase + K * 1, indices_n0, 4);
                __m256 neg11 = _mm256_i32gather_ps(Xbase + K * 1, indices_n1, 4);
                __m256 neg21 = _mm256_i32gather_ps(Xbase + K * 1, indices_n2, 4);
                __m256 neg31 = _mm256_i32gather_ps(Xbase + K * 1, indices_n3, 4);
                __m256 neg41 = _mm256_i32gather_ps(Xbase + K * 1, indices_n4, 4);
                __m256 neg51 = _mm256_i32gather_ps(Xbase + K * 1, indices_n5, 4);
                __m256 neg61 = _mm256_i32gather_ps(Xbase + K * 1, indices_n6, 4);
                __m256 neg71 = _mm256_i32gather_ps(Xbase + K * 1, indices_n7, 4);
                __m256 neg02 = _mm256_i32gather_ps(Xbase + K * 2, indices_n0, 4);
                __m256 neg12 = _mm256_i32gather_ps(Xbase + K * 2, indices_n1, 4);
                __m256 neg22 = _mm256_i32gather_ps(Xbase + K * 2, indices_n2, 4);
                __m256 neg32 = _mm256_i32gather_ps(Xbase + K * 2, indices_n3, 4);
                __m256 neg42 = _mm256_i32gather_ps(Xbase + K * 2, indices_n4, 4);
                __m256 neg52 = _mm256_i32gather_ps(Xbase + K * 2, indices_n5, 4);
                __m256 neg62 = _mm256_i32gather_ps(Xbase + K * 2, indices_n6, 4);
                __m256 neg72 = _mm256_i32gather_ps(Xbase + K * 2, indices_n7, 4);
                __m256 neg03 = _mm256_i32gather_ps(Xbase + K * 3, indices_n0, 4);
                __m256 neg13 = _mm256_i32gather_ps(Xbase + K * 3, indices_n1, 4);
                __m256 neg23 = _mm256_i32gather_ps(Xbase + K * 3, indices_n2, 4);
                __m256 neg33 = _mm256_i32gather_ps(Xbase + K * 3, indices_n3, 4);
                __m256 neg43 = _mm256_i32gather_ps(Xbase + K * 3, indices_n4, 4);
                __m256 neg53 = _mm256_i32gather_ps(Xbase + K * 3, indices_n5, 4);
                __m256 neg63 = _mm256_i32gather_ps(Xbase + K * 3, indices_n6, 4);
                __m256 neg73 = _mm256_i32gather_ps(Xbase + K * 3, indices_n7, 4);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res20 = _mm256_add_ps(res20, _mm256_sub_ps(pos20, neg20));
                res30 = _mm256_add_ps(res30, _mm256_sub_ps(pos30, neg30));
                res40 = _mm256_add_ps(res40, _mm256_sub_ps(pos40, neg40));
                res50 = _mm256_add_ps(res50, _mm256_sub_ps(pos50, neg50));
                res60 = _mm256_add_ps(res60, _mm256_sub_ps(pos60, neg60));
                res70 = _mm256_add_ps(res70, _mm256_sub_ps(pos70, neg70));
                res01 = _mm256_add_ps(res01, _mm256_sub_ps(pos01, neg01));
                res11 = _mm256_add_ps(res11, _mm256_sub_ps(pos11, neg11));
                res21 = _mm256_add_ps(res21, _mm256_sub_ps(pos21, neg21));
                res31 = _mm256_add_ps(res31, _mm256_sub_ps(pos31, neg31));
                res41 = _mm256_add_ps(res41, _mm256_sub_ps(pos41, neg41));
                res51 = _mm256_add_ps(res51, _mm256_sub_ps(pos51, neg51));
                res61 = _mm256_add_ps(res61, _mm256_sub_ps(pos61, neg61));
                res71 = _mm256_add_ps(res71, _mm256_sub_ps(pos71, neg71));
                res02 = _mm256_add_ps(res02, _mm256_sub_ps(pos02, neg02));
                res12 = _mm256_add_ps(res12, _mm256_sub_ps(pos12, neg12));
                res22 = _mm256_add_ps(res22, _mm256_sub_ps(pos22, neg22));
                res32 = _mm256_add_ps(res32, _mm256_sub_ps(pos32, neg32));
                res42 = _mm256_add_ps(res42, _mm256_sub_ps(pos42, neg42));
                res52 = _mm256_add_ps(res52, _mm256_sub_ps(pos52, neg52));
                res62 = _mm256_add_ps(res62, _mm256_sub_ps(pos62, neg62));
                res72 = _mm256_add_ps(res72, _mm256_sub_ps(pos72, neg72));
                res03 = _mm256_add_ps(res03, _mm256_sub_ps(pos03, neg03));
                res13 = _mm256_add_ps(res13, _mm256_sub_ps(pos13, neg13));
                res23 = _mm256_add_ps(res23, _mm256_sub_ps(pos23, neg23));
                res33 = _mm256_add_ps(res33, _mm256_sub_ps(pos33, neg33));
                res43 = _mm256_add_ps(res43, _mm256_sub_ps(pos43, neg43));
                res53 = _mm256_add_ps(res53, _mm256_sub_ps(pos53, neg53));
                res63 = _mm256_add_ps(res63, _mm256_sub_ps(pos63, neg63));
                res73 = _mm256_add_ps(res73, _mm256_sub_ps(pos73, neg73));
            }
            float* Ybase = result + i * N_COL + j * 64;
            _mm256_store_ps(Ybase + N_COL * 0 + 0, res00);
            _mm256_store_ps(Ybase + N_COL * 0 + 8, res10);
            _mm256_store_ps(Ybase + N_COL * 0 + 16, res20);
            _mm256_store_ps(Ybase + N_COL * 0 + 24, res30);
            _mm256_store_ps(Ybase + N_COL * 0 + 32, res40);
            _mm256_store_ps(Ybase + N_COL * 0 + 40, res50);
            _mm256_store_ps(Ybase + N_COL * 0 + 48, res60);
            _mm256_store_ps(Ybase + N_COL * 0 + 56, res70);
            _mm256_store_ps(Ybase + N_COL * 1 + 0, res01);
            _mm256_store_ps(Ybase + N_COL * 1 + 8, res11);
            _mm256_store_ps(Ybase + N_COL * 1 + 16, res21);
            _mm256_store_ps(Ybase + N_COL * 1 + 24, res31);
            _mm256_store_ps(Ybase + N_COL * 1 + 32, res41);
            _mm256_store_ps(Ybase + N_COL * 1 + 40, res51);
            _mm256_store_ps(Ybase + N_COL * 1 + 48, res61);
            _mm256_store_ps(Ybase + N_COL * 1 + 56, res71);
            _mm256_store_ps(Ybase + N_COL * 2 + 0, res02);
            _mm256_store_ps(Ybase + N_COL * 2 + 8, res12);
            _mm256_store_ps(Ybase + N_COL * 2 + 16, res22);
            _mm256_store_ps(Ybase + N_COL * 2 + 24, res32);
            _mm256_store_ps(Ybase + N_COL * 2 + 32, res42);
            _mm256_store_ps(Ybase + N_COL * 2 + 40, res52);
            _mm256_store_ps(Ybase + N_COL * 2 + 48, res62);
            _mm256_store_ps(Ybase + N_COL * 2 + 56, res72);
            _mm256_store_ps(Ybase + N_COL * 3 + 0, res03);
            _mm256_store_ps(Ybase + N_COL * 3 + 8, res13);
            _mm256_store_ps(Ybase + N_COL * 3 + 16, res23);
            _mm256_store_ps(Ybase + N_COL * 3 + 24, res33);
            _mm256_store_ps(Ybase + N_COL * 3 + 32, res43);
            _mm256_store_ps(Ybase + N_COL * 3 + 40, res53);
            _mm256_store_ps(Ybase + N_COL * 3 + 48, res63);
            _mm256_store_ps(Ybase + N_COL * 3 + 56, res73);
            for (int g = 0; g < 64; g++) {
                for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++) {
                    Ybase[N_COL * 0 + g] += Xbase[K * 0 + row_index[k]];
                    Ybase[N_COL * 1 + g] += Xbase[K * 1 + row_index[k]];
                    Ybase[N_COL * 2 + g] += Xbase[K * 2 + row_index[k]];
                    Ybase[N_COL * 3 + g] += Xbase[K * 3 + row_index[k]];
                }
                for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++) {
                    Ybase[N_COL * 0 + g] -= Xbase[K * 0 + row_index[k]];
                    Ybase[N_COL * 1 + g] -= Xbase[K * 1 + row_index[k]];
                    Ybase[N_COL * 2 + g] -= Xbase[K * 2 + row_index[k]];
                    Ybase[N_COL * 3 + g] -= Xbase[K * 3 + row_index[k]];
                }
            }
        }
    }
}

void GEMM_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_8xG64_AVX2_OpenMP(float* X, int32_t* metadata, int32_t* row_index, float* result, int M_ROW, int N_COL, int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 64; j++) {
        for (int i = 0; i < M_ROW; i += 8) {
            float* Xbase = X + i * K;
            const int* groupData  = &metadata[j * 130];
            __m256 res00 = _mm256_setzero_ps();
            __m256 res01 = _mm256_setzero_ps();
            __m256 res02 = _mm256_setzero_ps();
            __m256 res03 = _mm256_setzero_ps();
            __m256 res04 = _mm256_setzero_ps();
            __m256 res05 = _mm256_setzero_ps();
            __m256 res06 = _mm256_setzero_ps();
            __m256 res07 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res11 = _mm256_setzero_ps();
            __m256 res12 = _mm256_setzero_ps();
            __m256 res13 = _mm256_setzero_ps();
            __m256 res14 = _mm256_setzero_ps();
            __m256 res15 = _mm256_setzero_ps();
            __m256 res16 = _mm256_setzero_ps();
            __m256 res17 = _mm256_setzero_ps();
            __m256 res20 = _mm256_setzero_ps();
            __m256 res21 = _mm256_setzero_ps();
            __m256 res22 = _mm256_setzero_ps();
            __m256 res23 = _mm256_setzero_ps();
            __m256 res24 = _mm256_setzero_ps();
            __m256 res25 = _mm256_setzero_ps();
            __m256 res26 = _mm256_setzero_ps();
            __m256 res27 = _mm256_setzero_ps();
            __m256 res30 = _mm256_setzero_ps();
            __m256 res31 = _mm256_setzero_ps();
            __m256 res32 = _mm256_setzero_ps();
            __m256 res33 = _mm256_setzero_ps();
            __m256 res34 = _mm256_setzero_ps();
            __m256 res35 = _mm256_setzero_ps();
            __m256 res36 = _mm256_setzero_ps();
            __m256 res37 = _mm256_setzero_ps();
            __m256 res40 = _mm256_setzero_ps();
            __m256 res41 = _mm256_setzero_ps();
            __m256 res42 = _mm256_setzero_ps();
            __m256 res43 = _mm256_setzero_ps();
            __m256 res44 = _mm256_setzero_ps();
            __m256 res45 = _mm256_setzero_ps();
            __m256 res46 = _mm256_setzero_ps();
            __m256 res47 = _mm256_setzero_ps();
            __m256 res50 = _mm256_setzero_ps();
            __m256 res51 = _mm256_setzero_ps();
            __m256 res52 = _mm256_setzero_ps();
            __m256 res53 = _mm256_setzero_ps();
            __m256 res54 = _mm256_setzero_ps();
            __m256 res55 = _mm256_setzero_ps();
            __m256 res56 = _mm256_setzero_ps();
            __m256 res57 = _mm256_setzero_ps();
            __m256 res60 = _mm256_setzero_ps();
            __m256 res61 = _mm256_setzero_ps();
            __m256 res62 = _mm256_setzero_ps();
            __m256 res63 = _mm256_setzero_ps();
            __m256 res64 = _mm256_setzero_ps();
            __m256 res65 = _mm256_setzero_ps();
            __m256 res66 = _mm256_setzero_ps();
            __m256 res67 = _mm256_setzero_ps();
            __m256 res70 = _mm256_setzero_ps();
            __m256 res71 = _mm256_setzero_ps();
            __m256 res72 = _mm256_setzero_ps();
            __m256 res73 = _mm256_setzero_ps();
            __m256 res74 = _mm256_setzero_ps();
            __m256 res75 = _mm256_setzero_ps();
            __m256 res76 = _mm256_setzero_ps();
            __m256 res77 = _mm256_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 128) {
                __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
                __m256i indices_p1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
                __m256i indices_p2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 16));
                __m256i indices_p3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 24));
                __m256i indices_p4 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 32));
                __m256i indices_p5 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 40));
                __m256i indices_p6 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 48));
                __m256i indices_p7 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 56));
                __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 64));
                __m256i indices_n1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 72));
                __m256i indices_n2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 80));
                __m256i indices_n3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 88));
                __m256i indices_n4 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 96));
                __m256i indices_n5 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 104));
                __m256i indices_n6 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 112));
                __m256i indices_n7 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 120));
                __m256 pos00 = _mm256_i32gather_ps(Xbase + K * 0, indices_p0, 4);
                __m256 pos10 = _mm256_i32gather_ps(Xbase + K * 0, indices_p1, 4);
                __m256 pos20 = _mm256_i32gather_ps(Xbase + K * 0, indices_p2, 4);
                __m256 pos30 = _mm256_i32gather_ps(Xbase + K * 0, indices_p3, 4);
                __m256 pos40 = _mm256_i32gather_ps(Xbase + K * 0, indices_p4, 4);
                __m256 pos50 = _mm256_i32gather_ps(Xbase + K * 0, indices_p5, 4);
                __m256 pos60 = _mm256_i32gather_ps(Xbase + K * 0, indices_p6, 4);
                __m256 pos70 = _mm256_i32gather_ps(Xbase + K * 0, indices_p7, 4);
                __m256 pos01 = _mm256_i32gather_ps(Xbase + K * 1, indices_p0, 4);
                __m256 pos11 = _mm256_i32gather_ps(Xbase + K * 1, indices_p1, 4);
                __m256 pos21 = _mm256_i32gather_ps(Xbase + K * 1, indices_p2, 4);
                __m256 pos31 = _mm256_i32gather_ps(Xbase + K * 1, indices_p3, 4);
                __m256 pos41 = _mm256_i32gather_ps(Xbase + K * 1, indices_p4, 4);
                __m256 pos51 = _mm256_i32gather_ps(Xbase + K * 1, indices_p5, 4);
                __m256 pos61 = _mm256_i32gather_ps(Xbase + K * 1, indices_p6, 4);
                __m256 pos71 = _mm256_i32gather_ps(Xbase + K * 1, indices_p7, 4);
                __m256 pos02 = _mm256_i32gather_ps(Xbase + K * 2, indices_p0, 4);
                __m256 pos12 = _mm256_i32gather_ps(Xbase + K * 2, indices_p1, 4);
                __m256 pos22 = _mm256_i32gather_ps(Xbase + K * 2, indices_p2, 4);
                __m256 pos32 = _mm256_i32gather_ps(Xbase + K * 2, indices_p3, 4);
                __m256 pos42 = _mm256_i32gather_ps(Xbase + K * 2, indices_p4, 4);
                __m256 pos52 = _mm256_i32gather_ps(Xbase + K * 2, indices_p5, 4);
                __m256 pos62 = _mm256_i32gather_ps(Xbase + K * 2, indices_p6, 4);
                __m256 pos72 = _mm256_i32gather_ps(Xbase + K * 2, indices_p7, 4);
                __m256 pos03 = _mm256_i32gather_ps(Xbase + K * 3, indices_p0, 4);
                __m256 pos13 = _mm256_i32gather_ps(Xbase + K * 3, indices_p1, 4);
                __m256 pos23 = _mm256_i32gather_ps(Xbase + K * 3, indices_p2, 4);
                __m256 pos33 = _mm256_i32gather_ps(Xbase + K * 3, indices_p3, 4);
                __m256 pos43 = _mm256_i32gather_ps(Xbase + K * 3, indices_p4, 4);
                __m256 pos53 = _mm256_i32gather_ps(Xbase + K * 3, indices_p5, 4);
                __m256 pos63 = _mm256_i32gather_ps(Xbase + K * 3, indices_p6, 4);
                __m256 pos73 = _mm256_i32gather_ps(Xbase + K * 3, indices_p7, 4);
                __m256 pos04 = _mm256_i32gather_ps(Xbase + K * 4, indices_p0, 4);
                __m256 pos14 = _mm256_i32gather_ps(Xbase + K * 4, indices_p1, 4);
                __m256 pos24 = _mm256_i32gather_ps(Xbase + K * 4, indices_p2, 4);
                __m256 pos34 = _mm256_i32gather_ps(Xbase + K * 4, indices_p3, 4);
                __m256 pos44 = _mm256_i32gather_ps(Xbase + K * 4, indices_p4, 4);
                __m256 pos54 = _mm256_i32gather_ps(Xbase + K * 4, indices_p5, 4);
                __m256 pos64 = _mm256_i32gather_ps(Xbase + K * 4, indices_p6, 4);
                __m256 pos74 = _mm256_i32gather_ps(Xbase + K * 4, indices_p7, 4);
                __m256 pos05 = _mm256_i32gather_ps(Xbase + K * 5, indices_p0, 4);
                __m256 pos15 = _mm256_i32gather_ps(Xbase + K * 5, indices_p1, 4);
                __m256 pos25 = _mm256_i32gather_ps(Xbase + K * 5, indices_p2, 4);
                __m256 pos35 = _mm256_i32gather_ps(Xbase + K * 5, indices_p3, 4);
                __m256 pos45 = _mm256_i32gather_ps(Xbase + K * 5, indices_p4, 4);
                __m256 pos55 = _mm256_i32gather_ps(Xbase + K * 5, indices_p5, 4);
                __m256 pos65 = _mm256_i32gather_ps(Xbase + K * 5, indices_p6, 4);
                __m256 pos75 = _mm256_i32gather_ps(Xbase + K * 5, indices_p7, 4);
                __m256 pos06 = _mm256_i32gather_ps(Xbase + K * 6, indices_p0, 4);
                __m256 pos16 = _mm256_i32gather_ps(Xbase + K * 6, indices_p1, 4);
                __m256 pos26 = _mm256_i32gather_ps(Xbase + K * 6, indices_p2, 4);
                __m256 pos36 = _mm256_i32gather_ps(Xbase + K * 6, indices_p3, 4);
                __m256 pos46 = _mm256_i32gather_ps(Xbase + K * 6, indices_p4, 4);
                __m256 pos56 = _mm256_i32gather_ps(Xbase + K * 6, indices_p5, 4);
                __m256 pos66 = _mm256_i32gather_ps(Xbase + K * 6, indices_p6, 4);
                __m256 pos76 = _mm256_i32gather_ps(Xbase + K * 6, indices_p7, 4);
                __m256 pos07 = _mm256_i32gather_ps(Xbase + K * 7, indices_p0, 4);
                __m256 pos17 = _mm256_i32gather_ps(Xbase + K * 7, indices_p1, 4);
                __m256 pos27 = _mm256_i32gather_ps(Xbase + K * 7, indices_p2, 4);
                __m256 pos37 = _mm256_i32gather_ps(Xbase + K * 7, indices_p3, 4);
                __m256 pos47 = _mm256_i32gather_ps(Xbase + K * 7, indices_p4, 4);
                __m256 pos57 = _mm256_i32gather_ps(Xbase + K * 7, indices_p5, 4);
                __m256 pos67 = _mm256_i32gather_ps(Xbase + K * 7, indices_p6, 4);
                __m256 pos77 = _mm256_i32gather_ps(Xbase + K * 7, indices_p7, 4);
                __m256 neg00 = _mm256_i32gather_ps(Xbase + K * 0, indices_n0, 4);
                __m256 neg10 = _mm256_i32gather_ps(Xbase + K * 0, indices_n1, 4);
                __m256 neg20 = _mm256_i32gather_ps(Xbase + K * 0, indices_n2, 4);
                __m256 neg30 = _mm256_i32gather_ps(Xbase + K * 0, indices_n3, 4);
                __m256 neg40 = _mm256_i32gather_ps(Xbase + K * 0, indices_n4, 4);
                __m256 neg50 = _mm256_i32gather_ps(Xbase + K * 0, indices_n5, 4);
                __m256 neg60 = _mm256_i32gather_ps(Xbase + K * 0, indices_n6, 4);
                __m256 neg70 = _mm256_i32gather_ps(Xbase + K * 0, indices_n7, 4);
                __m256 neg01 = _mm256_i32gather_ps(Xbase + K * 1, indices_n0, 4);
                __m256 neg11 = _mm256_i32gather_ps(Xbase + K * 1, indices_n1, 4);
                __m256 neg21 = _mm256_i32gather_ps(Xbase + K * 1, indices_n2, 4);
                __m256 neg31 = _mm256_i32gather_ps(Xbase + K * 1, indices_n3, 4);
                __m256 neg41 = _mm256_i32gather_ps(Xbase + K * 1, indices_n4, 4);
                __m256 neg51 = _mm256_i32gather_ps(Xbase + K * 1, indices_n5, 4);
                __m256 neg61 = _mm256_i32gather_ps(Xbase + K * 1, indices_n6, 4);
                __m256 neg71 = _mm256_i32gather_ps(Xbase + K * 1, indices_n7, 4);
                __m256 neg02 = _mm256_i32gather_ps(Xbase + K * 2, indices_n0, 4);
                __m256 neg12 = _mm256_i32gather_ps(Xbase + K * 2, indices_n1, 4);
                __m256 neg22 = _mm256_i32gather_ps(Xbase + K * 2, indices_n2, 4);
                __m256 neg32 = _mm256_i32gather_ps(Xbase + K * 2, indices_n3, 4);
                __m256 neg42 = _mm256_i32gather_ps(Xbase + K * 2, indices_n4, 4);
                __m256 neg52 = _mm256_i32gather_ps(Xbase + K * 2, indices_n5, 4);
                __m256 neg62 = _mm256_i32gather_ps(Xbase + K * 2, indices_n6, 4);
                __m256 neg72 = _mm256_i32gather_ps(Xbase + K * 2, indices_n7, 4);
                __m256 neg03 = _mm256_i32gather_ps(Xbase + K * 3, indices_n0, 4);
                __m256 neg13 = _mm256_i32gather_ps(Xbase + K * 3, indices_n1, 4);
                __m256 neg23 = _mm256_i32gather_ps(Xbase + K * 3, indices_n2, 4);
                __m256 neg33 = _mm256_i32gather_ps(Xbase + K * 3, indices_n3, 4);
                __m256 neg43 = _mm256_i32gather_ps(Xbase + K * 3, indices_n4, 4);
                __m256 neg53 = _mm256_i32gather_ps(Xbase + K * 3, indices_n5, 4);
                __m256 neg63 = _mm256_i32gather_ps(Xbase + K * 3, indices_n6, 4);
                __m256 neg73 = _mm256_i32gather_ps(Xbase + K * 3, indices_n7, 4);
                __m256 neg04 = _mm256_i32gather_ps(Xbase + K * 4, indices_n0, 4);
                __m256 neg14 = _mm256_i32gather_ps(Xbase + K * 4, indices_n1, 4);
                __m256 neg24 = _mm256_i32gather_ps(Xbase + K * 4, indices_n2, 4);
                __m256 neg34 = _mm256_i32gather_ps(Xbase + K * 4, indices_n3, 4);
                __m256 neg44 = _mm256_i32gather_ps(Xbase + K * 4, indices_n4, 4);
                __m256 neg54 = _mm256_i32gather_ps(Xbase + K * 4, indices_n5, 4);
                __m256 neg64 = _mm256_i32gather_ps(Xbase + K * 4, indices_n6, 4);
                __m256 neg74 = _mm256_i32gather_ps(Xbase + K * 4, indices_n7, 4);
                __m256 neg05 = _mm256_i32gather_ps(Xbase + K * 5, indices_n0, 4);
                __m256 neg15 = _mm256_i32gather_ps(Xbase + K * 5, indices_n1, 4);
                __m256 neg25 = _mm256_i32gather_ps(Xbase + K * 5, indices_n2, 4);
                __m256 neg35 = _mm256_i32gather_ps(Xbase + K * 5, indices_n3, 4);
                __m256 neg45 = _mm256_i32gather_ps(Xbase + K * 5, indices_n4, 4);
                __m256 neg55 = _mm256_i32gather_ps(Xbase + K * 5, indices_n5, 4);
                __m256 neg65 = _mm256_i32gather_ps(Xbase + K * 5, indices_n6, 4);
                __m256 neg75 = _mm256_i32gather_ps(Xbase + K * 5, indices_n7, 4);
                __m256 neg06 = _mm256_i32gather_ps(Xbase + K * 6, indices_n0, 4);
                __m256 neg16 = _mm256_i32gather_ps(Xbase + K * 6, indices_n1, 4);
                __m256 neg26 = _mm256_i32gather_ps(Xbase + K * 6, indices_n2, 4);
                __m256 neg36 = _mm256_i32gather_ps(Xbase + K * 6, indices_n3, 4);
                __m256 neg46 = _mm256_i32gather_ps(Xbase + K * 6, indices_n4, 4);
                __m256 neg56 = _mm256_i32gather_ps(Xbase + K * 6, indices_n5, 4);
                __m256 neg66 = _mm256_i32gather_ps(Xbase + K * 6, indices_n6, 4);
                __m256 neg76 = _mm256_i32gather_ps(Xbase + K * 6, indices_n7, 4);
                __m256 neg07 = _mm256_i32gather_ps(Xbase + K * 7, indices_n0, 4);
                __m256 neg17 = _mm256_i32gather_ps(Xbase + K * 7, indices_n1, 4);
                __m256 neg27 = _mm256_i32gather_ps(Xbase + K * 7, indices_n2, 4);
                __m256 neg37 = _mm256_i32gather_ps(Xbase + K * 7, indices_n3, 4);
                __m256 neg47 = _mm256_i32gather_ps(Xbase + K * 7, indices_n4, 4);
                __m256 neg57 = _mm256_i32gather_ps(Xbase + K * 7, indices_n5, 4);
                __m256 neg67 = _mm256_i32gather_ps(Xbase + K * 7, indices_n6, 4);
                __m256 neg77 = _mm256_i32gather_ps(Xbase + K * 7, indices_n7, 4);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res20 = _mm256_add_ps(res20, _mm256_sub_ps(pos20, neg20));
                res30 = _mm256_add_ps(res30, _mm256_sub_ps(pos30, neg30));
                res40 = _mm256_add_ps(res40, _mm256_sub_ps(pos40, neg40));
                res50 = _mm256_add_ps(res50, _mm256_sub_ps(pos50, neg50));
                res60 = _mm256_add_ps(res60, _mm256_sub_ps(pos60, neg60));
                res70 = _mm256_add_ps(res70, _mm256_sub_ps(pos70, neg70));
                res01 = _mm256_add_ps(res01, _mm256_sub_ps(pos01, neg01));
                res11 = _mm256_add_ps(res11, _mm256_sub_ps(pos11, neg11));
                res21 = _mm256_add_ps(res21, _mm256_sub_ps(pos21, neg21));
                res31 = _mm256_add_ps(res31, _mm256_sub_ps(pos31, neg31));
                res41 = _mm256_add_ps(res41, _mm256_sub_ps(pos41, neg41));
                res51 = _mm256_add_ps(res51, _mm256_sub_ps(pos51, neg51));
                res61 = _mm256_add_ps(res61, _mm256_sub_ps(pos61, neg61));
                res71 = _mm256_add_ps(res71, _mm256_sub_ps(pos71, neg71));
                res02 = _mm256_add_ps(res02, _mm256_sub_ps(pos02, neg02));
                res12 = _mm256_add_ps(res12, _mm256_sub_ps(pos12, neg12));
                res22 = _mm256_add_ps(res22, _mm256_sub_ps(pos22, neg22));
                res32 = _mm256_add_ps(res32, _mm256_sub_ps(pos32, neg32));
                res42 = _mm256_add_ps(res42, _mm256_sub_ps(pos42, neg42));
                res52 = _mm256_add_ps(res52, _mm256_sub_ps(pos52, neg52));
                res62 = _mm256_add_ps(res62, _mm256_sub_ps(pos62, neg62));
                res72 = _mm256_add_ps(res72, _mm256_sub_ps(pos72, neg72));
                res03 = _mm256_add_ps(res03, _mm256_sub_ps(pos03, neg03));
                res13 = _mm256_add_ps(res13, _mm256_sub_ps(pos13, neg13));
                res23 = _mm256_add_ps(res23, _mm256_sub_ps(pos23, neg23));
                res33 = _mm256_add_ps(res33, _mm256_sub_ps(pos33, neg33));
                res43 = _mm256_add_ps(res43, _mm256_sub_ps(pos43, neg43));
                res53 = _mm256_add_ps(res53, _mm256_sub_ps(pos53, neg53));
                res63 = _mm256_add_ps(res63, _mm256_sub_ps(pos63, neg63));
                res73 = _mm256_add_ps(res73, _mm256_sub_ps(pos73, neg73));
                res04 = _mm256_add_ps(res04, _mm256_sub_ps(pos04, neg04));
                res14 = _mm256_add_ps(res14, _mm256_sub_ps(pos14, neg14));
                res24 = _mm256_add_ps(res24, _mm256_sub_ps(pos24, neg24));
                res34 = _mm256_add_ps(res34, _mm256_sub_ps(pos34, neg34));
                res44 = _mm256_add_ps(res44, _mm256_sub_ps(pos44, neg44));
                res54 = _mm256_add_ps(res54, _mm256_sub_ps(pos54, neg54));
                res64 = _mm256_add_ps(res64, _mm256_sub_ps(pos64, neg64));
                res74 = _mm256_add_ps(res74, _mm256_sub_ps(pos74, neg74));
                res05 = _mm256_add_ps(res05, _mm256_sub_ps(pos05, neg05));
                res15 = _mm256_add_ps(res15, _mm256_sub_ps(pos15, neg15));
                res25 = _mm256_add_ps(res25, _mm256_sub_ps(pos25, neg25));
                res35 = _mm256_add_ps(res35, _mm256_sub_ps(pos35, neg35));
                res45 = _mm256_add_ps(res45, _mm256_sub_ps(pos45, neg45));
                res55 = _mm256_add_ps(res55, _mm256_sub_ps(pos55, neg55));
                res65 = _mm256_add_ps(res65, _mm256_sub_ps(pos65, neg65));
                res75 = _mm256_add_ps(res75, _mm256_sub_ps(pos75, neg75));
                res06 = _mm256_add_ps(res06, _mm256_sub_ps(pos06, neg06));
                res16 = _mm256_add_ps(res16, _mm256_sub_ps(pos16, neg16));
                res26 = _mm256_add_ps(res26, _mm256_sub_ps(pos26, neg26));
                res36 = _mm256_add_ps(res36, _mm256_sub_ps(pos36, neg36));
                res46 = _mm256_add_ps(res46, _mm256_sub_ps(pos46, neg46));
                res56 = _mm256_add_ps(res56, _mm256_sub_ps(pos56, neg56));
                res66 = _mm256_add_ps(res66, _mm256_sub_ps(pos66, neg66));
                res76 = _mm256_add_ps(res76, _mm256_sub_ps(pos76, neg76));
                res07 = _mm256_add_ps(res07, _mm256_sub_ps(pos07, neg07));
                res17 = _mm256_add_ps(res17, _mm256_sub_ps(pos17, neg17));
                res27 = _mm256_add_ps(res27, _mm256_sub_ps(pos27, neg27));
                res37 = _mm256_add_ps(res37, _mm256_sub_ps(pos37, neg37));
                res47 = _mm256_add_ps(res47, _mm256_sub_ps(pos47, neg47));
                res57 = _mm256_add_ps(res57, _mm256_sub_ps(pos57, neg57));
                res67 = _mm256_add_ps(res67, _mm256_sub_ps(pos67, neg67));
                res77 = _mm256_add_ps(res77, _mm256_sub_ps(pos77, neg77));
            }
            float* Ybase = result + i * N_COL + j * 64;
            _mm256_store_ps(Ybase + N_COL * 0 + 0, res00);
            _mm256_store_ps(Ybase + N_COL * 0 + 8, res10);
            _mm256_store_ps(Ybase + N_COL * 0 + 16, res20);
            _mm256_store_ps(Ybase + N_COL * 0 + 24, res30);
            _mm256_store_ps(Ybase + N_COL * 0 + 32, res40);
            _mm256_store_ps(Ybase + N_COL * 0 + 40, res50);
            _mm256_store_ps(Ybase + N_COL * 0 + 48, res60);
            _mm256_store_ps(Ybase + N_COL * 0 + 56, res70);
            _mm256_store_ps(Ybase + N_COL * 1 + 0, res01);
            _mm256_store_ps(Ybase + N_COL * 1 + 8, res11);
            _mm256_store_ps(Ybase + N_COL * 1 + 16, res21);
            _mm256_store_ps(Ybase + N_COL * 1 + 24, res31);
            _mm256_store_ps(Ybase + N_COL * 1 + 32, res41);
            _mm256_store_ps(Ybase + N_COL * 1 + 40, res51);
            _mm256_store_ps(Ybase + N_COL * 1 + 48, res61);
            _mm256_store_ps(Ybase + N_COL * 1 + 56, res71);
            _mm256_store_ps(Ybase + N_COL * 2 + 0, res02);
            _mm256_store_ps(Ybase + N_COL * 2 + 8, res12);
            _mm256_store_ps(Ybase + N_COL * 2 + 16, res22);
            _mm256_store_ps(Ybase + N_COL * 2 + 24, res32);
            _mm256_store_ps(Ybase + N_COL * 2 + 32, res42);
            _mm256_store_ps(Ybase + N_COL * 2 + 40, res52);
            _mm256_store_ps(Ybase + N_COL * 2 + 48, res62);
            _mm256_store_ps(Ybase + N_COL * 2 + 56, res72);
            _mm256_store_ps(Ybase + N_COL * 3 + 0, res03);
            _mm256_store_ps(Ybase + N_COL * 3 + 8, res13);
            _mm256_store_ps(Ybase + N_COL * 3 + 16, res23);
            _mm256_store_ps(Ybase + N_COL * 3 + 24, res33);
            _mm256_store_ps(Ybase + N_COL * 3 + 32, res43);
            _mm256_store_ps(Ybase + N_COL * 3 + 40, res53);
            _mm256_store_ps(Ybase + N_COL * 3 + 48, res63);
            _mm256_store_ps(Ybase + N_COL * 3 + 56, res73);
            _mm256_store_ps(Ybase + N_COL * 4 + 0, res04);
            _mm256_store_ps(Ybase + N_COL * 4 + 8, res14);
            _mm256_store_ps(Ybase + N_COL * 4 + 16, res24);
            _mm256_store_ps(Ybase + N_COL * 4 + 24, res34);
            _mm256_store_ps(Ybase + N_COL * 4 + 32, res44);
            _mm256_store_ps(Ybase + N_COL * 4 + 40, res54);
            _mm256_store_ps(Ybase + N_COL * 4 + 48, res64);
            _mm256_store_ps(Ybase + N_COL * 4 + 56, res74);
            _mm256_store_ps(Ybase + N_COL * 5 + 0, res05);
            _mm256_store_ps(Ybase + N_COL * 5 + 8, res15);
            _mm256_store_ps(Ybase + N_COL * 5 + 16, res25);
            _mm256_store_ps(Ybase + N_COL * 5 + 24, res35);
            _mm256_store_ps(Ybase + N_COL * 5 + 32, res45);
            _mm256_store_ps(Ybase + N_COL * 5 + 40, res55);
            _mm256_store_ps(Ybase + N_COL * 5 + 48, res65);
            _mm256_store_ps(Ybase + N_COL * 5 + 56, res75);
            _mm256_store_ps(Ybase + N_COL * 6 + 0, res06);
            _mm256_store_ps(Ybase + N_COL * 6 + 8, res16);
            _mm256_store_ps(Ybase + N_COL * 6 + 16, res26);
            _mm256_store_ps(Ybase + N_COL * 6 + 24, res36);
            _mm256_store_ps(Ybase + N_COL * 6 + 32, res46);
            _mm256_store_ps(Ybase + N_COL * 6 + 40, res56);
            _mm256_store_ps(Ybase + N_COL * 6 + 48, res66);
            _mm256_store_ps(Ybase + N_COL * 6 + 56, res76);
            _mm256_store_ps(Ybase + N_COL * 7 + 0, res07);
            _mm256_store_ps(Ybase + N_COL * 7 + 8, res17);
            _mm256_store_ps(Ybase + N_COL * 7 + 16, res27);
            _mm256_store_ps(Ybase + N_COL * 7 + 24, res37);
            _mm256_store_ps(Ybase + N_COL * 7 + 32, res47);
            _mm256_store_ps(Ybase + N_COL * 7 + 40, res57);
            _mm256_store_ps(Ybase + N_COL * 7 + 48, res67);
            _mm256_store_ps(Ybase + N_COL * 7 + 56, res77);
            for (int g = 0; g < 64; g++) {
                for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++) {
                    Ybase[N_COL * 0 + g] += Xbase[K * 0 + row_index[k]];
                    Ybase[N_COL * 1 + g] += Xbase[K * 1 + row_index[k]];
                    Ybase[N_COL * 2 + g] += Xbase[K * 2 + row_index[k]];
                    Ybase[N_COL * 3 + g] += Xbase[K * 3 + row_index[k]];
                    Ybase[N_COL * 4 + g] += Xbase[K * 4 + row_index[k]];
                    Ybase[N_COL * 5 + g] += Xbase[K * 5 + row_index[k]];
                    Ybase[N_COL * 6 + g] += Xbase[K * 6 + row_index[k]];
                    Ybase[N_COL * 7 + g] += Xbase[K * 7 + row_index[k]];
                }
                for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++) {
                    Ybase[N_COL * 0 + g] -= Xbase[K * 0 + row_index[k]];
                    Ybase[N_COL * 1 + g] -= Xbase[K * 1 + row_index[k]];
                    Ybase[N_COL * 2 + g] -= Xbase[K * 2 + row_index[k]];
                    Ybase[N_COL * 3 + g] -= Xbase[K * 3 + row_index[k]];
                    Ybase[N_COL * 4 + g] -= Xbase[K * 4 + row_index[k]];
                    Ybase[N_COL * 5 + g] -= Xbase[K * 5 + row_index[k]];
                    Ybase[N_COL * 6 + g] -= Xbase[K * 6 + row_index[k]];
                    Ybase[N_COL * 7 + g] -= Xbase[K * 7 + row_index[k]];
                }
            }
        }
    }
}

void GEMM_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_16xG64_AVX2_OpenMP(float* X, int32_t* metadata, int32_t* row_index, float* result, int M_ROW, int N_COL, int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 64; j++) {
        for (int i = 0; i < M_ROW; i += 16) {
            float* Xbase = X + i * K;
            const int* groupData  = &metadata[j * 130];
            __m256 res00 = _mm256_setzero_ps();
            __m256 res01 = _mm256_setzero_ps();
            __m256 res02 = _mm256_setzero_ps();
            __m256 res03 = _mm256_setzero_ps();
            __m256 res04 = _mm256_setzero_ps();
            __m256 res05 = _mm256_setzero_ps();
            __m256 res06 = _mm256_setzero_ps();
            __m256 res07 = _mm256_setzero_ps();
            __m256 res08 = _mm256_setzero_ps();
            __m256 res09 = _mm256_setzero_ps();
            __m256 res010 = _mm256_setzero_ps();
            __m256 res011 = _mm256_setzero_ps();
            __m256 res012 = _mm256_setzero_ps();
            __m256 res013 = _mm256_setzero_ps();
            __m256 res014 = _mm256_setzero_ps();
            __m256 res015 = _mm256_setzero_ps();
            __m256 res10 = _mm256_setzero_ps();
            __m256 res11 = _mm256_setzero_ps();
            __m256 res12 = _mm256_setzero_ps();
            __m256 res13 = _mm256_setzero_ps();
            __m256 res14 = _mm256_setzero_ps();
            __m256 res15 = _mm256_setzero_ps();
            __m256 res16 = _mm256_setzero_ps();
            __m256 res17 = _mm256_setzero_ps();
            __m256 res18 = _mm256_setzero_ps();
            __m256 res19 = _mm256_setzero_ps();
            __m256 res110 = _mm256_setzero_ps();
            __m256 res111 = _mm256_setzero_ps();
            __m256 res112 = _mm256_setzero_ps();
            __m256 res113 = _mm256_setzero_ps();
            __m256 res114 = _mm256_setzero_ps();
            __m256 res115 = _mm256_setzero_ps();
            __m256 res20 = _mm256_setzero_ps();
            __m256 res21 = _mm256_setzero_ps();
            __m256 res22 = _mm256_setzero_ps();
            __m256 res23 = _mm256_setzero_ps();
            __m256 res24 = _mm256_setzero_ps();
            __m256 res25 = _mm256_setzero_ps();
            __m256 res26 = _mm256_setzero_ps();
            __m256 res27 = _mm256_setzero_ps();
            __m256 res28 = _mm256_setzero_ps();
            __m256 res29 = _mm256_setzero_ps();
            __m256 res210 = _mm256_setzero_ps();
            __m256 res211 = _mm256_setzero_ps();
            __m256 res212 = _mm256_setzero_ps();
            __m256 res213 = _mm256_setzero_ps();
            __m256 res214 = _mm256_setzero_ps();
            __m256 res215 = _mm256_setzero_ps();
            __m256 res30 = _mm256_setzero_ps();
            __m256 res31 = _mm256_setzero_ps();
            __m256 res32 = _mm256_setzero_ps();
            __m256 res33 = _mm256_setzero_ps();
            __m256 res34 = _mm256_setzero_ps();
            __m256 res35 = _mm256_setzero_ps();
            __m256 res36 = _mm256_setzero_ps();
            __m256 res37 = _mm256_setzero_ps();
            __m256 res38 = _mm256_setzero_ps();
            __m256 res39 = _mm256_setzero_ps();
            __m256 res310 = _mm256_setzero_ps();
            __m256 res311 = _mm256_setzero_ps();
            __m256 res312 = _mm256_setzero_ps();
            __m256 res313 = _mm256_setzero_ps();
            __m256 res314 = _mm256_setzero_ps();
            __m256 res315 = _mm256_setzero_ps();
            __m256 res40 = _mm256_setzero_ps();
            __m256 res41 = _mm256_setzero_ps();
            __m256 res42 = _mm256_setzero_ps();
            __m256 res43 = _mm256_setzero_ps();
            __m256 res44 = _mm256_setzero_ps();
            __m256 res45 = _mm256_setzero_ps();
            __m256 res46 = _mm256_setzero_ps();
            __m256 res47 = _mm256_setzero_ps();
            __m256 res48 = _mm256_setzero_ps();
            __m256 res49 = _mm256_setzero_ps();
            __m256 res410 = _mm256_setzero_ps();
            __m256 res411 = _mm256_setzero_ps();
            __m256 res412 = _mm256_setzero_ps();
            __m256 res413 = _mm256_setzero_ps();
            __m256 res414 = _mm256_setzero_ps();
            __m256 res415 = _mm256_setzero_ps();
            __m256 res50 = _mm256_setzero_ps();
            __m256 res51 = _mm256_setzero_ps();
            __m256 res52 = _mm256_setzero_ps();
            __m256 res53 = _mm256_setzero_ps();
            __m256 res54 = _mm256_setzero_ps();
            __m256 res55 = _mm256_setzero_ps();
            __m256 res56 = _mm256_setzero_ps();
            __m256 res57 = _mm256_setzero_ps();
            __m256 res58 = _mm256_setzero_ps();
            __m256 res59 = _mm256_setzero_ps();
            __m256 res510 = _mm256_setzero_ps();
            __m256 res511 = _mm256_setzero_ps();
            __m256 res512 = _mm256_setzero_ps();
            __m256 res513 = _mm256_setzero_ps();
            __m256 res514 = _mm256_setzero_ps();
            __m256 res515 = _mm256_setzero_ps();
            __m256 res60 = _mm256_setzero_ps();
            __m256 res61 = _mm256_setzero_ps();
            __m256 res62 = _mm256_setzero_ps();
            __m256 res63 = _mm256_setzero_ps();
            __m256 res64 = _mm256_setzero_ps();
            __m256 res65 = _mm256_setzero_ps();
            __m256 res66 = _mm256_setzero_ps();
            __m256 res67 = _mm256_setzero_ps();
            __m256 res68 = _mm256_setzero_ps();
            __m256 res69 = _mm256_setzero_ps();
            __m256 res610 = _mm256_setzero_ps();
            __m256 res611 = _mm256_setzero_ps();
            __m256 res612 = _mm256_setzero_ps();
            __m256 res613 = _mm256_setzero_ps();
            __m256 res614 = _mm256_setzero_ps();
            __m256 res615 = _mm256_setzero_ps();
            __m256 res70 = _mm256_setzero_ps();
            __m256 res71 = _mm256_setzero_ps();
            __m256 res72 = _mm256_setzero_ps();
            __m256 res73 = _mm256_setzero_ps();
            __m256 res74 = _mm256_setzero_ps();
            __m256 res75 = _mm256_setzero_ps();
            __m256 res76 = _mm256_setzero_ps();
            __m256 res77 = _mm256_setzero_ps();
            __m256 res78 = _mm256_setzero_ps();
            __m256 res79 = _mm256_setzero_ps();
            __m256 res710 = _mm256_setzero_ps();
            __m256 res711 = _mm256_setzero_ps();
            __m256 res712 = _mm256_setzero_ps();
            __m256 res713 = _mm256_setzero_ps();
            __m256 res714 = _mm256_setzero_ps();
            __m256 res715 = _mm256_setzero_ps();
            for (int k = groupData[0]; k < groupData[1]; k += 128) {
                __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
                __m256i indices_p1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
                __m256i indices_p2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 16));
                __m256i indices_p3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 24));
                __m256i indices_p4 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 32));
                __m256i indices_p5 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 40));
                __m256i indices_p6 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 48));
                __m256i indices_p7 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 56));
                __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 64));
                __m256i indices_n1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 72));
                __m256i indices_n2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 80));
                __m256i indices_n3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 88));
                __m256i indices_n4 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 96));
                __m256i indices_n5 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 104));
                __m256i indices_n6 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 112));
                __m256i indices_n7 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 120));
                __m256 pos00 = _mm256_i32gather_ps(Xbase + K * 0, indices_p0, 4);
                __m256 pos10 = _mm256_i32gather_ps(Xbase + K * 0, indices_p1, 4);
                __m256 pos20 = _mm256_i32gather_ps(Xbase + K * 0, indices_p2, 4);
                __m256 pos30 = _mm256_i32gather_ps(Xbase + K * 0, indices_p3, 4);
                __m256 pos40 = _mm256_i32gather_ps(Xbase + K * 0, indices_p4, 4);
                __m256 pos50 = _mm256_i32gather_ps(Xbase + K * 0, indices_p5, 4);
                __m256 pos60 = _mm256_i32gather_ps(Xbase + K * 0, indices_p6, 4);
                __m256 pos70 = _mm256_i32gather_ps(Xbase + K * 0, indices_p7, 4);
                __m256 pos01 = _mm256_i32gather_ps(Xbase + K * 1, indices_p0, 4);
                __m256 pos11 = _mm256_i32gather_ps(Xbase + K * 1, indices_p1, 4);
                __m256 pos21 = _mm256_i32gather_ps(Xbase + K * 1, indices_p2, 4);
                __m256 pos31 = _mm256_i32gather_ps(Xbase + K * 1, indices_p3, 4);
                __m256 pos41 = _mm256_i32gather_ps(Xbase + K * 1, indices_p4, 4);
                __m256 pos51 = _mm256_i32gather_ps(Xbase + K * 1, indices_p5, 4);
                __m256 pos61 = _mm256_i32gather_ps(Xbase + K * 1, indices_p6, 4);
                __m256 pos71 = _mm256_i32gather_ps(Xbase + K * 1, indices_p7, 4);
                __m256 pos02 = _mm256_i32gather_ps(Xbase + K * 2, indices_p0, 4);
                __m256 pos12 = _mm256_i32gather_ps(Xbase + K * 2, indices_p1, 4);
                __m256 pos22 = _mm256_i32gather_ps(Xbase + K * 2, indices_p2, 4);
                __m256 pos32 = _mm256_i32gather_ps(Xbase + K * 2, indices_p3, 4);
                __m256 pos42 = _mm256_i32gather_ps(Xbase + K * 2, indices_p4, 4);
                __m256 pos52 = _mm256_i32gather_ps(Xbase + K * 2, indices_p5, 4);
                __m256 pos62 = _mm256_i32gather_ps(Xbase + K * 2, indices_p6, 4);
                __m256 pos72 = _mm256_i32gather_ps(Xbase + K * 2, indices_p7, 4);
                __m256 pos03 = _mm256_i32gather_ps(Xbase + K * 3, indices_p0, 4);
                __m256 pos13 = _mm256_i32gather_ps(Xbase + K * 3, indices_p1, 4);
                __m256 pos23 = _mm256_i32gather_ps(Xbase + K * 3, indices_p2, 4);
                __m256 pos33 = _mm256_i32gather_ps(Xbase + K * 3, indices_p3, 4);
                __m256 pos43 = _mm256_i32gather_ps(Xbase + K * 3, indices_p4, 4);
                __m256 pos53 = _mm256_i32gather_ps(Xbase + K * 3, indices_p5, 4);
                __m256 pos63 = _mm256_i32gather_ps(Xbase + K * 3, indices_p6, 4);
                __m256 pos73 = _mm256_i32gather_ps(Xbase + K * 3, indices_p7, 4);
                __m256 pos04 = _mm256_i32gather_ps(Xbase + K * 4, indices_p0, 4);
                __m256 pos14 = _mm256_i32gather_ps(Xbase + K * 4, indices_p1, 4);
                __m256 pos24 = _mm256_i32gather_ps(Xbase + K * 4, indices_p2, 4);
                __m256 pos34 = _mm256_i32gather_ps(Xbase + K * 4, indices_p3, 4);
                __m256 pos44 = _mm256_i32gather_ps(Xbase + K * 4, indices_p4, 4);
                __m256 pos54 = _mm256_i32gather_ps(Xbase + K * 4, indices_p5, 4);
                __m256 pos64 = _mm256_i32gather_ps(Xbase + K * 4, indices_p6, 4);
                __m256 pos74 = _mm256_i32gather_ps(Xbase + K * 4, indices_p7, 4);
                __m256 pos05 = _mm256_i32gather_ps(Xbase + K * 5, indices_p0, 4);
                __m256 pos15 = _mm256_i32gather_ps(Xbase + K * 5, indices_p1, 4);
                __m256 pos25 = _mm256_i32gather_ps(Xbase + K * 5, indices_p2, 4);
                __m256 pos35 = _mm256_i32gather_ps(Xbase + K * 5, indices_p3, 4);
                __m256 pos45 = _mm256_i32gather_ps(Xbase + K * 5, indices_p4, 4);
                __m256 pos55 = _mm256_i32gather_ps(Xbase + K * 5, indices_p5, 4);
                __m256 pos65 = _mm256_i32gather_ps(Xbase + K * 5, indices_p6, 4);
                __m256 pos75 = _mm256_i32gather_ps(Xbase + K * 5, indices_p7, 4);
                __m256 pos06 = _mm256_i32gather_ps(Xbase + K * 6, indices_p0, 4);
                __m256 pos16 = _mm256_i32gather_ps(Xbase + K * 6, indices_p1, 4);
                __m256 pos26 = _mm256_i32gather_ps(Xbase + K * 6, indices_p2, 4);
                __m256 pos36 = _mm256_i32gather_ps(Xbase + K * 6, indices_p3, 4);
                __m256 pos46 = _mm256_i32gather_ps(Xbase + K * 6, indices_p4, 4);
                __m256 pos56 = _mm256_i32gather_ps(Xbase + K * 6, indices_p5, 4);
                __m256 pos66 = _mm256_i32gather_ps(Xbase + K * 6, indices_p6, 4);
                __m256 pos76 = _mm256_i32gather_ps(Xbase + K * 6, indices_p7, 4);
                __m256 pos07 = _mm256_i32gather_ps(Xbase + K * 7, indices_p0, 4);
                __m256 pos17 = _mm256_i32gather_ps(Xbase + K * 7, indices_p1, 4);
                __m256 pos27 = _mm256_i32gather_ps(Xbase + K * 7, indices_p2, 4);
                __m256 pos37 = _mm256_i32gather_ps(Xbase + K * 7, indices_p3, 4);
                __m256 pos47 = _mm256_i32gather_ps(Xbase + K * 7, indices_p4, 4);
                __m256 pos57 = _mm256_i32gather_ps(Xbase + K * 7, indices_p5, 4);
                __m256 pos67 = _mm256_i32gather_ps(Xbase + K * 7, indices_p6, 4);
                __m256 pos77 = _mm256_i32gather_ps(Xbase + K * 7, indices_p7, 4);
                __m256 pos08 = _mm256_i32gather_ps(Xbase + K * 8, indices_p0, 4);
                __m256 pos18 = _mm256_i32gather_ps(Xbase + K * 8, indices_p1, 4);
                __m256 pos28 = _mm256_i32gather_ps(Xbase + K * 8, indices_p2, 4);
                __m256 pos38 = _mm256_i32gather_ps(Xbase + K * 8, indices_p3, 4);
                __m256 pos48 = _mm256_i32gather_ps(Xbase + K * 8, indices_p4, 4);
                __m256 pos58 = _mm256_i32gather_ps(Xbase + K * 8, indices_p5, 4);
                __m256 pos68 = _mm256_i32gather_ps(Xbase + K * 8, indices_p6, 4);
                __m256 pos78 = _mm256_i32gather_ps(Xbase + K * 8, indices_p7, 4);
                __m256 pos09 = _mm256_i32gather_ps(Xbase + K * 9, indices_p0, 4);
                __m256 pos19 = _mm256_i32gather_ps(Xbase + K * 9, indices_p1, 4);
                __m256 pos29 = _mm256_i32gather_ps(Xbase + K * 9, indices_p2, 4);
                __m256 pos39 = _mm256_i32gather_ps(Xbase + K * 9, indices_p3, 4);
                __m256 pos49 = _mm256_i32gather_ps(Xbase + K * 9, indices_p4, 4);
                __m256 pos59 = _mm256_i32gather_ps(Xbase + K * 9, indices_p5, 4);
                __m256 pos69 = _mm256_i32gather_ps(Xbase + K * 9, indices_p6, 4);
                __m256 pos79 = _mm256_i32gather_ps(Xbase + K * 9, indices_p7, 4);
                __m256 pos010 = _mm256_i32gather_ps(Xbase + K * 10, indices_p0, 4);
                __m256 pos110 = _mm256_i32gather_ps(Xbase + K * 10, indices_p1, 4);
                __m256 pos210 = _mm256_i32gather_ps(Xbase + K * 10, indices_p2, 4);
                __m256 pos310 = _mm256_i32gather_ps(Xbase + K * 10, indices_p3, 4);
                __m256 pos410 = _mm256_i32gather_ps(Xbase + K * 10, indices_p4, 4);
                __m256 pos510 = _mm256_i32gather_ps(Xbase + K * 10, indices_p5, 4);
                __m256 pos610 = _mm256_i32gather_ps(Xbase + K * 10, indices_p6, 4);
                __m256 pos710 = _mm256_i32gather_ps(Xbase + K * 10, indices_p7, 4);
                __m256 pos011 = _mm256_i32gather_ps(Xbase + K * 11, indices_p0, 4);
                __m256 pos111 = _mm256_i32gather_ps(Xbase + K * 11, indices_p1, 4);
                __m256 pos211 = _mm256_i32gather_ps(Xbase + K * 11, indices_p2, 4);
                __m256 pos311 = _mm256_i32gather_ps(Xbase + K * 11, indices_p3, 4);
                __m256 pos411 = _mm256_i32gather_ps(Xbase + K * 11, indices_p4, 4);
                __m256 pos511 = _mm256_i32gather_ps(Xbase + K * 11, indices_p5, 4);
                __m256 pos611 = _mm256_i32gather_ps(Xbase + K * 11, indices_p6, 4);
                __m256 pos711 = _mm256_i32gather_ps(Xbase + K * 11, indices_p7, 4);
                __m256 pos012 = _mm256_i32gather_ps(Xbase + K * 12, indices_p0, 4);
                __m256 pos112 = _mm256_i32gather_ps(Xbase + K * 12, indices_p1, 4);
                __m256 pos212 = _mm256_i32gather_ps(Xbase + K * 12, indices_p2, 4);
                __m256 pos312 = _mm256_i32gather_ps(Xbase + K * 12, indices_p3, 4);
                __m256 pos412 = _mm256_i32gather_ps(Xbase + K * 12, indices_p4, 4);
                __m256 pos512 = _mm256_i32gather_ps(Xbase + K * 12, indices_p5, 4);
                __m256 pos612 = _mm256_i32gather_ps(Xbase + K * 12, indices_p6, 4);
                __m256 pos712 = _mm256_i32gather_ps(Xbase + K * 12, indices_p7, 4);
                __m256 pos013 = _mm256_i32gather_ps(Xbase + K * 13, indices_p0, 4);
                __m256 pos113 = _mm256_i32gather_ps(Xbase + K * 13, indices_p1, 4);
                __m256 pos213 = _mm256_i32gather_ps(Xbase + K * 13, indices_p2, 4);
                __m256 pos313 = _mm256_i32gather_ps(Xbase + K * 13, indices_p3, 4);
                __m256 pos413 = _mm256_i32gather_ps(Xbase + K * 13, indices_p4, 4);
                __m256 pos513 = _mm256_i32gather_ps(Xbase + K * 13, indices_p5, 4);
                __m256 pos613 = _mm256_i32gather_ps(Xbase + K * 13, indices_p6, 4);
                __m256 pos713 = _mm256_i32gather_ps(Xbase + K * 13, indices_p7, 4);
                __m256 pos014 = _mm256_i32gather_ps(Xbase + K * 14, indices_p0, 4);
                __m256 pos114 = _mm256_i32gather_ps(Xbase + K * 14, indices_p1, 4);
                __m256 pos214 = _mm256_i32gather_ps(Xbase + K * 14, indices_p2, 4);
                __m256 pos314 = _mm256_i32gather_ps(Xbase + K * 14, indices_p3, 4);
                __m256 pos414 = _mm256_i32gather_ps(Xbase + K * 14, indices_p4, 4);
                __m256 pos514 = _mm256_i32gather_ps(Xbase + K * 14, indices_p5, 4);
                __m256 pos614 = _mm256_i32gather_ps(Xbase + K * 14, indices_p6, 4);
                __m256 pos714 = _mm256_i32gather_ps(Xbase + K * 14, indices_p7, 4);
                __m256 pos015 = _mm256_i32gather_ps(Xbase + K * 15, indices_p0, 4);
                __m256 pos115 = _mm256_i32gather_ps(Xbase + K * 15, indices_p1, 4);
                __m256 pos215 = _mm256_i32gather_ps(Xbase + K * 15, indices_p2, 4);
                __m256 pos315 = _mm256_i32gather_ps(Xbase + K * 15, indices_p3, 4);
                __m256 pos415 = _mm256_i32gather_ps(Xbase + K * 15, indices_p4, 4);
                __m256 pos515 = _mm256_i32gather_ps(Xbase + K * 15, indices_p5, 4);
                __m256 pos615 = _mm256_i32gather_ps(Xbase + K * 15, indices_p6, 4);
                __m256 pos715 = _mm256_i32gather_ps(Xbase + K * 15, indices_p7, 4);
                __m256 neg00 = _mm256_i32gather_ps(Xbase + K * 0, indices_n0, 4);
                __m256 neg10 = _mm256_i32gather_ps(Xbase + K * 0, indices_n1, 4);
                __m256 neg20 = _mm256_i32gather_ps(Xbase + K * 0, indices_n2, 4);
                __m256 neg30 = _mm256_i32gather_ps(Xbase + K * 0, indices_n3, 4);
                __m256 neg40 = _mm256_i32gather_ps(Xbase + K * 0, indices_n4, 4);
                __m256 neg50 = _mm256_i32gather_ps(Xbase + K * 0, indices_n5, 4);
                __m256 neg60 = _mm256_i32gather_ps(Xbase + K * 0, indices_n6, 4);
                __m256 neg70 = _mm256_i32gather_ps(Xbase + K * 0, indices_n7, 4);
                __m256 neg01 = _mm256_i32gather_ps(Xbase + K * 1, indices_n0, 4);
                __m256 neg11 = _mm256_i32gather_ps(Xbase + K * 1, indices_n1, 4);
                __m256 neg21 = _mm256_i32gather_ps(Xbase + K * 1, indices_n2, 4);
                __m256 neg31 = _mm256_i32gather_ps(Xbase + K * 1, indices_n3, 4);
                __m256 neg41 = _mm256_i32gather_ps(Xbase + K * 1, indices_n4, 4);
                __m256 neg51 = _mm256_i32gather_ps(Xbase + K * 1, indices_n5, 4);
                __m256 neg61 = _mm256_i32gather_ps(Xbase + K * 1, indices_n6, 4);
                __m256 neg71 = _mm256_i32gather_ps(Xbase + K * 1, indices_n7, 4);
                __m256 neg02 = _mm256_i32gather_ps(Xbase + K * 2, indices_n0, 4);
                __m256 neg12 = _mm256_i32gather_ps(Xbase + K * 2, indices_n1, 4);
                __m256 neg22 = _mm256_i32gather_ps(Xbase + K * 2, indices_n2, 4);
                __m256 neg32 = _mm256_i32gather_ps(Xbase + K * 2, indices_n3, 4);
                __m256 neg42 = _mm256_i32gather_ps(Xbase + K * 2, indices_n4, 4);
                __m256 neg52 = _mm256_i32gather_ps(Xbase + K * 2, indices_n5, 4);
                __m256 neg62 = _mm256_i32gather_ps(Xbase + K * 2, indices_n6, 4);
                __m256 neg72 = _mm256_i32gather_ps(Xbase + K * 2, indices_n7, 4);
                __m256 neg03 = _mm256_i32gather_ps(Xbase + K * 3, indices_n0, 4);
                __m256 neg13 = _mm256_i32gather_ps(Xbase + K * 3, indices_n1, 4);
                __m256 neg23 = _mm256_i32gather_ps(Xbase + K * 3, indices_n2, 4);
                __m256 neg33 = _mm256_i32gather_ps(Xbase + K * 3, indices_n3, 4);
                __m256 neg43 = _mm256_i32gather_ps(Xbase + K * 3, indices_n4, 4);
                __m256 neg53 = _mm256_i32gather_ps(Xbase + K * 3, indices_n5, 4);
                __m256 neg63 = _mm256_i32gather_ps(Xbase + K * 3, indices_n6, 4);
                __m256 neg73 = _mm256_i32gather_ps(Xbase + K * 3, indices_n7, 4);
                __m256 neg04 = _mm256_i32gather_ps(Xbase + K * 4, indices_n0, 4);
                __m256 neg14 = _mm256_i32gather_ps(Xbase + K * 4, indices_n1, 4);
                __m256 neg24 = _mm256_i32gather_ps(Xbase + K * 4, indices_n2, 4);
                __m256 neg34 = _mm256_i32gather_ps(Xbase + K * 4, indices_n3, 4);
                __m256 neg44 = _mm256_i32gather_ps(Xbase + K * 4, indices_n4, 4);
                __m256 neg54 = _mm256_i32gather_ps(Xbase + K * 4, indices_n5, 4);
                __m256 neg64 = _mm256_i32gather_ps(Xbase + K * 4, indices_n6, 4);
                __m256 neg74 = _mm256_i32gather_ps(Xbase + K * 4, indices_n7, 4);
                __m256 neg05 = _mm256_i32gather_ps(Xbase + K * 5, indices_n0, 4);
                __m256 neg15 = _mm256_i32gather_ps(Xbase + K * 5, indices_n1, 4);
                __m256 neg25 = _mm256_i32gather_ps(Xbase + K * 5, indices_n2, 4);
                __m256 neg35 = _mm256_i32gather_ps(Xbase + K * 5, indices_n3, 4);
                __m256 neg45 = _mm256_i32gather_ps(Xbase + K * 5, indices_n4, 4);
                __m256 neg55 = _mm256_i32gather_ps(Xbase + K * 5, indices_n5, 4);
                __m256 neg65 = _mm256_i32gather_ps(Xbase + K * 5, indices_n6, 4);
                __m256 neg75 = _mm256_i32gather_ps(Xbase + K * 5, indices_n7, 4);
                __m256 neg06 = _mm256_i32gather_ps(Xbase + K * 6, indices_n0, 4);
                __m256 neg16 = _mm256_i32gather_ps(Xbase + K * 6, indices_n1, 4);
                __m256 neg26 = _mm256_i32gather_ps(Xbase + K * 6, indices_n2, 4);
                __m256 neg36 = _mm256_i32gather_ps(Xbase + K * 6, indices_n3, 4);
                __m256 neg46 = _mm256_i32gather_ps(Xbase + K * 6, indices_n4, 4);
                __m256 neg56 = _mm256_i32gather_ps(Xbase + K * 6, indices_n5, 4);
                __m256 neg66 = _mm256_i32gather_ps(Xbase + K * 6, indices_n6, 4);
                __m256 neg76 = _mm256_i32gather_ps(Xbase + K * 6, indices_n7, 4);
                __m256 neg07 = _mm256_i32gather_ps(Xbase + K * 7, indices_n0, 4);
                __m256 neg17 = _mm256_i32gather_ps(Xbase + K * 7, indices_n1, 4);
                __m256 neg27 = _mm256_i32gather_ps(Xbase + K * 7, indices_n2, 4);
                __m256 neg37 = _mm256_i32gather_ps(Xbase + K * 7, indices_n3, 4);
                __m256 neg47 = _mm256_i32gather_ps(Xbase + K * 7, indices_n4, 4);
                __m256 neg57 = _mm256_i32gather_ps(Xbase + K * 7, indices_n5, 4);
                __m256 neg67 = _mm256_i32gather_ps(Xbase + K * 7, indices_n6, 4);
                __m256 neg77 = _mm256_i32gather_ps(Xbase + K * 7, indices_n7, 4);
                __m256 neg08 = _mm256_i32gather_ps(Xbase + K * 8, indices_n0, 4);
                __m256 neg18 = _mm256_i32gather_ps(Xbase + K * 8, indices_n1, 4);
                __m256 neg28 = _mm256_i32gather_ps(Xbase + K * 8, indices_n2, 4);
                __m256 neg38 = _mm256_i32gather_ps(Xbase + K * 8, indices_n3, 4);
                __m256 neg48 = _mm256_i32gather_ps(Xbase + K * 8, indices_n4, 4);
                __m256 neg58 = _mm256_i32gather_ps(Xbase + K * 8, indices_n5, 4);
                __m256 neg68 = _mm256_i32gather_ps(Xbase + K * 8, indices_n6, 4);
                __m256 neg78 = _mm256_i32gather_ps(Xbase + K * 8, indices_n7, 4);
                __m256 neg09 = _mm256_i32gather_ps(Xbase + K * 9, indices_n0, 4);
                __m256 neg19 = _mm256_i32gather_ps(Xbase + K * 9, indices_n1, 4);
                __m256 neg29 = _mm256_i32gather_ps(Xbase + K * 9, indices_n2, 4);
                __m256 neg39 = _mm256_i32gather_ps(Xbase + K * 9, indices_n3, 4);
                __m256 neg49 = _mm256_i32gather_ps(Xbase + K * 9, indices_n4, 4);
                __m256 neg59 = _mm256_i32gather_ps(Xbase + K * 9, indices_n5, 4);
                __m256 neg69 = _mm256_i32gather_ps(Xbase + K * 9, indices_n6, 4);
                __m256 neg79 = _mm256_i32gather_ps(Xbase + K * 9, indices_n7, 4);
                __m256 neg010 = _mm256_i32gather_ps(Xbase + K * 10, indices_n0, 4);
                __m256 neg110 = _mm256_i32gather_ps(Xbase + K * 10, indices_n1, 4);
                __m256 neg210 = _mm256_i32gather_ps(Xbase + K * 10, indices_n2, 4);
                __m256 neg310 = _mm256_i32gather_ps(Xbase + K * 10, indices_n3, 4);
                __m256 neg410 = _mm256_i32gather_ps(Xbase + K * 10, indices_n4, 4);
                __m256 neg510 = _mm256_i32gather_ps(Xbase + K * 10, indices_n5, 4);
                __m256 neg610 = _mm256_i32gather_ps(Xbase + K * 10, indices_n6, 4);
                __m256 neg710 = _mm256_i32gather_ps(Xbase + K * 10, indices_n7, 4);
                __m256 neg011 = _mm256_i32gather_ps(Xbase + K * 11, indices_n0, 4);
                __m256 neg111 = _mm256_i32gather_ps(Xbase + K * 11, indices_n1, 4);
                __m256 neg211 = _mm256_i32gather_ps(Xbase + K * 11, indices_n2, 4);
                __m256 neg311 = _mm256_i32gather_ps(Xbase + K * 11, indices_n3, 4);
                __m256 neg411 = _mm256_i32gather_ps(Xbase + K * 11, indices_n4, 4);
                __m256 neg511 = _mm256_i32gather_ps(Xbase + K * 11, indices_n5, 4);
                __m256 neg611 = _mm256_i32gather_ps(Xbase + K * 11, indices_n6, 4);
                __m256 neg711 = _mm256_i32gather_ps(Xbase + K * 11, indices_n7, 4);
                __m256 neg012 = _mm256_i32gather_ps(Xbase + K * 12, indices_n0, 4);
                __m256 neg112 = _mm256_i32gather_ps(Xbase + K * 12, indices_n1, 4);
                __m256 neg212 = _mm256_i32gather_ps(Xbase + K * 12, indices_n2, 4);
                __m256 neg312 = _mm256_i32gather_ps(Xbase + K * 12, indices_n3, 4);
                __m256 neg412 = _mm256_i32gather_ps(Xbase + K * 12, indices_n4, 4);
                __m256 neg512 = _mm256_i32gather_ps(Xbase + K * 12, indices_n5, 4);
                __m256 neg612 = _mm256_i32gather_ps(Xbase + K * 12, indices_n6, 4);
                __m256 neg712 = _mm256_i32gather_ps(Xbase + K * 12, indices_n7, 4);
                __m256 neg013 = _mm256_i32gather_ps(Xbase + K * 13, indices_n0, 4);
                __m256 neg113 = _mm256_i32gather_ps(Xbase + K * 13, indices_n1, 4);
                __m256 neg213 = _mm256_i32gather_ps(Xbase + K * 13, indices_n2, 4);
                __m256 neg313 = _mm256_i32gather_ps(Xbase + K * 13, indices_n3, 4);
                __m256 neg413 = _mm256_i32gather_ps(Xbase + K * 13, indices_n4, 4);
                __m256 neg513 = _mm256_i32gather_ps(Xbase + K * 13, indices_n5, 4);
                __m256 neg613 = _mm256_i32gather_ps(Xbase + K * 13, indices_n6, 4);
                __m256 neg713 = _mm256_i32gather_ps(Xbase + K * 13, indices_n7, 4);
                __m256 neg014 = _mm256_i32gather_ps(Xbase + K * 14, indices_n0, 4);
                __m256 neg114 = _mm256_i32gather_ps(Xbase + K * 14, indices_n1, 4);
                __m256 neg214 = _mm256_i32gather_ps(Xbase + K * 14, indices_n2, 4);
                __m256 neg314 = _mm256_i32gather_ps(Xbase + K * 14, indices_n3, 4);
                __m256 neg414 = _mm256_i32gather_ps(Xbase + K * 14, indices_n4, 4);
                __m256 neg514 = _mm256_i32gather_ps(Xbase + K * 14, indices_n5, 4);
                __m256 neg614 = _mm256_i32gather_ps(Xbase + K * 14, indices_n6, 4);
                __m256 neg714 = _mm256_i32gather_ps(Xbase + K * 14, indices_n7, 4);
                __m256 neg015 = _mm256_i32gather_ps(Xbase + K * 15, indices_n0, 4);
                __m256 neg115 = _mm256_i32gather_ps(Xbase + K * 15, indices_n1, 4);
                __m256 neg215 = _mm256_i32gather_ps(Xbase + K * 15, indices_n2, 4);
                __m256 neg315 = _mm256_i32gather_ps(Xbase + K * 15, indices_n3, 4);
                __m256 neg415 = _mm256_i32gather_ps(Xbase + K * 15, indices_n4, 4);
                __m256 neg515 = _mm256_i32gather_ps(Xbase + K * 15, indices_n5, 4);
                __m256 neg615 = _mm256_i32gather_ps(Xbase + K * 15, indices_n6, 4);
                __m256 neg715 = _mm256_i32gather_ps(Xbase + K * 15, indices_n7, 4);
                res00 = _mm256_add_ps(res00, _mm256_sub_ps(pos00, neg00));
                res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
                res20 = _mm256_add_ps(res20, _mm256_sub_ps(pos20, neg20));
                res30 = _mm256_add_ps(res30, _mm256_sub_ps(pos30, neg30));
                res40 = _mm256_add_ps(res40, _mm256_sub_ps(pos40, neg40));
                res50 = _mm256_add_ps(res50, _mm256_sub_ps(pos50, neg50));
                res60 = _mm256_add_ps(res60, _mm256_sub_ps(pos60, neg60));
                res70 = _mm256_add_ps(res70, _mm256_sub_ps(pos70, neg70));
                res01 = _mm256_add_ps(res01, _mm256_sub_ps(pos01, neg01));
                res11 = _mm256_add_ps(res11, _mm256_sub_ps(pos11, neg11));
                res21 = _mm256_add_ps(res21, _mm256_sub_ps(pos21, neg21));
                res31 = _mm256_add_ps(res31, _mm256_sub_ps(pos31, neg31));
                res41 = _mm256_add_ps(res41, _mm256_sub_ps(pos41, neg41));
                res51 = _mm256_add_ps(res51, _mm256_sub_ps(pos51, neg51));
                res61 = _mm256_add_ps(res61, _mm256_sub_ps(pos61, neg61));
                res71 = _mm256_add_ps(res71, _mm256_sub_ps(pos71, neg71));
                res02 = _mm256_add_ps(res02, _mm256_sub_ps(pos02, neg02));
                res12 = _mm256_add_ps(res12, _mm256_sub_ps(pos12, neg12));
                res22 = _mm256_add_ps(res22, _mm256_sub_ps(pos22, neg22));
                res32 = _mm256_add_ps(res32, _mm256_sub_ps(pos32, neg32));
                res42 = _mm256_add_ps(res42, _mm256_sub_ps(pos42, neg42));
                res52 = _mm256_add_ps(res52, _mm256_sub_ps(pos52, neg52));
                res62 = _mm256_add_ps(res62, _mm256_sub_ps(pos62, neg62));
                res72 = _mm256_add_ps(res72, _mm256_sub_ps(pos72, neg72));
                res03 = _mm256_add_ps(res03, _mm256_sub_ps(pos03, neg03));
                res13 = _mm256_add_ps(res13, _mm256_sub_ps(pos13, neg13));
                res23 = _mm256_add_ps(res23, _mm256_sub_ps(pos23, neg23));
                res33 = _mm256_add_ps(res33, _mm256_sub_ps(pos33, neg33));
                res43 = _mm256_add_ps(res43, _mm256_sub_ps(pos43, neg43));
                res53 = _mm256_add_ps(res53, _mm256_sub_ps(pos53, neg53));
                res63 = _mm256_add_ps(res63, _mm256_sub_ps(pos63, neg63));
                res73 = _mm256_add_ps(res73, _mm256_sub_ps(pos73, neg73));
                res04 = _mm256_add_ps(res04, _mm256_sub_ps(pos04, neg04));
                res14 = _mm256_add_ps(res14, _mm256_sub_ps(pos14, neg14));
                res24 = _mm256_add_ps(res24, _mm256_sub_ps(pos24, neg24));
                res34 = _mm256_add_ps(res34, _mm256_sub_ps(pos34, neg34));
                res44 = _mm256_add_ps(res44, _mm256_sub_ps(pos44, neg44));
                res54 = _mm256_add_ps(res54, _mm256_sub_ps(pos54, neg54));
                res64 = _mm256_add_ps(res64, _mm256_sub_ps(pos64, neg64));
                res74 = _mm256_add_ps(res74, _mm256_sub_ps(pos74, neg74));
                res05 = _mm256_add_ps(res05, _mm256_sub_ps(pos05, neg05));
                res15 = _mm256_add_ps(res15, _mm256_sub_ps(pos15, neg15));
                res25 = _mm256_add_ps(res25, _mm256_sub_ps(pos25, neg25));
                res35 = _mm256_add_ps(res35, _mm256_sub_ps(pos35, neg35));
                res45 = _mm256_add_ps(res45, _mm256_sub_ps(pos45, neg45));
                res55 = _mm256_add_ps(res55, _mm256_sub_ps(pos55, neg55));
                res65 = _mm256_add_ps(res65, _mm256_sub_ps(pos65, neg65));
                res75 = _mm256_add_ps(res75, _mm256_sub_ps(pos75, neg75));
                res06 = _mm256_add_ps(res06, _mm256_sub_ps(pos06, neg06));
                res16 = _mm256_add_ps(res16, _mm256_sub_ps(pos16, neg16));
                res26 = _mm256_add_ps(res26, _mm256_sub_ps(pos26, neg26));
                res36 = _mm256_add_ps(res36, _mm256_sub_ps(pos36, neg36));
                res46 = _mm256_add_ps(res46, _mm256_sub_ps(pos46, neg46));
                res56 = _mm256_add_ps(res56, _mm256_sub_ps(pos56, neg56));
                res66 = _mm256_add_ps(res66, _mm256_sub_ps(pos66, neg66));
                res76 = _mm256_add_ps(res76, _mm256_sub_ps(pos76, neg76));
                res07 = _mm256_add_ps(res07, _mm256_sub_ps(pos07, neg07));
                res17 = _mm256_add_ps(res17, _mm256_sub_ps(pos17, neg17));
                res27 = _mm256_add_ps(res27, _mm256_sub_ps(pos27, neg27));
                res37 = _mm256_add_ps(res37, _mm256_sub_ps(pos37, neg37));
                res47 = _mm256_add_ps(res47, _mm256_sub_ps(pos47, neg47));
                res57 = _mm256_add_ps(res57, _mm256_sub_ps(pos57, neg57));
                res67 = _mm256_add_ps(res67, _mm256_sub_ps(pos67, neg67));
                res77 = _mm256_add_ps(res77, _mm256_sub_ps(pos77, neg77));
                res08 = _mm256_add_ps(res08, _mm256_sub_ps(pos08, neg08));
                res18 = _mm256_add_ps(res18, _mm256_sub_ps(pos18, neg18));
                res28 = _mm256_add_ps(res28, _mm256_sub_ps(pos28, neg28));
                res38 = _mm256_add_ps(res38, _mm256_sub_ps(pos38, neg38));
                res48 = _mm256_add_ps(res48, _mm256_sub_ps(pos48, neg48));
                res58 = _mm256_add_ps(res58, _mm256_sub_ps(pos58, neg58));
                res68 = _mm256_add_ps(res68, _mm256_sub_ps(pos68, neg68));
                res78 = _mm256_add_ps(res78, _mm256_sub_ps(pos78, neg78));
                res09 = _mm256_add_ps(res09, _mm256_sub_ps(pos09, neg09));
                res19 = _mm256_add_ps(res19, _mm256_sub_ps(pos19, neg19));
                res29 = _mm256_add_ps(res29, _mm256_sub_ps(pos29, neg29));
                res39 = _mm256_add_ps(res39, _mm256_sub_ps(pos39, neg39));
                res49 = _mm256_add_ps(res49, _mm256_sub_ps(pos49, neg49));
                res59 = _mm256_add_ps(res59, _mm256_sub_ps(pos59, neg59));
                res69 = _mm256_add_ps(res69, _mm256_sub_ps(pos69, neg69));
                res79 = _mm256_add_ps(res79, _mm256_sub_ps(pos79, neg79));
                res010 = _mm256_add_ps(res010, _mm256_sub_ps(pos010, neg010));
                res110 = _mm256_add_ps(res110, _mm256_sub_ps(pos110, neg110));
                res210 = _mm256_add_ps(res210, _mm256_sub_ps(pos210, neg210));
                res310 = _mm256_add_ps(res310, _mm256_sub_ps(pos310, neg310));
                res410 = _mm256_add_ps(res410, _mm256_sub_ps(pos410, neg410));
                res510 = _mm256_add_ps(res510, _mm256_sub_ps(pos510, neg510));
                res610 = _mm256_add_ps(res610, _mm256_sub_ps(pos610, neg610));
                res710 = _mm256_add_ps(res710, _mm256_sub_ps(pos710, neg710));
                res011 = _mm256_add_ps(res011, _mm256_sub_ps(pos011, neg011));
                res111 = _mm256_add_ps(res111, _mm256_sub_ps(pos111, neg111));
                res211 = _mm256_add_ps(res211, _mm256_sub_ps(pos211, neg211));
                res311 = _mm256_add_ps(res311, _mm256_sub_ps(pos311, neg311));
                res411 = _mm256_add_ps(res411, _mm256_sub_ps(pos411, neg411));
                res511 = _mm256_add_ps(res511, _mm256_sub_ps(pos511, neg511));
                res611 = _mm256_add_ps(res611, _mm256_sub_ps(pos611, neg611));
                res711 = _mm256_add_ps(res711, _mm256_sub_ps(pos711, neg711));
                res012 = _mm256_add_ps(res012, _mm256_sub_ps(pos012, neg012));
                res112 = _mm256_add_ps(res112, _mm256_sub_ps(pos112, neg112));
                res212 = _mm256_add_ps(res212, _mm256_sub_ps(pos212, neg212));
                res312 = _mm256_add_ps(res312, _mm256_sub_ps(pos312, neg312));
                res412 = _mm256_add_ps(res412, _mm256_sub_ps(pos412, neg412));
                res512 = _mm256_add_ps(res512, _mm256_sub_ps(pos512, neg512));
                res612 = _mm256_add_ps(res612, _mm256_sub_ps(pos612, neg612));
                res712 = _mm256_add_ps(res712, _mm256_sub_ps(pos712, neg712));
                res013 = _mm256_add_ps(res013, _mm256_sub_ps(pos013, neg013));
                res113 = _mm256_add_ps(res113, _mm256_sub_ps(pos113, neg113));
                res213 = _mm256_add_ps(res213, _mm256_sub_ps(pos213, neg213));
                res313 = _mm256_add_ps(res313, _mm256_sub_ps(pos313, neg313));
                res413 = _mm256_add_ps(res413, _mm256_sub_ps(pos413, neg413));
                res513 = _mm256_add_ps(res513, _mm256_sub_ps(pos513, neg513));
                res613 = _mm256_add_ps(res613, _mm256_sub_ps(pos613, neg613));
                res713 = _mm256_add_ps(res713, _mm256_sub_ps(pos713, neg713));
                res014 = _mm256_add_ps(res014, _mm256_sub_ps(pos014, neg014));
                res114 = _mm256_add_ps(res114, _mm256_sub_ps(pos114, neg114));
                res214 = _mm256_add_ps(res214, _mm256_sub_ps(pos214, neg214));
                res314 = _mm256_add_ps(res314, _mm256_sub_ps(pos314, neg314));
                res414 = _mm256_add_ps(res414, _mm256_sub_ps(pos414, neg414));
                res514 = _mm256_add_ps(res514, _mm256_sub_ps(pos514, neg514));
                res614 = _mm256_add_ps(res614, _mm256_sub_ps(pos614, neg614));
                res714 = _mm256_add_ps(res714, _mm256_sub_ps(pos714, neg714));
                res015 = _mm256_add_ps(res015, _mm256_sub_ps(pos015, neg015));
                res115 = _mm256_add_ps(res115, _mm256_sub_ps(pos115, neg115));
                res215 = _mm256_add_ps(res215, _mm256_sub_ps(pos215, neg215));
                res315 = _mm256_add_ps(res315, _mm256_sub_ps(pos315, neg315));
                res415 = _mm256_add_ps(res415, _mm256_sub_ps(pos415, neg415));
                res515 = _mm256_add_ps(res515, _mm256_sub_ps(pos515, neg515));
                res615 = _mm256_add_ps(res615, _mm256_sub_ps(pos615, neg615));
                res715 = _mm256_add_ps(res715, _mm256_sub_ps(pos715, neg715));
            }
            float* Ybase = result + i * N_COL + j * 64;
            _mm256_store_ps(Ybase + N_COL * 0 + 0, res00);
            _mm256_store_ps(Ybase + N_COL * 0 + 8, res10);
            _mm256_store_ps(Ybase + N_COL * 0 + 16, res20);
            _mm256_store_ps(Ybase + N_COL * 0 + 24, res30);
            _mm256_store_ps(Ybase + N_COL * 0 + 32, res40);
            _mm256_store_ps(Ybase + N_COL * 0 + 40, res50);
            _mm256_store_ps(Ybase + N_COL * 0 + 48, res60);
            _mm256_store_ps(Ybase + N_COL * 0 + 56, res70);
            _mm256_store_ps(Ybase + N_COL * 1 + 0, res01);
            _mm256_store_ps(Ybase + N_COL * 1 + 8, res11);
            _mm256_store_ps(Ybase + N_COL * 1 + 16, res21);
            _mm256_store_ps(Ybase + N_COL * 1 + 24, res31);
            _mm256_store_ps(Ybase + N_COL * 1 + 32, res41);
            _mm256_store_ps(Ybase + N_COL * 1 + 40, res51);
            _mm256_store_ps(Ybase + N_COL * 1 + 48, res61);
            _mm256_store_ps(Ybase + N_COL * 1 + 56, res71);
            _mm256_store_ps(Ybase + N_COL * 2 + 0, res02);
            _mm256_store_ps(Ybase + N_COL * 2 + 8, res12);
            _mm256_store_ps(Ybase + N_COL * 2 + 16, res22);
            _mm256_store_ps(Ybase + N_COL * 2 + 24, res32);
            _mm256_store_ps(Ybase + N_COL * 2 + 32, res42);
            _mm256_store_ps(Ybase + N_COL * 2 + 40, res52);
            _mm256_store_ps(Ybase + N_COL * 2 + 48, res62);
            _mm256_store_ps(Ybase + N_COL * 2 + 56, res72);
            _mm256_store_ps(Ybase + N_COL * 3 + 0, res03);
            _mm256_store_ps(Ybase + N_COL * 3 + 8, res13);
            _mm256_store_ps(Ybase + N_COL * 3 + 16, res23);
            _mm256_store_ps(Ybase + N_COL * 3 + 24, res33);
            _mm256_store_ps(Ybase + N_COL * 3 + 32, res43);
            _mm256_store_ps(Ybase + N_COL * 3 + 40, res53);
            _mm256_store_ps(Ybase + N_COL * 3 + 48, res63);
            _mm256_store_ps(Ybase + N_COL * 3 + 56, res73);
            _mm256_store_ps(Ybase + N_COL * 4 + 0, res04);
            _mm256_store_ps(Ybase + N_COL * 4 + 8, res14);
            _mm256_store_ps(Ybase + N_COL * 4 + 16, res24);
            _mm256_store_ps(Ybase + N_COL * 4 + 24, res34);
            _mm256_store_ps(Ybase + N_COL * 4 + 32, res44);
            _mm256_store_ps(Ybase + N_COL * 4 + 40, res54);
            _mm256_store_ps(Ybase + N_COL * 4 + 48, res64);
            _mm256_store_ps(Ybase + N_COL * 4 + 56, res74);
            _mm256_store_ps(Ybase + N_COL * 5 + 0, res05);
            _mm256_store_ps(Ybase + N_COL * 5 + 8, res15);
            _mm256_store_ps(Ybase + N_COL * 5 + 16, res25);
            _mm256_store_ps(Ybase + N_COL * 5 + 24, res35);
            _mm256_store_ps(Ybase + N_COL * 5 + 32, res45);
            _mm256_store_ps(Ybase + N_COL * 5 + 40, res55);
            _mm256_store_ps(Ybase + N_COL * 5 + 48, res65);
            _mm256_store_ps(Ybase + N_COL * 5 + 56, res75);
            _mm256_store_ps(Ybase + N_COL * 6 + 0, res06);
            _mm256_store_ps(Ybase + N_COL * 6 + 8, res16);
            _mm256_store_ps(Ybase + N_COL * 6 + 16, res26);
            _mm256_store_ps(Ybase + N_COL * 6 + 24, res36);
            _mm256_store_ps(Ybase + N_COL * 6 + 32, res46);
            _mm256_store_ps(Ybase + N_COL * 6 + 40, res56);
            _mm256_store_ps(Ybase + N_COL * 6 + 48, res66);
            _mm256_store_ps(Ybase + N_COL * 6 + 56, res76);
            _mm256_store_ps(Ybase + N_COL * 7 + 0, res07);
            _mm256_store_ps(Ybase + N_COL * 7 + 8, res17);
            _mm256_store_ps(Ybase + N_COL * 7 + 16, res27);
            _mm256_store_ps(Ybase + N_COL * 7 + 24, res37);
            _mm256_store_ps(Ybase + N_COL * 7 + 32, res47);
            _mm256_store_ps(Ybase + N_COL * 7 + 40, res57);
            _mm256_store_ps(Ybase + N_COL * 7 + 48, res67);
            _mm256_store_ps(Ybase + N_COL * 7 + 56, res77);
            _mm256_store_ps(Ybase + N_COL * 8 + 0, res08);
            _mm256_store_ps(Ybase + N_COL * 8 + 8, res18);
            _mm256_store_ps(Ybase + N_COL * 8 + 16, res28);
            _mm256_store_ps(Ybase + N_COL * 8 + 24, res38);
            _mm256_store_ps(Ybase + N_COL * 8 + 32, res48);
            _mm256_store_ps(Ybase + N_COL * 8 + 40, res58);
            _mm256_store_ps(Ybase + N_COL * 8 + 48, res68);
            _mm256_store_ps(Ybase + N_COL * 8 + 56, res78);
            _mm256_store_ps(Ybase + N_COL * 9 + 0, res09);
            _mm256_store_ps(Ybase + N_COL * 9 + 8, res19);
            _mm256_store_ps(Ybase + N_COL * 9 + 16, res29);
            _mm256_store_ps(Ybase + N_COL * 9 + 24, res39);
            _mm256_store_ps(Ybase + N_COL * 9 + 32, res49);
            _mm256_store_ps(Ybase + N_COL * 9 + 40, res59);
            _mm256_store_ps(Ybase + N_COL * 9 + 48, res69);
            _mm256_store_ps(Ybase + N_COL * 9 + 56, res79);
            _mm256_store_ps(Ybase + N_COL * 10 + 0, res010);
            _mm256_store_ps(Ybase + N_COL * 10 + 8, res110);
            _mm256_store_ps(Ybase + N_COL * 10 + 16, res210);
            _mm256_store_ps(Ybase + N_COL * 10 + 24, res310);
            _mm256_store_ps(Ybase + N_COL * 10 + 32, res410);
            _mm256_store_ps(Ybase + N_COL * 10 + 40, res510);
            _mm256_store_ps(Ybase + N_COL * 10 + 48, res610);
            _mm256_store_ps(Ybase + N_COL * 10 + 56, res710);
            _mm256_store_ps(Ybase + N_COL * 11 + 0, res011);
            _mm256_store_ps(Ybase + N_COL * 11 + 8, res111);
            _mm256_store_ps(Ybase + N_COL * 11 + 16, res211);
            _mm256_store_ps(Ybase + N_COL * 11 + 24, res311);
            _mm256_store_ps(Ybase + N_COL * 11 + 32, res411);
            _mm256_store_ps(Ybase + N_COL * 11 + 40, res511);
            _mm256_store_ps(Ybase + N_COL * 11 + 48, res611);
            _mm256_store_ps(Ybase + N_COL * 11 + 56, res711);
            _mm256_store_ps(Ybase + N_COL * 12 + 0, res012);
            _mm256_store_ps(Ybase + N_COL * 12 + 8, res112);
            _mm256_store_ps(Ybase + N_COL * 12 + 16, res212);
            _mm256_store_ps(Ybase + N_COL * 12 + 24, res312);
            _mm256_store_ps(Ybase + N_COL * 12 + 32, res412);
            _mm256_store_ps(Ybase + N_COL * 12 + 40, res512);
            _mm256_store_ps(Ybase + N_COL * 12 + 48, res612);
            _mm256_store_ps(Ybase + N_COL * 12 + 56, res712);
            _mm256_store_ps(Ybase + N_COL * 13 + 0, res013);
            _mm256_store_ps(Ybase + N_COL * 13 + 8, res113);
            _mm256_store_ps(Ybase + N_COL * 13 + 16, res213);
            _mm256_store_ps(Ybase + N_COL * 13 + 24, res313);
            _mm256_store_ps(Ybase + N_COL * 13 + 32, res413);
            _mm256_store_ps(Ybase + N_COL * 13 + 40, res513);
            _mm256_store_ps(Ybase + N_COL * 13 + 48, res613);
            _mm256_store_ps(Ybase + N_COL * 13 + 56, res713);
            _mm256_store_ps(Ybase + N_COL * 14 + 0, res014);
            _mm256_store_ps(Ybase + N_COL * 14 + 8, res114);
            _mm256_store_ps(Ybase + N_COL * 14 + 16, res214);
            _mm256_store_ps(Ybase + N_COL * 14 + 24, res314);
            _mm256_store_ps(Ybase + N_COL * 14 + 32, res414);
            _mm256_store_ps(Ybase + N_COL * 14 + 40, res514);
            _mm256_store_ps(Ybase + N_COL * 14 + 48, res614);
            _mm256_store_ps(Ybase + N_COL * 14 + 56, res714);
            _mm256_store_ps(Ybase + N_COL * 15 + 0, res015);
            _mm256_store_ps(Ybase + N_COL * 15 + 8, res115);
            _mm256_store_ps(Ybase + N_COL * 15 + 16, res215);
            _mm256_store_ps(Ybase + N_COL * 15 + 24, res315);
            _mm256_store_ps(Ybase + N_COL * 15 + 32, res415);
            _mm256_store_ps(Ybase + N_COL * 15 + 40, res515);
            _mm256_store_ps(Ybase + N_COL * 15 + 48, res615);
            _mm256_store_ps(Ybase + N_COL * 15 + 56, res715);
            for (int g = 0; g < 64; g++) {
                for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++) {
                    Ybase[N_COL * 0 + g] += Xbase[K * 0 + row_index[k]];
                    Ybase[N_COL * 1 + g] += Xbase[K * 1 + row_index[k]];
                    Ybase[N_COL * 2 + g] += Xbase[K * 2 + row_index[k]];
                    Ybase[N_COL * 3 + g] += Xbase[K * 3 + row_index[k]];
                    Ybase[N_COL * 4 + g] += Xbase[K * 4 + row_index[k]];
                    Ybase[N_COL * 5 + g] += Xbase[K * 5 + row_index[k]];
                    Ybase[N_COL * 6 + g] += Xbase[K * 6 + row_index[k]];
                    Ybase[N_COL * 7 + g] += Xbase[K * 7 + row_index[k]];
                    Ybase[N_COL * 8 + g] += Xbase[K * 8 + row_index[k]];
                    Ybase[N_COL * 9 + g] += Xbase[K * 9 + row_index[k]];
                    Ybase[N_COL * 10 + g] += Xbase[K * 10 + row_index[k]];
                    Ybase[N_COL * 11 + g] += Xbase[K * 11 + row_index[k]];
                    Ybase[N_COL * 12 + g] += Xbase[K * 12 + row_index[k]];
                    Ybase[N_COL * 13 + g] += Xbase[K * 13 + row_index[k]];
                    Ybase[N_COL * 14 + g] += Xbase[K * 14 + row_index[k]];
                    Ybase[N_COL * 15 + g] += Xbase[K * 15 + row_index[k]];
                }
                for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++) {
                    Ybase[N_COL * 0 + g] -= Xbase[K * 0 + row_index[k]];
                    Ybase[N_COL * 1 + g] -= Xbase[K * 1 + row_index[k]];
                    Ybase[N_COL * 2 + g] -= Xbase[K * 2 + row_index[k]];
                    Ybase[N_COL * 3 + g] -= Xbase[K * 3 + row_index[k]];
                    Ybase[N_COL * 4 + g] -= Xbase[K * 4 + row_index[k]];
                    Ybase[N_COL * 5 + g] -= Xbase[K * 5 + row_index[k]];
                    Ybase[N_COL * 6 + g] -= Xbase[K * 6 + row_index[k]];
                    Ybase[N_COL * 7 + g] -= Xbase[K * 7 + row_index[k]];
                    Ybase[N_COL * 8 + g] -= Xbase[K * 8 + row_index[k]];
                    Ybase[N_COL * 9 + g] -= Xbase[K * 9 + row_index[k]];
                    Ybase[N_COL * 10 + g] -= Xbase[K * 10 + row_index[k]];
                    Ybase[N_COL * 11 + g] -= Xbase[K * 11 + row_index[k]];
                    Ybase[N_COL * 12 + g] -= Xbase[K * 12 + row_index[k]];
                    Ybase[N_COL * 13 + g] -= Xbase[K * 13 + row_index[k]];
                    Ybase[N_COL * 14 + g] -= Xbase[K * 14 + row_index[k]];
                    Ybase[N_COL * 15 + g] -= Xbase[K * 15 + row_index[k]];
                }
            }
        }
    }
}


void GEMV_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_G8_AVX2_OpenMP(const float* X, const int32_t* metadata, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 8; j++) {
        const int* groupData  = &metadata[j * 18];
        __m256 res0 = _mm256_setzero_ps();
        for (int k = groupData[0]; k < groupData[1]; k += 16) {
            __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
            __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
            __m256 pos0 = _mm256_i32gather_ps(X, indices_p0, 4);
            __m256 neg0 = _mm256_i32gather_ps(X, indices_n0, 4);
            res0 = _mm256_add_ps(res0, _mm256_sub_ps(pos0, neg0));
        }
        float* Ybase = result + j * 8;
        _mm256_store_ps(Ybase + 0, res0);
        for (int k = groupData[1]; k < groupData[2]; k++)
            Ybase[0] += X[row_index[k]];
        for (int k = groupData[2]; k < groupData[3]; k++)
            Ybase[0] -= X[row_index[k]];
        for (int k = groupData[3]; k < groupData[4]; k++)
            Ybase[1] += X[row_index[k]];
        for (int k = groupData[4]; k < groupData[5]; k++)
            Ybase[1] -= X[row_index[k]];
        for (int k = groupData[5]; k < groupData[6]; k++)
            Ybase[2] += X[row_index[k]];
        for (int k = groupData[6]; k < groupData[7]; k++)
            Ybase[2] -= X[row_index[k]];
        for (int k = groupData[7]; k < groupData[8]; k++)
            Ybase[3] += X[row_index[k]];
        for (int k = groupData[8]; k < groupData[9]; k++)
            Ybase[3] -= X[row_index[k]];
        for (int k = groupData[9]; k < groupData[10]; k++)
            Ybase[4] += X[row_index[k]];
        for (int k = groupData[10]; k < groupData[11]; k++)
            Ybase[4] -= X[row_index[k]];
        for (int k = groupData[11]; k < groupData[12]; k++)
            Ybase[5] += X[row_index[k]];
        for (int k = groupData[12]; k < groupData[13]; k++)
            Ybase[5] -= X[row_index[k]];
        for (int k = groupData[13]; k < groupData[14]; k++)
            Ybase[6] += X[row_index[k]];
        for (int k = groupData[14]; k < groupData[15]; k++)
            Ybase[6] -= X[row_index[k]];
        for (int k = groupData[15]; k < groupData[16]; k++)
            Ybase[7] += X[row_index[k]];
        for (int k = groupData[16]; k < groupData[17]; k++)
            Ybase[7] -= X[row_index[k]];
    }
}

void GEMV_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_G16_AVX2_OpenMP(const float* X, const int32_t* metadata, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 16; j++) {
        const int* groupData  = &metadata[j * 34];
        __m256 res0 = _mm256_setzero_ps();
        __m256 res1 = _mm256_setzero_ps();
        for (int k = groupData[0]; k < groupData[1]; k += 32) {
            __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
            __m256i indices_p1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
            __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 16));
            __m256i indices_n1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 24));
            __m256 pos0 = _mm256_i32gather_ps(X, indices_p0, 4);
            __m256 pos1 = _mm256_i32gather_ps(X, indices_p1, 4);
            __m256 neg0 = _mm256_i32gather_ps(X, indices_n0, 4);
            __m256 neg1 = _mm256_i32gather_ps(X, indices_n1, 4);
            res0 = _mm256_add_ps(res0, _mm256_sub_ps(pos0, neg0));
            res1 = _mm256_add_ps(res1, _mm256_sub_ps(pos1, neg1));
        }
        float* Ybase = result + j * 16;
        _mm256_store_ps(Ybase + 0, res0);
        _mm256_store_ps(Ybase + 8, res1);
#pragma omp simd
        for (int g = 0; g < 16; g++) {
#pragma omp simd
            for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++)
                Ybase[g] += X[row_index[k]];
#pragma omp simd
            for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++)
                Ybase[g] -= X[row_index[k]];
        }
    }
}

void GEMV_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_G32_AVX2_OpenMP(const float* X, const int32_t* metadata, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 32; j++) {
        const int* groupData  = &metadata[j * 66];
        __m256 res0 = _mm256_setzero_ps();
        __m256 res1 = _mm256_setzero_ps();
        __m256 res2 = _mm256_setzero_ps();
        __m256 res3 = _mm256_setzero_ps();
        for (int k = groupData[0]; k < groupData[1]; k += 64) {
            __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
            __m256i indices_p1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
            __m256i indices_p2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 16));
            __m256i indices_p3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 24));
            __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 32));
            __m256i indices_n1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 40));
            __m256i indices_n2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 48));
            __m256i indices_n3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 56));
            __m256 pos0 = _mm256_i32gather_ps(X, indices_p0, 4);
            __m256 pos1 = _mm256_i32gather_ps(X, indices_p1, 4);
            __m256 pos2 = _mm256_i32gather_ps(X, indices_p2, 4);
            __m256 pos3 = _mm256_i32gather_ps(X, indices_p3, 4);
            __m256 neg0 = _mm256_i32gather_ps(X, indices_n0, 4);
            __m256 neg1 = _mm256_i32gather_ps(X, indices_n1, 4);
            __m256 neg2 = _mm256_i32gather_ps(X, indices_n2, 4);
            __m256 neg3 = _mm256_i32gather_ps(X, indices_n3, 4);
            res0 = _mm256_add_ps(res0, _mm256_sub_ps(pos0, neg0));
            res1 = _mm256_add_ps(res1, _mm256_sub_ps(pos1, neg1));
            res2 = _mm256_add_ps(res2, _mm256_sub_ps(pos2, neg2));
            res3 = _mm256_add_ps(res3, _mm256_sub_ps(pos3, neg3));
        }
        float* Ybase = result + j * 32;
        _mm256_store_ps(Ybase + 0, res0);
        _mm256_store_ps(Ybase + 8, res1);
        _mm256_store_ps(Ybase + 16, res2);
        _mm256_store_ps(Ybase + 24, res3);
#pragma omp simd
        for (int g = 0; g < 32; g++) {
#pragma omp simd
            for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++)
                Ybase[g] += X[row_index[k]];
#pragma omp simd
            for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++)
                Ybase[g] -= X[row_index[k]];
        }
    }
}

void GEMV_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_G64_AVX2_OpenMP(const float* X, const int32_t* metadata, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 64; j++) {
        const int* groupData  = &metadata[j * 130];
        __m256 res0 = _mm256_setzero_ps();
        __m256 res1 = _mm256_setzero_ps();
        __m256 res2 = _mm256_setzero_ps();
        __m256 res3 = _mm256_setzero_ps();
        __m256 res4 = _mm256_setzero_ps();
        __m256 res5 = _mm256_setzero_ps();
        __m256 res6 = _mm256_setzero_ps();
        __m256 res7 = _mm256_setzero_ps();
        for (int k = groupData[0]; k < groupData[1]; k += 128) {
            __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
            __m256i indices_p1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
            __m256i indices_p2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 16));
            __m256i indices_p3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 24));
            __m256i indices_p4 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 32));
            __m256i indices_p5 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 40));
            __m256i indices_p6 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 48));
            __m256i indices_p7 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 56));
            __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 64));
            __m256i indices_n1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 72));
            __m256i indices_n2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 80));
            __m256i indices_n3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 88));
            __m256i indices_n4 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 96));
            __m256i indices_n5 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 104));
            __m256i indices_n6 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 112));
            __m256i indices_n7 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 120));
            __m256 pos0 = _mm256_i32gather_ps(X, indices_p0, 4);
            __m256 pos1 = _mm256_i32gather_ps(X, indices_p1, 4);
            __m256 pos2 = _mm256_i32gather_ps(X, indices_p2, 4);
            __m256 pos3 = _mm256_i32gather_ps(X, indices_p3, 4);
            __m256 pos4 = _mm256_i32gather_ps(X, indices_p4, 4);
            __m256 pos5 = _mm256_i32gather_ps(X, indices_p5, 4);
            __m256 pos6 = _mm256_i32gather_ps(X, indices_p6, 4);
            __m256 pos7 = _mm256_i32gather_ps(X, indices_p7, 4);
            __m256 neg0 = _mm256_i32gather_ps(X, indices_n0, 4);
            __m256 neg1 = _mm256_i32gather_ps(X, indices_n1, 4);
            __m256 neg2 = _mm256_i32gather_ps(X, indices_n2, 4);
            __m256 neg3 = _mm256_i32gather_ps(X, indices_n3, 4);
            __m256 neg4 = _mm256_i32gather_ps(X, indices_n4, 4);
            __m256 neg5 = _mm256_i32gather_ps(X, indices_n5, 4);
            __m256 neg6 = _mm256_i32gather_ps(X, indices_n6, 4);
            __m256 neg7 = _mm256_i32gather_ps(X, indices_n7, 4);
            res0 = _mm256_add_ps(res0, _mm256_sub_ps(pos0, neg0));
            res1 = _mm256_add_ps(res1, _mm256_sub_ps(pos1, neg1));
            res2 = _mm256_add_ps(res2, _mm256_sub_ps(pos2, neg2));
            res3 = _mm256_add_ps(res3, _mm256_sub_ps(pos3, neg3));
            res4 = _mm256_add_ps(res4, _mm256_sub_ps(pos4, neg4));
            res5 = _mm256_add_ps(res5, _mm256_sub_ps(pos5, neg5));
            res6 = _mm256_add_ps(res6, _mm256_sub_ps(pos6, neg6));
            res7 = _mm256_add_ps(res7, _mm256_sub_ps(pos7, neg7));
        }
        float* Ybase = result + j * 64;
        _mm256_store_ps(Ybase + 0, res0);
        _mm256_store_ps(Ybase + 8, res1);
        _mm256_store_ps(Ybase + 16, res2);
        _mm256_store_ps(Ybase + 24, res3);
        _mm256_store_ps(Ybase + 32, res4);
        _mm256_store_ps(Ybase + 40, res5);
        _mm256_store_ps(Ybase + 48, res6);
        _mm256_store_ps(Ybase + 56, res7);
#pragma omp simd
        for (int g = 0; g < 64; g++) {
#pragma omp simd
            for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++)
                Ybase[g] += X[row_index[k]];
#pragma omp simd
            for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++)
                Ybase[g] -= X[row_index[k]];
        }
    }
}

void GEMV_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_G16_CS8_AVX2_OpenMP(const float* X, const int32_t* metadata, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 16; j++) {
        const int* groupData  = &metadata[j * 34];
        __m256 res0 = _mm256_setzero_ps();
        __m256 res1 = _mm256_setzero_ps();
        for (int k = groupData[0]; k < groupData[1]; k += 32) {
            __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
            __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
            __m256i indices_p1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 16));
            __m256i indices_n1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 24));
            __m256 pos0 = _mm256_i32gather_ps(X, indices_p0, 4);
            __m256 neg0 = _mm256_i32gather_ps(X, indices_n0, 4);
            __m256 pos1 = _mm256_i32gather_ps(X, indices_p1, 4);
            __m256 neg1 = _mm256_i32gather_ps(X, indices_n1, 4);
            res0 = _mm256_add_ps(res0, _mm256_sub_ps(pos0, neg0));
            res1 = _mm256_add_ps(res1, _mm256_sub_ps(pos1, neg1));
        }
        float* Ybase = result + j * 16;
        _mm256_store_ps(Ybase + 0, res0);
        _mm256_store_ps(Ybase + 8, res1);
#pragma omp simd
        for (int g = 0; g < 16; g++) {
#pragma omp simd
            for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++)
                Ybase[g] += X[row_index[k]];
#pragma omp simd
            for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++)
                Ybase[g] -= X[row_index[k]];
        }
    }
}

void GEMV_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_G32_CS8_AVX2_OpenMP(const float* X, const int32_t* metadata, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 32; j++) {
        const int* groupData  = &metadata[j * 66];
        __m256 res0 = _mm256_setzero_ps();
        __m256 res1 = _mm256_setzero_ps();
        __m256 res2 = _mm256_setzero_ps();
        __m256 res3 = _mm256_setzero_ps();
        for (int k = groupData[0]; k < groupData[1]; k += 64) {
            __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
            __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
            __m256i indices_p1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 16));
            __m256i indices_n1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 24));
            __m256i indices_p2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 32));
            __m256i indices_n2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 40));
            __m256i indices_p3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 48));
            __m256i indices_n3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 56));
            __m256 pos0 = _mm256_i32gather_ps(X, indices_p0, 4);
            __m256 neg0 = _mm256_i32gather_ps(X, indices_n0, 4);
            __m256 pos1 = _mm256_i32gather_ps(X, indices_p1, 4);
            __m256 neg1 = _mm256_i32gather_ps(X, indices_n1, 4);
            __m256 pos2 = _mm256_i32gather_ps(X, indices_p2, 4);
            __m256 neg2 = _mm256_i32gather_ps(X, indices_n2, 4);
            __m256 pos3 = _mm256_i32gather_ps(X, indices_p3, 4);
            __m256 neg3 = _mm256_i32gather_ps(X, indices_n3, 4);
            res0 = _mm256_add_ps(res0, _mm256_sub_ps(pos0, neg0));
            res1 = _mm256_add_ps(res1, _mm256_sub_ps(pos1, neg1));
            res2 = _mm256_add_ps(res2, _mm256_sub_ps(pos2, neg2));
            res3 = _mm256_add_ps(res3, _mm256_sub_ps(pos3, neg3));
        }
        float* Ybase = result + j * 32;
        _mm256_store_ps(Ybase + 0, res0);
        _mm256_store_ps(Ybase + 8, res1);
        _mm256_store_ps(Ybase + 16, res2);
        _mm256_store_ps(Ybase + 24, res3);
#pragma omp simd
        for (int g = 0; g < 32; g++) {
#pragma omp simd
            for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++)
                Ybase[g] += X[row_index[k]];
#pragma omp simd
            for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++)
                Ybase[g] -= X[row_index[k]];
        }
    }
}

void GEMV_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_G64_CS8_AVX2_OpenMP(const float* X, const int32_t* metadata, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 64; j++) {
        const int* groupData  = &metadata[j * 130];
        __m256 res0 = _mm256_setzero_ps();
        __m256 res1 = _mm256_setzero_ps();
        __m256 res2 = _mm256_setzero_ps();
        __m256 res3 = _mm256_setzero_ps();
        __m256 res4 = _mm256_setzero_ps();
        __m256 res5 = _mm256_setzero_ps();
        __m256 res6 = _mm256_setzero_ps();
        __m256 res7 = _mm256_setzero_ps();
        for (int k = groupData[0]; k < groupData[1]; k += 128) {
            __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
            __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
            __m256i indices_p1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 16));
            __m256i indices_n1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 24));
            __m256i indices_p2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 32));
            __m256i indices_n2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 40));
            __m256i indices_p3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 48));
            __m256i indices_n3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 56));
            __m256i indices_p4 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 64));
            __m256i indices_n4 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 72));
            __m256i indices_p5 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 80));
            __m256i indices_n5 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 88));
            __m256i indices_p6 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 96));
            __m256i indices_n6 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 104));
            __m256i indices_p7 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 112));
            __m256i indices_n7 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 120));
            __m256 pos0 = _mm256_i32gather_ps(X, indices_p0, 4);
            __m256 neg0 = _mm256_i32gather_ps(X, indices_n0, 4);
            __m256 pos1 = _mm256_i32gather_ps(X, indices_p1, 4);
            __m256 neg1 = _mm256_i32gather_ps(X, indices_n1, 4);
            __m256 pos2 = _mm256_i32gather_ps(X, indices_p2, 4);
            __m256 neg2 = _mm256_i32gather_ps(X, indices_n2, 4);
            __m256 pos3 = _mm256_i32gather_ps(X, indices_p3, 4);
            __m256 neg3 = _mm256_i32gather_ps(X, indices_n3, 4);
            __m256 pos4 = _mm256_i32gather_ps(X, indices_p4, 4);
            __m256 neg4 = _mm256_i32gather_ps(X, indices_n4, 4);
            __m256 pos5 = _mm256_i32gather_ps(X, indices_p5, 4);
            __m256 neg5 = _mm256_i32gather_ps(X, indices_n5, 4);
            __m256 pos6 = _mm256_i32gather_ps(X, indices_p6, 4);
            __m256 neg6 = _mm256_i32gather_ps(X, indices_n6, 4);
            __m256 pos7 = _mm256_i32gather_ps(X, indices_p7, 4);
            __m256 neg7 = _mm256_i32gather_ps(X, indices_n7, 4);
            res0 = _mm256_add_ps(res0, _mm256_sub_ps(pos0, neg0));
            res1 = _mm256_add_ps(res1, _mm256_sub_ps(pos1, neg1));
            res2 = _mm256_add_ps(res2, _mm256_sub_ps(pos2, neg2));
            res3 = _mm256_add_ps(res3, _mm256_sub_ps(pos3, neg3));
            res4 = _mm256_add_ps(res4, _mm256_sub_ps(pos4, neg4));
            res5 = _mm256_add_ps(res5, _mm256_sub_ps(pos5, neg5));
            res6 = _mm256_add_ps(res6, _mm256_sub_ps(pos6, neg6));
            res7 = _mm256_add_ps(res7, _mm256_sub_ps(pos7, neg7));
        }
        float* Ybase = result + j * 64;
        _mm256_store_ps(Ybase + 0, res0);
        _mm256_store_ps(Ybase + 8, res1);
        _mm256_store_ps(Ybase + 16, res2);
        _mm256_store_ps(Ybase + 24, res3);
        _mm256_store_ps(Ybase + 32, res4);
        _mm256_store_ps(Ybase + 40, res5);
        _mm256_store_ps(Ybase + 48, res6);
        _mm256_store_ps(Ybase + 56, res7);
#pragma omp simd
        for (int g = 0; g < 64; g++) {
#pragma omp simd
            for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++)
                Ybase[g] += X[row_index[k]];
#pragma omp simd
            for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++)
                Ybase[g] -= X[row_index[k]];
        }
    }
}

void GEMV_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_G64_CS8_SIMD1_AVX2_OpenMP(const float* X, const int32_t* metadata, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 64; j++) {
        const int* groupData  = &metadata[j * 130];
        __m256 res0 = _mm256_setzero_ps();
        __m256 res1 = _mm256_setzero_ps();
        __m256 res2 = _mm256_setzero_ps();
        __m256 res3 = _mm256_setzero_ps();
        __m256 res4 = _mm256_setzero_ps();
        __m256 res5 = _mm256_setzero_ps();
        __m256 res6 = _mm256_setzero_ps();
        __m256 res7 = _mm256_setzero_ps();
        for (int k = groupData[0]; k < groupData[1]; k += 128) {
            __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
            __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
            __m256i indices_p1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 16));
            __m256i indices_n1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 24));
            __m256i indices_p2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 32));
            __m256i indices_n2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 40));
            __m256i indices_p3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 48));
            __m256i indices_n3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 56));
            __m256i indices_p4 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 64));
            __m256i indices_n4 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 72));
            __m256i indices_p5 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 80));
            __m256i indices_n5 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 88));
            __m256i indices_p6 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 96));
            __m256i indices_n6 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 104));
            __m256i indices_p7 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 112));
            __m256i indices_n7 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 120));
            __m256 pos0 = _mm256_i32gather_ps(X, indices_p0, 4);
            __m256 neg0 = _mm256_i32gather_ps(X, indices_n0, 4);
            __m256 pos1 = _mm256_i32gather_ps(X, indices_p1, 4);
            __m256 neg1 = _mm256_i32gather_ps(X, indices_n1, 4);
            __m256 pos2 = _mm256_i32gather_ps(X, indices_p2, 4);
            __m256 neg2 = _mm256_i32gather_ps(X, indices_n2, 4);
            __m256 pos3 = _mm256_i32gather_ps(X, indices_p3, 4);
            __m256 neg3 = _mm256_i32gather_ps(X, indices_n3, 4);
            __m256 pos4 = _mm256_i32gather_ps(X, indices_p4, 4);
            __m256 neg4 = _mm256_i32gather_ps(X, indices_n4, 4);
            __m256 pos5 = _mm256_i32gather_ps(X, indices_p5, 4);
            __m256 neg5 = _mm256_i32gather_ps(X, indices_n5, 4);
            __m256 pos6 = _mm256_i32gather_ps(X, indices_p6, 4);
            __m256 neg6 = _mm256_i32gather_ps(X, indices_n6, 4);
            __m256 pos7 = _mm256_i32gather_ps(X, indices_p7, 4);
            __m256 neg7 = _mm256_i32gather_ps(X, indices_n7, 4);
            res0 = _mm256_add_ps(res0, _mm256_sub_ps(pos0, neg0));
            res1 = _mm256_add_ps(res1, _mm256_sub_ps(pos1, neg1));
            res2 = _mm256_add_ps(res2, _mm256_sub_ps(pos2, neg2));
            res3 = _mm256_add_ps(res3, _mm256_sub_ps(pos3, neg3));
            res4 = _mm256_add_ps(res4, _mm256_sub_ps(pos4, neg4));
            res5 = _mm256_add_ps(res5, _mm256_sub_ps(pos5, neg5));
            res6 = _mm256_add_ps(res6, _mm256_sub_ps(pos6, neg6));
            res7 = _mm256_add_ps(res7, _mm256_sub_ps(pos7, neg7));
        }
        float* Ybase = result + j * 64;
        _mm256_store_ps(Ybase + 0, res0);
        _mm256_store_ps(Ybase + 8, res1);
        _mm256_store_ps(Ybase + 16, res2);
        _mm256_store_ps(Ybase + 24, res3);
        _mm256_store_ps(Ybase + 32, res4);
        _mm256_store_ps(Ybase + 40, res5);
        _mm256_store_ps(Ybase + 48, res6);
        _mm256_store_ps(Ybase + 56, res7);
        #pragma omp simd
        for (int g = 0; g < 64; g++) {
            for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++)
                Ybase[g] += X[row_index[k]];
            for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++)
                Ybase[g] -= X[row_index[k]];
        }
    }
}

void GEMV_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_G64_CS8_SIMD2_AVX2_OpenMP(const float* X, const int32_t* metadata, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 64; j++) {
        const int* groupData  = &metadata[j * 130];
        __m256 res0 = _mm256_setzero_ps();
        __m256 res1 = _mm256_setzero_ps();
        __m256 res2 = _mm256_setzero_ps();
        __m256 res3 = _mm256_setzero_ps();
        __m256 res4 = _mm256_setzero_ps();
        __m256 res5 = _mm256_setzero_ps();
        __m256 res6 = _mm256_setzero_ps();
        __m256 res7 = _mm256_setzero_ps();
        for (int k = groupData[0]; k < groupData[1]; k += 128) {
            __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
            __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
            __m256i indices_p1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 16));
            __m256i indices_n1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 24));
            __m256i indices_p2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 32));
            __m256i indices_n2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 40));
            __m256i indices_p3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 48));
            __m256i indices_n3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 56));
            __m256i indices_p4 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 64));
            __m256i indices_n4 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 72));
            __m256i indices_p5 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 80));
            __m256i indices_n5 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 88));
            __m256i indices_p6 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 96));
            __m256i indices_n6 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 104));
            __m256i indices_p7 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 112));
            __m256i indices_n7 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 120));
            __m256 pos0 = _mm256_i32gather_ps(X, indices_p0, 4);
            __m256 neg0 = _mm256_i32gather_ps(X, indices_n0, 4);
            __m256 pos1 = _mm256_i32gather_ps(X, indices_p1, 4);
            __m256 neg1 = _mm256_i32gather_ps(X, indices_n1, 4);
            __m256 pos2 = _mm256_i32gather_ps(X, indices_p2, 4);
            __m256 neg2 = _mm256_i32gather_ps(X, indices_n2, 4);
            __m256 pos3 = _mm256_i32gather_ps(X, indices_p3, 4);
            __m256 neg3 = _mm256_i32gather_ps(X, indices_n3, 4);
            __m256 pos4 = _mm256_i32gather_ps(X, indices_p4, 4);
            __m256 neg4 = _mm256_i32gather_ps(X, indices_n4, 4);
            __m256 pos5 = _mm256_i32gather_ps(X, indices_p5, 4);
            __m256 neg5 = _mm256_i32gather_ps(X, indices_n5, 4);
            __m256 pos6 = _mm256_i32gather_ps(X, indices_p6, 4);
            __m256 neg6 = _mm256_i32gather_ps(X, indices_n6, 4);
            __m256 pos7 = _mm256_i32gather_ps(X, indices_p7, 4);
            __m256 neg7 = _mm256_i32gather_ps(X, indices_n7, 4);
            res0 = _mm256_add_ps(res0, _mm256_sub_ps(pos0, neg0));
            res1 = _mm256_add_ps(res1, _mm256_sub_ps(pos1, neg1));
            res2 = _mm256_add_ps(res2, _mm256_sub_ps(pos2, neg2));
            res3 = _mm256_add_ps(res3, _mm256_sub_ps(pos3, neg3));
            res4 = _mm256_add_ps(res4, _mm256_sub_ps(pos4, neg4));
            res5 = _mm256_add_ps(res5, _mm256_sub_ps(pos5, neg5));
            res6 = _mm256_add_ps(res6, _mm256_sub_ps(pos6, neg6));
            res7 = _mm256_add_ps(res7, _mm256_sub_ps(pos7, neg7));
        }
        float* Ybase = result + j * 64;
        _mm256_store_ps(Ybase + 0, res0);
        _mm256_store_ps(Ybase + 8, res1);
        _mm256_store_ps(Ybase + 16, res2);
        _mm256_store_ps(Ybase + 24, res3);
        _mm256_store_ps(Ybase + 32, res4);
        _mm256_store_ps(Ybase + 40, res5);
        _mm256_store_ps(Ybase + 48, res6);
        _mm256_store_ps(Ybase + 56, res7);

        for (int g = 0; g < 64; g++) {
            #pragma omp simd
            for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++)
                Ybase[g] += X[row_index[k]];
            #pragma omp simd
            for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++)
                Ybase[g] -= X[row_index[k]];
        }
    }
}

void GEMV_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_G64_CS8_SIMD3_AVX2_OpenMP(const float* X, const int32_t* metadata, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 64; j++) {
        const int* groupData  = &metadata[j * 130];
        __m256 res0 = _mm256_setzero_ps();
        __m256 res1 = _mm256_setzero_ps();
        __m256 res2 = _mm256_setzero_ps();
        __m256 res3 = _mm256_setzero_ps();
        __m256 res4 = _mm256_setzero_ps();
        __m256 res5 = _mm256_setzero_ps();
        __m256 res6 = _mm256_setzero_ps();
        __m256 res7 = _mm256_setzero_ps();
        for (int k = groupData[0]; k < groupData[1]; k += 128) {
            __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
            __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
            __m256i indices_p1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 16));
            __m256i indices_n1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 24));
            __m256i indices_p2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 32));
            __m256i indices_n2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 40));
            __m256i indices_p3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 48));
            __m256i indices_n3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 56));
            __m256i indices_p4 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 64));
            __m256i indices_n4 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 72));
            __m256i indices_p5 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 80));
            __m256i indices_n5 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 88));
            __m256i indices_p6 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 96));
            __m256i indices_n6 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 104));
            __m256i indices_p7 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 112));
            __m256i indices_n7 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 120));
            __m256 pos0 = _mm256_i32gather_ps(X, indices_p0, 4);
            __m256 neg0 = _mm256_i32gather_ps(X, indices_n0, 4);
            __m256 pos1 = _mm256_i32gather_ps(X, indices_p1, 4);
            __m256 neg1 = _mm256_i32gather_ps(X, indices_n1, 4);
            __m256 pos2 = _mm256_i32gather_ps(X, indices_p2, 4);
            __m256 neg2 = _mm256_i32gather_ps(X, indices_n2, 4);
            __m256 pos3 = _mm256_i32gather_ps(X, indices_p3, 4);
            __m256 neg3 = _mm256_i32gather_ps(X, indices_n3, 4);
            __m256 pos4 = _mm256_i32gather_ps(X, indices_p4, 4);
            __m256 neg4 = _mm256_i32gather_ps(X, indices_n4, 4);
            __m256 pos5 = _mm256_i32gather_ps(X, indices_p5, 4);
            __m256 neg5 = _mm256_i32gather_ps(X, indices_n5, 4);
            __m256 pos6 = _mm256_i32gather_ps(X, indices_p6, 4);
            __m256 neg6 = _mm256_i32gather_ps(X, indices_n6, 4);
            __m256 pos7 = _mm256_i32gather_ps(X, indices_p7, 4);
            __m256 neg7 = _mm256_i32gather_ps(X, indices_n7, 4);
            res0 = _mm256_add_ps(res0, _mm256_sub_ps(pos0, neg0));
            res1 = _mm256_add_ps(res1, _mm256_sub_ps(pos1, neg1));
            res2 = _mm256_add_ps(res2, _mm256_sub_ps(pos2, neg2));
            res3 = _mm256_add_ps(res3, _mm256_sub_ps(pos3, neg3));
            res4 = _mm256_add_ps(res4, _mm256_sub_ps(pos4, neg4));
            res5 = _mm256_add_ps(res5, _mm256_sub_ps(pos5, neg5));
            res6 = _mm256_add_ps(res6, _mm256_sub_ps(pos6, neg6));
            res7 = _mm256_add_ps(res7, _mm256_sub_ps(pos7, neg7));
        }
        float* Ybase = result + j * 64;
        _mm256_store_ps(Ybase + 0, res0);
        _mm256_store_ps(Ybase + 8, res1);
        _mm256_store_ps(Ybase + 16, res2);
        _mm256_store_ps(Ybase + 24, res3);
        _mm256_store_ps(Ybase + 32, res4);
        _mm256_store_ps(Ybase + 40, res5);
        _mm256_store_ps(Ybase + 48, res6);
        _mm256_store_ps(Ybase + 56, res7);
#pragma omp simd
        for (int g = 0; g < 64; g++) {
#pragma omp simd
            for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++)
                Ybase[g] += X[row_index[k]];
#pragma omp simd
            for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++)
                Ybase[g] -= X[row_index[k]];
        }
    }
}

void GEMV_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_G128_CS8_AVX2_OpenMP(const float* X, const int32_t* metadata, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 128; j++) {
        const int* groupData  = &metadata[j * 258];
        __m256 res0 = _mm256_setzero_ps();
        __m256 res1 = _mm256_setzero_ps();
        __m256 res2 = _mm256_setzero_ps();
        __m256 res3 = _mm256_setzero_ps();
        __m256 res4 = _mm256_setzero_ps();
        __m256 res5 = _mm256_setzero_ps();
        __m256 res6 = _mm256_setzero_ps();
        __m256 res7 = _mm256_setzero_ps();
        __m256 res8 = _mm256_setzero_ps();
        __m256 res9 = _mm256_setzero_ps();
        __m256 res10 = _mm256_setzero_ps();
        __m256 res11 = _mm256_setzero_ps();
        __m256 res12 = _mm256_setzero_ps();
        __m256 res13 = _mm256_setzero_ps();
        __m256 res14 = _mm256_setzero_ps();
        __m256 res15 = _mm256_setzero_ps();
        for (int k = groupData[0]; k < groupData[1]; k += 256) {
            __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
            __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
            __m256i indices_p1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 16));
            __m256i indices_n1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 24));
            __m256i indices_p2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 32));
            __m256i indices_n2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 40));
            __m256i indices_p3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 48));
            __m256i indices_n3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 56));
            __m256i indices_p4 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 64));
            __m256i indices_n4 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 72));
            __m256i indices_p5 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 80));
            __m256i indices_n5 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 88));
            __m256i indices_p6 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 96));
            __m256i indices_n6 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 104));
            __m256i indices_p7 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 112));
            __m256i indices_n7 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 120));
            __m256i indices_p8 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 128));
            __m256i indices_n8 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 136));
            __m256i indices_p9 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 144));
            __m256i indices_n9 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 152));
            __m256i indices_p10 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 160));
            __m256i indices_n10 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 168));
            __m256i indices_p11 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 176));
            __m256i indices_n11 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 184));
            __m256i indices_p12 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 192));
            __m256i indices_n12 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 200));
            __m256i indices_p13 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 208));
            __m256i indices_n13 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 216));
            __m256i indices_p14 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 224));
            __m256i indices_n14 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 232));
            __m256i indices_p15 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 240));
            __m256i indices_n15 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 248));
            __m256 pos0 = _mm256_i32gather_ps(X, indices_p0, 4);
            __m256 neg0 = _mm256_i32gather_ps(X, indices_n0, 4);
            __m256 pos1 = _mm256_i32gather_ps(X, indices_p1, 4);
            __m256 neg1 = _mm256_i32gather_ps(X, indices_n1, 4);
            __m256 pos2 = _mm256_i32gather_ps(X, indices_p2, 4);
            __m256 neg2 = _mm256_i32gather_ps(X, indices_n2, 4);
            __m256 pos3 = _mm256_i32gather_ps(X, indices_p3, 4);
            __m256 neg3 = _mm256_i32gather_ps(X, indices_n3, 4);
            __m256 pos4 = _mm256_i32gather_ps(X, indices_p4, 4);
            __m256 neg4 = _mm256_i32gather_ps(X, indices_n4, 4);
            __m256 pos5 = _mm256_i32gather_ps(X, indices_p5, 4);
            __m256 neg5 = _mm256_i32gather_ps(X, indices_n5, 4);
            __m256 pos6 = _mm256_i32gather_ps(X, indices_p6, 4);
            __m256 neg6 = _mm256_i32gather_ps(X, indices_n6, 4);
            __m256 pos7 = _mm256_i32gather_ps(X, indices_p7, 4);
            __m256 neg7 = _mm256_i32gather_ps(X, indices_n7, 4);
            __m256 pos8 = _mm256_i32gather_ps(X, indices_p8, 4);
            __m256 neg8 = _mm256_i32gather_ps(X, indices_n8, 4);
            __m256 pos9 = _mm256_i32gather_ps(X, indices_p9, 4);
            __m256 neg9 = _mm256_i32gather_ps(X, indices_n9, 4);
            __m256 pos10 = _mm256_i32gather_ps(X, indices_p10, 4);
            __m256 neg10 = _mm256_i32gather_ps(X, indices_n10, 4);
            __m256 pos11 = _mm256_i32gather_ps(X, indices_p11, 4);
            __m256 neg11 = _mm256_i32gather_ps(X, indices_n11, 4);
            __m256 pos12 = _mm256_i32gather_ps(X, indices_p12, 4);
            __m256 neg12 = _mm256_i32gather_ps(X, indices_n12, 4);
            __m256 pos13 = _mm256_i32gather_ps(X, indices_p13, 4);
            __m256 neg13 = _mm256_i32gather_ps(X, indices_n13, 4);
            __m256 pos14 = _mm256_i32gather_ps(X, indices_p14, 4);
            __m256 neg14 = _mm256_i32gather_ps(X, indices_n14, 4);
            __m256 pos15 = _mm256_i32gather_ps(X, indices_p15, 4);
            __m256 neg15 = _mm256_i32gather_ps(X, indices_n15, 4);
            res0 = _mm256_add_ps(res0, _mm256_sub_ps(pos0, neg0));
            res1 = _mm256_add_ps(res1, _mm256_sub_ps(pos1, neg1));
            res2 = _mm256_add_ps(res2, _mm256_sub_ps(pos2, neg2));
            res3 = _mm256_add_ps(res3, _mm256_sub_ps(pos3, neg3));
            res4 = _mm256_add_ps(res4, _mm256_sub_ps(pos4, neg4));
            res5 = _mm256_add_ps(res5, _mm256_sub_ps(pos5, neg5));
            res6 = _mm256_add_ps(res6, _mm256_sub_ps(pos6, neg6));
            res7 = _mm256_add_ps(res7, _mm256_sub_ps(pos7, neg7));
            res8 = _mm256_add_ps(res8, _mm256_sub_ps(pos8, neg8));
            res9 = _mm256_add_ps(res9, _mm256_sub_ps(pos9, neg9));
            res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
            res11 = _mm256_add_ps(res11, _mm256_sub_ps(pos11, neg11));
            res12 = _mm256_add_ps(res12, _mm256_sub_ps(pos12, neg12));
            res13 = _mm256_add_ps(res13, _mm256_sub_ps(pos13, neg13));
            res14 = _mm256_add_ps(res14, _mm256_sub_ps(pos14, neg14));
            res15 = _mm256_add_ps(res15, _mm256_sub_ps(pos15, neg15));
        }
        float* Ybase = result + j * 128;
        _mm256_store_ps(Ybase + 0, res0);
        _mm256_store_ps(Ybase + 8, res1);
        _mm256_store_ps(Ybase + 16, res2);
        _mm256_store_ps(Ybase + 24, res3);
        _mm256_store_ps(Ybase + 32, res4);
        _mm256_store_ps(Ybase + 40, res5);
        _mm256_store_ps(Ybase + 48, res6);
        _mm256_store_ps(Ybase + 56, res7);
        _mm256_store_ps(Ybase + 64, res8);
        _mm256_store_ps(Ybase + 72, res9);
        _mm256_store_ps(Ybase + 80, res10);
        _mm256_store_ps(Ybase + 88, res11);
        _mm256_store_ps(Ybase + 96, res12);
        _mm256_store_ps(Ybase + 104, res13);
        _mm256_store_ps(Ybase + 112, res14);
        _mm256_store_ps(Ybase + 120, res15);
#pragma omp simd
        for (int g = 0; g < 128; g++) {
#pragma omp simd
            for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++)
                Ybase[g] += X[row_index[k]];
#pragma omp simd
            for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++)
                Ybase[g] -= X[row_index[k]];
        }
    }
}

void GEMV_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_G16_AVX512_OpenMP(const float* X, const int32_t* metadata, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 16; j++) {
        const int* groupData  = &metadata[j * 34];
        __m512 res0 = _mm512_setzero_ps();
        for (int k = groupData[0]; k < groupData[1]; k += 32) {
            __m512i indices_p0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 0));
            __m512i indices_n0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 16));
            __m512 pos0 = _mm512_i32gather_ps(indices_p0, X, 4);
            __m512 neg0 = _mm512_i32gather_ps(indices_n0, X, 4);
            res0 = _mm512_add_ps(res0, _mm512_sub_ps(pos0, neg0));
        }
        float* Ybase = result + j * 16;
        _mm512_store_ps(Ybase + 0, res0);
#pragma omp simd
        for (int g = 0; g < 16; g++) {
#pragma omp simd
            for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++)
                Ybase[g] += X[row_index[k]];
#pragma omp simd
            for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++)
                Ybase[g] -= X[row_index[k]];
        }
    }
}

void GEMV_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_G32_AVX512_OpenMP(const float* X, const int32_t* metadata, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 32; j++) {
        const int* groupData  = &metadata[j * 66];
        __m512 res0 = _mm512_setzero_ps();
        __m512 res1 = _mm512_setzero_ps();
        for (int k = groupData[0]; k < groupData[1]; k += 64) {
            __m512i indices_p0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 0));
            __m512i indices_p1 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 16));
            __m512i indices_n0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 32));
            __m512i indices_n1 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 48));
            __m512 pos0 = _mm512_i32gather_ps(indices_p0, X, 4);
            __m512 pos1 = _mm512_i32gather_ps(indices_p1, X, 4);
            __m512 neg0 = _mm512_i32gather_ps(indices_n0, X, 4);
            __m512 neg1 = _mm512_i32gather_ps(indices_n1, X, 4);
            res0 = _mm512_add_ps(res0, _mm512_sub_ps(pos0, neg0));
            res1 = _mm512_add_ps(res1, _mm512_sub_ps(pos1, neg1));
        }
        float* Ybase = result + j * 32;
        _mm512_store_ps(Ybase + 0, res0);
        _mm512_store_ps(Ybase + 16, res1);
#pragma omp simd
        for (int g = 0; g < 32; g++) {
#pragma omp simd
            for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++)
                Ybase[g] += X[row_index[k]];
#pragma omp simd
            for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++)
                Ybase[g] -= X[row_index[k]];
        }
    }
}

void GEMV_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_G32_SIMD3_AVX512_OpenMP(const float* X, const int32_t* metadata, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 32; j++) {
        const int* groupData  = &metadata[j * 66];
        __m512 res0 = _mm512_setzero_ps();
        __m512 res1 = _mm512_setzero_ps();
        for (int k = groupData[0]; k < groupData[1]; k += 64) {
            __m512i indices_p0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 0));
            __m512i indices_p1 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 16));
            __m512i indices_n0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 32));
            __m512i indices_n1 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 48));
            __m512 pos0 = _mm512_i32gather_ps(indices_p0, X, 4);
            __m512 pos1 = _mm512_i32gather_ps(indices_p1, X, 4);
            __m512 neg0 = _mm512_i32gather_ps(indices_n0, X, 4);
            __m512 neg1 = _mm512_i32gather_ps(indices_n1, X, 4);
            res0 = _mm512_add_ps(res0, _mm512_sub_ps(pos0, neg0));
            res1 = _mm512_add_ps(res1, _mm512_sub_ps(pos1, neg1));
        }
        float* Ybase = result + j * 32;
        _mm512_store_ps(Ybase + 0, res0);
        _mm512_store_ps(Ybase + 16, res1);
#pragma omp simd
        for (int g = 0; g < 32; g++) {
#pragma omp simd
            for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++)
                Ybase[g] += X[row_index[k]];
#pragma omp simd
            for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++)
                Ybase[g] -= X[row_index[k]];
        }
    }
}

void GEMV_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_G64_AVX512_OpenMP(const float* X, const int32_t* metadata, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 64; j++) {
        const int* groupData  = &metadata[j * 130];
        __m512 res0 = _mm512_setzero_ps();
        __m512 res1 = _mm512_setzero_ps();
        __m512 res2 = _mm512_setzero_ps();
        __m512 res3 = _mm512_setzero_ps();
        for (int k = groupData[0]; k < groupData[1]; k += 128) {
            __m512i indices_p0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 0));
            __m512i indices_p1 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 16));
            __m512i indices_p2 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 32));
            __m512i indices_p3 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 48));
            __m512i indices_n0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 64));
            __m512i indices_n1 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 80));
            __m512i indices_n2 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 96));
            __m512i indices_n3 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 112));
            __m512 pos0 = _mm512_i32gather_ps(indices_p0, X, 4);
            __m512 pos1 = _mm512_i32gather_ps(indices_p1, X, 4);
            __m512 pos2 = _mm512_i32gather_ps(indices_p2, X, 4);
            __m512 pos3 = _mm512_i32gather_ps(indices_p3, X, 4);
            __m512 neg0 = _mm512_i32gather_ps(indices_n0, X, 4);
            __m512 neg1 = _mm512_i32gather_ps(indices_n1, X, 4);
            __m512 neg2 = _mm512_i32gather_ps(indices_n2, X, 4);
            __m512 neg3 = _mm512_i32gather_ps(indices_n3, X, 4);
            res0 = _mm512_add_ps(res0, _mm512_sub_ps(pos0, neg0));
            res1 = _mm512_add_ps(res1, _mm512_sub_ps(pos1, neg1));
            res2 = _mm512_add_ps(res2, _mm512_sub_ps(pos2, neg2));
            res3 = _mm512_add_ps(res3, _mm512_sub_ps(pos3, neg3));
        }
        float* Ybase = result + j * 64;
        _mm512_store_ps(Ybase + 0, res0);
        _mm512_store_ps(Ybase + 16, res1);
        _mm512_store_ps(Ybase + 32, res2);
        _mm512_store_ps(Ybase + 48, res3);
#pragma omp simd
        for (int g = 0; g < 64; g++) {
#pragma omp simd
            for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++)
                Ybase[g] += X[row_index[k]];
#pragma omp simd
            for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++)
                Ybase[g] -= X[row_index[k]];
        }
    }
}

void GEMV_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_G128_AVX512_OpenMP(const float* X, const int32_t* metadata, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 128; j++) {
        const int* groupData  = &metadata[j * 258];
        __m512 res0 = _mm512_setzero_ps();
        __m512 res1 = _mm512_setzero_ps();
        __m512 res2 = _mm512_setzero_ps();
        __m512 res3 = _mm512_setzero_ps();
        __m512 res4 = _mm512_setzero_ps();
        __m512 res5 = _mm512_setzero_ps();
        __m512 res6 = _mm512_setzero_ps();
        __m512 res7 = _mm512_setzero_ps();
        for (int k = groupData[0]; k < groupData[1]; k += 256) {
            __m512i indices_p0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 0));
            __m512i indices_p1 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 16));
            __m512i indices_p2 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 32));
            __m512i indices_p3 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 48));
            __m512i indices_p4 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 64));
            __m512i indices_p5 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 80));
            __m512i indices_p6 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 96));
            __m512i indices_p7 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 112));
            __m512i indices_n0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 128));
            __m512i indices_n1 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 144));
            __m512i indices_n2 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 160));
            __m512i indices_n3 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 176));
            __m512i indices_n4 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 192));
            __m512i indices_n5 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 208));
            __m512i indices_n6 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 224));
            __m512i indices_n7 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 240));
            __m512 pos0 = _mm512_i32gather_ps(indices_p0, X, 4);
            __m512 pos1 = _mm512_i32gather_ps(indices_p1, X, 4);
            __m512 pos2 = _mm512_i32gather_ps(indices_p2, X, 4);
            __m512 pos3 = _mm512_i32gather_ps(indices_p3, X, 4);
            __m512 pos4 = _mm512_i32gather_ps(indices_p4, X, 4);
            __m512 pos5 = _mm512_i32gather_ps(indices_p5, X, 4);
            __m512 pos6 = _mm512_i32gather_ps(indices_p6, X, 4);
            __m512 pos7 = _mm512_i32gather_ps(indices_p7, X, 4);
            __m512 neg0 = _mm512_i32gather_ps(indices_n0, X, 4);
            __m512 neg1 = _mm512_i32gather_ps(indices_n1, X, 4);
            __m512 neg2 = _mm512_i32gather_ps(indices_n2, X, 4);
            __m512 neg3 = _mm512_i32gather_ps(indices_n3, X, 4);
            __m512 neg4 = _mm512_i32gather_ps(indices_n4, X, 4);
            __m512 neg5 = _mm512_i32gather_ps(indices_n5, X, 4);
            __m512 neg6 = _mm512_i32gather_ps(indices_n6, X, 4);
            __m512 neg7 = _mm512_i32gather_ps(indices_n7, X, 4);
            res0 = _mm512_add_ps(res0, _mm512_sub_ps(pos0, neg0));
            res1 = _mm512_add_ps(res1, _mm512_sub_ps(pos1, neg1));
            res2 = _mm512_add_ps(res2, _mm512_sub_ps(pos2, neg2));
            res3 = _mm512_add_ps(res3, _mm512_sub_ps(pos3, neg3));
            res4 = _mm512_add_ps(res4, _mm512_sub_ps(pos4, neg4));
            res5 = _mm512_add_ps(res5, _mm512_sub_ps(pos5, neg5));
            res6 = _mm512_add_ps(res6, _mm512_sub_ps(pos6, neg6));
            res7 = _mm512_add_ps(res7, _mm512_sub_ps(pos7, neg7));
        }
        float* Ybase = result + j * 128;
        _mm512_store_ps(Ybase + 0, res0);
        _mm512_store_ps(Ybase + 16, res1);
        _mm512_store_ps(Ybase + 32, res2);
        _mm512_store_ps(Ybase + 48, res3);
        _mm512_store_ps(Ybase + 64, res4);
        _mm512_store_ps(Ybase + 80, res5);
        _mm512_store_ps(Ybase + 96, res6);
        _mm512_store_ps(Ybase + 112, res7);
#pragma omp simd
        for (int g = 0; g < 128; g++) {
#pragma omp simd
            for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++)
                Ybase[g] += X[row_index[k]];
#pragma omp simd
            for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++)
                Ybase[g] -= X[row_index[k]];
        }
    }
}

void GEMV_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_G32_CS16_AVX512_OpenMP(const float* X, const int32_t* metadata, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 32; j++) {
        const int* groupData  = &metadata[j * 66];
        __m512 res0 = _mm512_setzero_ps();
        __m512 res1 = _mm512_setzero_ps();
        for (int k = groupData[0]; k < groupData[1]; k += 64) {
            __m512i indices_p0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 0));
            __m512i indices_n0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 16));
            __m512i indices_p1 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 32));
            __m512i indices_n1 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 48));
            __m512 pos0 = _mm512_i32gather_ps(indices_p0, X, 4);
            __m512 neg0 = _mm512_i32gather_ps(indices_n0, X, 4);
            __m512 pos1 = _mm512_i32gather_ps(indices_p1, X, 4);
            __m512 neg1 = _mm512_i32gather_ps(indices_n1, X, 4);
            res0 = _mm512_add_ps(res0, _mm512_sub_ps(pos0, neg0));
            res1 = _mm512_add_ps(res1, _mm512_sub_ps(pos1, neg1));
        }
        float* Ybase = result + j * 32;
        _mm512_store_ps(Ybase + 0, res0);
        _mm512_store_ps(Ybase + 16, res1);
#pragma omp simd
        for (int g = 0; g < 32; g++) {
#pragma omp simd
            for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++)
                Ybase[g] += X[row_index[k]];
#pragma omp simd
            for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++)
                Ybase[g] -= X[row_index[k]];
        }
    }
}

void GEMV_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_G64_CS16_AVX512_OpenMP(const float* X, const int32_t* metadata, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 64; j++) {
        const int* groupData  = &metadata[j * 130];
        __m512 res0 = _mm512_setzero_ps();
        __m512 res1 = _mm512_setzero_ps();
        __m512 res2 = _mm512_setzero_ps();
        __m512 res3 = _mm512_setzero_ps();
        for (int k = groupData[0]; k < groupData[1]; k += 128) {
            __m512i indices_p0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 0));
            __m512i indices_n0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 16));
            __m512i indices_p1 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 32));
            __m512i indices_n1 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 48));
            __m512i indices_p2 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 64));
            __m512i indices_n2 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 80));
            __m512i indices_p3 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 96));
            __m512i indices_n3 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 112));
            __m512 pos0 = _mm512_i32gather_ps(indices_p0, X, 4);
            __m512 neg0 = _mm512_i32gather_ps(indices_n0, X, 4);
            __m512 pos1 = _mm512_i32gather_ps(indices_p1, X, 4);
            __m512 neg1 = _mm512_i32gather_ps(indices_n1, X, 4);
            __m512 pos2 = _mm512_i32gather_ps(indices_p2, X, 4);
            __m512 neg2 = _mm512_i32gather_ps(indices_n2, X, 4);
            __m512 pos3 = _mm512_i32gather_ps(indices_p3, X, 4);
            __m512 neg3 = _mm512_i32gather_ps(indices_n3, X, 4);
            res0 = _mm512_add_ps(res0, _mm512_sub_ps(pos0, neg0));
            res1 = _mm512_add_ps(res1, _mm512_sub_ps(pos1, neg1));
            res2 = _mm512_add_ps(res2, _mm512_sub_ps(pos2, neg2));
            res3 = _mm512_add_ps(res3, _mm512_sub_ps(pos3, neg3));
        }
        float* Ybase = result + j * 64;
        _mm512_store_ps(Ybase + 0, res0);
        _mm512_store_ps(Ybase + 16, res1);
        _mm512_store_ps(Ybase + 32, res2);
        _mm512_store_ps(Ybase + 48, res3);
#pragma omp simd
        for (int g = 0; g < 64; g++) {
#pragma omp simd
            for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++)
                Ybase[g] += X[row_index[k]];
#pragma omp simd
            for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++)
                Ybase[g] -= X[row_index[k]];
        }
    }
}

void GEMV_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_G128_CS16_AVX512_OpenMP(const float* X, const int32_t* metadata, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 128; j++) {
        const int* groupData  = &metadata[j * 258];
        __m512 res0 = _mm512_setzero_ps();
        __m512 res1 = _mm512_setzero_ps();
        __m512 res2 = _mm512_setzero_ps();
        __m512 res3 = _mm512_setzero_ps();
        __m512 res4 = _mm512_setzero_ps();
        __m512 res5 = _mm512_setzero_ps();
        __m512 res6 = _mm512_setzero_ps();
        __m512 res7 = _mm512_setzero_ps();
        for (int k = groupData[0]; k < groupData[1]; k += 256) {
            __m512i indices_p0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 0));
            __m512i indices_n0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 16));
            __m512i indices_p1 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 32));
            __m512i indices_n1 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 48));
            __m512i indices_p2 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 64));
            __m512i indices_n2 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 80));
            __m512i indices_p3 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 96));
            __m512i indices_n3 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 112));
            __m512i indices_p4 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 128));
            __m512i indices_n4 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 144));
            __m512i indices_p5 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 160));
            __m512i indices_n5 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 176));
            __m512i indices_p6 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 192));
            __m512i indices_n6 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 208));
            __m512i indices_p7 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 224));
            __m512i indices_n7 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 240));
            __m512 pos0 = _mm512_i32gather_ps(indices_p0, X, 4);
            __m512 neg0 = _mm512_i32gather_ps(indices_n0, X, 4);
            __m512 pos1 = _mm512_i32gather_ps(indices_p1, X, 4);
            __m512 neg1 = _mm512_i32gather_ps(indices_n1, X, 4);
            __m512 pos2 = _mm512_i32gather_ps(indices_p2, X, 4);
            __m512 neg2 = _mm512_i32gather_ps(indices_n2, X, 4);
            __m512 pos3 = _mm512_i32gather_ps(indices_p3, X, 4);
            __m512 neg3 = _mm512_i32gather_ps(indices_n3, X, 4);
            __m512 pos4 = _mm512_i32gather_ps(indices_p4, X, 4);
            __m512 neg4 = _mm512_i32gather_ps(indices_n4, X, 4);
            __m512 pos5 = _mm512_i32gather_ps(indices_p5, X, 4);
            __m512 neg5 = _mm512_i32gather_ps(indices_n5, X, 4);
            __m512 pos6 = _mm512_i32gather_ps(indices_p6, X, 4);
            __m512 neg6 = _mm512_i32gather_ps(indices_n6, X, 4);
            __m512 pos7 = _mm512_i32gather_ps(indices_p7, X, 4);
            __m512 neg7 = _mm512_i32gather_ps(indices_n7, X, 4);
            res0 = _mm512_add_ps(res0, _mm512_sub_ps(pos0, neg0));
            res1 = _mm512_add_ps(res1, _mm512_sub_ps(pos1, neg1));
            res2 = _mm512_add_ps(res2, _mm512_sub_ps(pos2, neg2));
            res3 = _mm512_add_ps(res3, _mm512_sub_ps(pos3, neg3));
            res4 = _mm512_add_ps(res4, _mm512_sub_ps(pos4, neg4));
            res5 = _mm512_add_ps(res5, _mm512_sub_ps(pos5, neg5));
            res6 = _mm512_add_ps(res6, _mm512_sub_ps(pos6, neg6));
            res7 = _mm512_add_ps(res7, _mm512_sub_ps(pos7, neg7));
        }
        float* Ybase = result + j * 128;
        _mm512_store_ps(Ybase + 0, res0);
        _mm512_store_ps(Ybase + 16, res1);
        _mm512_store_ps(Ybase + 32, res2);
        _mm512_store_ps(Ybase + 48, res3);
        _mm512_store_ps(Ybase + 64, res4);
        _mm512_store_ps(Ybase + 80, res5);
        _mm512_store_ps(Ybase + 96, res6);
        _mm512_store_ps(Ybase + 112, res7);
#pragma omp simd
        for (int g = 0; g < 128; g++) {
#pragma omp simd
            for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++)
                Ybase[g] += X[row_index[k]];
#pragma omp simd
            for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++)
                Ybase[g] -= X[row_index[k]];
        }
    }
}

void GEMV_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_G128_CS16_SIMD1_AVX512_OpenMP(const float* X, const int32_t* metadata, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 128; j++) {
        const int* groupData  = &metadata[j * 258];
        __m512 res0 = _mm512_setzero_ps();
        __m512 res1 = _mm512_setzero_ps();
        __m512 res2 = _mm512_setzero_ps();
        __m512 res3 = _mm512_setzero_ps();
        __m512 res4 = _mm512_setzero_ps();
        __m512 res5 = _mm512_setzero_ps();
        __m512 res6 = _mm512_setzero_ps();
        __m512 res7 = _mm512_setzero_ps();
        for (int k = groupData[0]; k < groupData[1]; k += 256) {
            __m512i indices_p0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 0));
            __m512i indices_n0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 16));
            __m512i indices_p1 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 32));
            __m512i indices_n1 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 48));
            __m512i indices_p2 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 64));
            __m512i indices_n2 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 80));
            __m512i indices_p3 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 96));
            __m512i indices_n3 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 112));
            __m512i indices_p4 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 128));
            __m512i indices_n4 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 144));
            __m512i indices_p5 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 160));
            __m512i indices_n5 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 176));
            __m512i indices_p6 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 192));
            __m512i indices_n6 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 208));
            __m512i indices_p7 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 224));
            __m512i indices_n7 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 240));
            __m512 pos0 = _mm512_i32gather_ps(indices_p0, X, 4);
            __m512 neg0 = _mm512_i32gather_ps(indices_n0, X, 4);
            __m512 pos1 = _mm512_i32gather_ps(indices_p1, X, 4);
            __m512 neg1 = _mm512_i32gather_ps(indices_n1, X, 4);
            __m512 pos2 = _mm512_i32gather_ps(indices_p2, X, 4);
            __m512 neg2 = _mm512_i32gather_ps(indices_n2, X, 4);
            __m512 pos3 = _mm512_i32gather_ps(indices_p3, X, 4);
            __m512 neg3 = _mm512_i32gather_ps(indices_n3, X, 4);
            __m512 pos4 = _mm512_i32gather_ps(indices_p4, X, 4);
            __m512 neg4 = _mm512_i32gather_ps(indices_n4, X, 4);
            __m512 pos5 = _mm512_i32gather_ps(indices_p5, X, 4);
            __m512 neg5 = _mm512_i32gather_ps(indices_n5, X, 4);
            __m512 pos6 = _mm512_i32gather_ps(indices_p6, X, 4);
            __m512 neg6 = _mm512_i32gather_ps(indices_n6, X, 4);
            __m512 pos7 = _mm512_i32gather_ps(indices_p7, X, 4);
            __m512 neg7 = _mm512_i32gather_ps(indices_n7, X, 4);
            res0 = _mm512_add_ps(res0, _mm512_sub_ps(pos0, neg0));
            res1 = _mm512_add_ps(res1, _mm512_sub_ps(pos1, neg1));
            res2 = _mm512_add_ps(res2, _mm512_sub_ps(pos2, neg2));
            res3 = _mm512_add_ps(res3, _mm512_sub_ps(pos3, neg3));
            res4 = _mm512_add_ps(res4, _mm512_sub_ps(pos4, neg4));
            res5 = _mm512_add_ps(res5, _mm512_sub_ps(pos5, neg5));
            res6 = _mm512_add_ps(res6, _mm512_sub_ps(pos6, neg6));
            res7 = _mm512_add_ps(res7, _mm512_sub_ps(pos7, neg7));
        }
        float* Ybase = result + j * 128;
        _mm512_store_ps(Ybase + 0, res0);
        _mm512_store_ps(Ybase + 16, res1);
        _mm512_store_ps(Ybase + 32, res2);
        _mm512_store_ps(Ybase + 48, res3);
        _mm512_store_ps(Ybase + 64, res4);
        _mm512_store_ps(Ybase + 80, res5);
        _mm512_store_ps(Ybase + 96, res6);
        _mm512_store_ps(Ybase + 112, res7);
#pragma omp simd
        for (int g = 0; g < 128; g++) {
            for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++)
                Ybase[g] += X[row_index[k]];
            for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++)
                Ybase[g] -= X[row_index[k]];
        }
    }
}

void GEMV_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_G128_CS16_SIMD2_AVX512_OpenMP(const float* X, const int32_t* metadata, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 128; j++) {
        const int* groupData  = &metadata[j * 258];
        __m512 res0 = _mm512_setzero_ps();
        __m512 res1 = _mm512_setzero_ps();
        __m512 res2 = _mm512_setzero_ps();
        __m512 res3 = _mm512_setzero_ps();
        __m512 res4 = _mm512_setzero_ps();
        __m512 res5 = _mm512_setzero_ps();
        __m512 res6 = _mm512_setzero_ps();
        __m512 res7 = _mm512_setzero_ps();
        for (int k = groupData[0]; k < groupData[1]; k += 256) {
            __m512i indices_p0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 0));
            __m512i indices_n0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 16));
            __m512i indices_p1 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 32));
            __m512i indices_n1 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 48));
            __m512i indices_p2 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 64));
            __m512i indices_n2 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 80));
            __m512i indices_p3 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 96));
            __m512i indices_n3 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 112));
            __m512i indices_p4 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 128));
            __m512i indices_n4 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 144));
            __m512i indices_p5 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 160));
            __m512i indices_n5 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 176));
            __m512i indices_p6 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 192));
            __m512i indices_n6 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 208));
            __m512i indices_p7 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 224));
            __m512i indices_n7 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 240));
            __m512 pos0 = _mm512_i32gather_ps(indices_p0, X, 4);
            __m512 neg0 = _mm512_i32gather_ps(indices_n0, X, 4);
            __m512 pos1 = _mm512_i32gather_ps(indices_p1, X, 4);
            __m512 neg1 = _mm512_i32gather_ps(indices_n1, X, 4);
            __m512 pos2 = _mm512_i32gather_ps(indices_p2, X, 4);
            __m512 neg2 = _mm512_i32gather_ps(indices_n2, X, 4);
            __m512 pos3 = _mm512_i32gather_ps(indices_p3, X, 4);
            __m512 neg3 = _mm512_i32gather_ps(indices_n3, X, 4);
            __m512 pos4 = _mm512_i32gather_ps(indices_p4, X, 4);
            __m512 neg4 = _mm512_i32gather_ps(indices_n4, X, 4);
            __m512 pos5 = _mm512_i32gather_ps(indices_p5, X, 4);
            __m512 neg5 = _mm512_i32gather_ps(indices_n5, X, 4);
            __m512 pos6 = _mm512_i32gather_ps(indices_p6, X, 4);
            __m512 neg6 = _mm512_i32gather_ps(indices_n6, X, 4);
            __m512 pos7 = _mm512_i32gather_ps(indices_p7, X, 4);
            __m512 neg7 = _mm512_i32gather_ps(indices_n7, X, 4);
            res0 = _mm512_add_ps(res0, _mm512_sub_ps(pos0, neg0));
            res1 = _mm512_add_ps(res1, _mm512_sub_ps(pos1, neg1));
            res2 = _mm512_add_ps(res2, _mm512_sub_ps(pos2, neg2));
            res3 = _mm512_add_ps(res3, _mm512_sub_ps(pos3, neg3));
            res4 = _mm512_add_ps(res4, _mm512_sub_ps(pos4, neg4));
            res5 = _mm512_add_ps(res5, _mm512_sub_ps(pos5, neg5));
            res6 = _mm512_add_ps(res6, _mm512_sub_ps(pos6, neg6));
            res7 = _mm512_add_ps(res7, _mm512_sub_ps(pos7, neg7));
        }
        float* Ybase = result + j * 128;
        _mm512_store_ps(Ybase + 0, res0);
        _mm512_store_ps(Ybase + 16, res1);
        _mm512_store_ps(Ybase + 32, res2);
        _mm512_store_ps(Ybase + 48, res3);
        _mm512_store_ps(Ybase + 64, res4);
        _mm512_store_ps(Ybase + 80, res5);
        _mm512_store_ps(Ybase + 96, res6);
        _mm512_store_ps(Ybase + 112, res7);
        for (int g = 0; g < 128; g++) {
#pragma omp simd
            for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++)
                Ybase[g] += X[row_index[k]];
#pragma omp simd
            for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++)
                Ybase[g] -= X[row_index[k]];
        }
    }
}

void GEMV_CPU_FP32_rowMajor_TCSC_Merged_GroupMin_G128_CS16_SIMD3_AVX512_OpenMP(const float* X, const int32_t* metadata, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 128; j++) {
        const int* groupData  = &metadata[j * 258];
        __m512 res0 = _mm512_setzero_ps();
        __m512 res1 = _mm512_setzero_ps();
        __m512 res2 = _mm512_setzero_ps();
        __m512 res3 = _mm512_setzero_ps();
        __m512 res4 = _mm512_setzero_ps();
        __m512 res5 = _mm512_setzero_ps();
        __m512 res6 = _mm512_setzero_ps();
        __m512 res7 = _mm512_setzero_ps();
        for (int k = groupData[0]; k < groupData[1]; k += 256) {
            __m512i indices_p0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 0));
            __m512i indices_n0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 16));
            __m512i indices_p1 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 32));
            __m512i indices_n1 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 48));
            __m512i indices_p2 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 64));
            __m512i indices_n2 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 80));
            __m512i indices_p3 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 96));
            __m512i indices_n3 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 112));
            __m512i indices_p4 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 128));
            __m512i indices_n4 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 144));
            __m512i indices_p5 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 160));
            __m512i indices_n5 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 176));
            __m512i indices_p6 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 192));
            __m512i indices_n6 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 208));
            __m512i indices_p7 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 224));
            __m512i indices_n7 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 240));
            __m512 pos0 = _mm512_i32gather_ps(indices_p0, X, 4);
            __m512 neg0 = _mm512_i32gather_ps(indices_n0, X, 4);
            __m512 pos1 = _mm512_i32gather_ps(indices_p1, X, 4);
            __m512 neg1 = _mm512_i32gather_ps(indices_n1, X, 4);
            __m512 pos2 = _mm512_i32gather_ps(indices_p2, X, 4);
            __m512 neg2 = _mm512_i32gather_ps(indices_n2, X, 4);
            __m512 pos3 = _mm512_i32gather_ps(indices_p3, X, 4);
            __m512 neg3 = _mm512_i32gather_ps(indices_n3, X, 4);
            __m512 pos4 = _mm512_i32gather_ps(indices_p4, X, 4);
            __m512 neg4 = _mm512_i32gather_ps(indices_n4, X, 4);
            __m512 pos5 = _mm512_i32gather_ps(indices_p5, X, 4);
            __m512 neg5 = _mm512_i32gather_ps(indices_n5, X, 4);
            __m512 pos6 = _mm512_i32gather_ps(indices_p6, X, 4);
            __m512 neg6 = _mm512_i32gather_ps(indices_n6, X, 4);
            __m512 pos7 = _mm512_i32gather_ps(indices_p7, X, 4);
            __m512 neg7 = _mm512_i32gather_ps(indices_n7, X, 4);
            res0 = _mm512_add_ps(res0, _mm512_sub_ps(pos0, neg0));
            res1 = _mm512_add_ps(res1, _mm512_sub_ps(pos1, neg1));
            res2 = _mm512_add_ps(res2, _mm512_sub_ps(pos2, neg2));
            res3 = _mm512_add_ps(res3, _mm512_sub_ps(pos3, neg3));
            res4 = _mm512_add_ps(res4, _mm512_sub_ps(pos4, neg4));
            res5 = _mm512_add_ps(res5, _mm512_sub_ps(pos5, neg5));
            res6 = _mm512_add_ps(res6, _mm512_sub_ps(pos6, neg6));
            res7 = _mm512_add_ps(res7, _mm512_sub_ps(pos7, neg7));
        }
        float* Ybase = result + j * 128;
        _mm512_store_ps(Ybase + 0, res0);
        _mm512_store_ps(Ybase + 16, res1);
        _mm512_store_ps(Ybase + 32, res2);
        _mm512_store_ps(Ybase + 48, res3);
        _mm512_store_ps(Ybase + 64, res4);
        _mm512_store_ps(Ybase + 80, res5);
        _mm512_store_ps(Ybase + 96, res6);
        _mm512_store_ps(Ybase + 112, res7);
#pragma omp simd
        for (int g = 0; g < 128; g++) {
#pragma omp simd
            for (int k = groupData[2 * g + 1]; k < groupData[2 * g + 2]; k++)
                Ybase[g] += X[row_index[k]];
#pragma omp simd
            for (int k = groupData[2 * g + 2]; k < groupData[2 * g + 3]; k++)
                Ybase[g] -= X[row_index[k]];
        }
    }
}




void GEMV_CPU_FP32_rowMajor_TCSC_Uniform_G8_AVX2_OpenMP(const float* X, const int32_t NonZeroPerCol, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 8; j++) {
        __m256 res0 = _mm256_setzero_ps();
        for (int k = j * 8 * NonZeroPerCol; k < (j + 1) * 8 * NonZeroPerCol; k += 16) {
            __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
            __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
            __m256 pos0 = _mm256_i32gather_ps(X, indices_p0, 4);
            __m256 neg0 = _mm256_i32gather_ps(X, indices_n0, 4);
            res0 = _mm256_add_ps(res0, _mm256_sub_ps(pos0, neg0));
        }
        float* Ybase = result + j * 8;
        _mm256_store_ps(Ybase + 0, res0);
    }
}

void GEMV_CPU_FP32_rowMajor_TCSC_Uniform_G16_AVX2_OpenMP(const float* X, const int32_t NonZeroPerCol, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 16; j++) {
        __m256 res0 = _mm256_setzero_ps();
        __m256 res1 = _mm256_setzero_ps();
        for (int k = j * 16 * NonZeroPerCol; k < (j + 1) * 16 * NonZeroPerCol; k += 32) {
            __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
            __m256i indices_p1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
            __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 16));
            __m256i indices_n1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 24));
            __m256 pos0 = _mm256_i32gather_ps(X, indices_p0, 4);
            __m256 pos1 = _mm256_i32gather_ps(X, indices_p1, 4);
            __m256 neg0 = _mm256_i32gather_ps(X, indices_n0, 4);
            __m256 neg1 = _mm256_i32gather_ps(X, indices_n1, 4);
            res0 = _mm256_add_ps(res0, _mm256_sub_ps(pos0, neg0));
            res1 = _mm256_add_ps(res1, _mm256_sub_ps(pos1, neg1));
        }
        float* Ybase = result + j * 16;
        _mm256_store_ps(Ybase + 0, res0);
        _mm256_store_ps(Ybase + 8, res1);
    }
}

void GEMV_CPU_FP32_rowMajor_TCSC_Uniform_G32_AVX2_OpenMP(const float* X, const int32_t NonZeroPerCol, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 32; j++) {
        __m256 res0 = _mm256_setzero_ps();
        __m256 res1 = _mm256_setzero_ps();
        __m256 res2 = _mm256_setzero_ps();
        __m256 res3 = _mm256_setzero_ps();
        for (int k = j * 32 * NonZeroPerCol; k < (j + 1) * 32 * NonZeroPerCol; k += 64) {
            __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
            __m256i indices_p1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
            __m256i indices_p2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 16));
            __m256i indices_p3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 24));
            __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 32));
            __m256i indices_n1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 40));
            __m256i indices_n2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 48));
            __m256i indices_n3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 56));
            __m256 pos0 = _mm256_i32gather_ps(X, indices_p0, 4);
            __m256 pos1 = _mm256_i32gather_ps(X, indices_p1, 4);
            __m256 pos2 = _mm256_i32gather_ps(X, indices_p2, 4);
            __m256 pos3 = _mm256_i32gather_ps(X, indices_p3, 4);
            __m256 neg0 = _mm256_i32gather_ps(X, indices_n0, 4);
            __m256 neg1 = _mm256_i32gather_ps(X, indices_n1, 4);
            __m256 neg2 = _mm256_i32gather_ps(X, indices_n2, 4);
            __m256 neg3 = _mm256_i32gather_ps(X, indices_n3, 4);
            res0 = _mm256_add_ps(res0, _mm256_sub_ps(pos0, neg0));
            res1 = _mm256_add_ps(res1, _mm256_sub_ps(pos1, neg1));
            res2 = _mm256_add_ps(res2, _mm256_sub_ps(pos2, neg2));
            res3 = _mm256_add_ps(res3, _mm256_sub_ps(pos3, neg3));
        }
        float* Ybase = result + j * 32;
        _mm256_store_ps(Ybase + 0, res0);
        _mm256_store_ps(Ybase + 8, res1);
        _mm256_store_ps(Ybase + 16, res2);
        _mm256_store_ps(Ybase + 24, res3);
    }
}

void GEMV_CPU_FP32_rowMajor_TCSC_Uniform_G64_AVX2_OpenMP(const float* X, const int32_t NonZeroPerCol, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 64; j++) {
        __m256 res0 = _mm256_setzero_ps();
        __m256 res1 = _mm256_setzero_ps();
        __m256 res2 = _mm256_setzero_ps();
        __m256 res3 = _mm256_setzero_ps();
        __m256 res4 = _mm256_setzero_ps();
        __m256 res5 = _mm256_setzero_ps();
        __m256 res6 = _mm256_setzero_ps();
        __m256 res7 = _mm256_setzero_ps();
        for (int k = j * 64 * NonZeroPerCol; k < (j + 1) * 64 * NonZeroPerCol; k += 128) {
            __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
            __m256i indices_p1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
            __m256i indices_p2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 16));
            __m256i indices_p3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 24));
            __m256i indices_p4 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 32));
            __m256i indices_p5 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 40));
            __m256i indices_p6 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 48));
            __m256i indices_p7 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 56));
            __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 64));
            __m256i indices_n1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 72));
            __m256i indices_n2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 80));
            __m256i indices_n3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 88));
            __m256i indices_n4 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 96));
            __m256i indices_n5 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 104));
            __m256i indices_n6 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 112));
            __m256i indices_n7 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 120));
            __m256 pos0 = _mm256_i32gather_ps(X, indices_p0, 4);
            __m256 pos1 = _mm256_i32gather_ps(X, indices_p1, 4);
            __m256 pos2 = _mm256_i32gather_ps(X, indices_p2, 4);
            __m256 pos3 = _mm256_i32gather_ps(X, indices_p3, 4);
            __m256 pos4 = _mm256_i32gather_ps(X, indices_p4, 4);
            __m256 pos5 = _mm256_i32gather_ps(X, indices_p5, 4);
            __m256 pos6 = _mm256_i32gather_ps(X, indices_p6, 4);
            __m256 pos7 = _mm256_i32gather_ps(X, indices_p7, 4);
            __m256 neg0 = _mm256_i32gather_ps(X, indices_n0, 4);
            __m256 neg1 = _mm256_i32gather_ps(X, indices_n1, 4);
            __m256 neg2 = _mm256_i32gather_ps(X, indices_n2, 4);
            __m256 neg3 = _mm256_i32gather_ps(X, indices_n3, 4);
            __m256 neg4 = _mm256_i32gather_ps(X, indices_n4, 4);
            __m256 neg5 = _mm256_i32gather_ps(X, indices_n5, 4);
            __m256 neg6 = _mm256_i32gather_ps(X, indices_n6, 4);
            __m256 neg7 = _mm256_i32gather_ps(X, indices_n7, 4);
            res0 = _mm256_add_ps(res0, _mm256_sub_ps(pos0, neg0));
            res1 = _mm256_add_ps(res1, _mm256_sub_ps(pos1, neg1));
            res2 = _mm256_add_ps(res2, _mm256_sub_ps(pos2, neg2));
            res3 = _mm256_add_ps(res3, _mm256_sub_ps(pos3, neg3));
            res4 = _mm256_add_ps(res4, _mm256_sub_ps(pos4, neg4));
            res5 = _mm256_add_ps(res5, _mm256_sub_ps(pos5, neg5));
            res6 = _mm256_add_ps(res6, _mm256_sub_ps(pos6, neg6));
            res7 = _mm256_add_ps(res7, _mm256_sub_ps(pos7, neg7));
        }
        float* Ybase = result + j * 64;
        _mm256_store_ps(Ybase + 0, res0);
        _mm256_store_ps(Ybase + 8, res1);
        _mm256_store_ps(Ybase + 16, res2);
        _mm256_store_ps(Ybase + 24, res3);
        _mm256_store_ps(Ybase + 32, res4);
        _mm256_store_ps(Ybase + 40, res5);
        _mm256_store_ps(Ybase + 48, res6);
        _mm256_store_ps(Ybase + 56, res7);
    }
}

void GEMV_CPU_FP32_rowMajor_TCSC_Uniform_G16_CS8_AVX2_OpenMP(const float* X, const int32_t NonZeroPerCol, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 16; j++) {
        __m256 res0 = _mm256_setzero_ps();
        __m256 res1 = _mm256_setzero_ps();
        for (int k = j * 16 * NonZeroPerCol; k < (j + 1) * 16 * NonZeroPerCol; k += 32) {
            __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
            __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
            __m256i indices_p1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 16));
            __m256i indices_n1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 24));
            __m256 pos0 = _mm256_i32gather_ps(X, indices_p0, 4);
            __m256 neg0 = _mm256_i32gather_ps(X, indices_n0, 4);
            __m256 pos1 = _mm256_i32gather_ps(X, indices_p1, 4);
            __m256 neg1 = _mm256_i32gather_ps(X, indices_n1, 4);
            res0 = _mm256_add_ps(res0, _mm256_sub_ps(pos0, neg0));
            res1 = _mm256_add_ps(res1, _mm256_sub_ps(pos1, neg1));
        }
        float* Ybase = result + j * 16;
        _mm256_store_ps(Ybase + 0, res0);
        _mm256_store_ps(Ybase + 8, res1);
    }
}

void GEMV_CPU_FP32_rowMajor_TCSC_Uniform_G32_CS8_AVX2_OpenMP(const float* X, const int32_t NonZeroPerCol, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 32; j++) {
        __m256 res0 = _mm256_setzero_ps();
        __m256 res1 = _mm256_setzero_ps();
        __m256 res2 = _mm256_setzero_ps();
        __m256 res3 = _mm256_setzero_ps();
        for (int k = j * 32 * NonZeroPerCol; k < (j + 1) * 32 * NonZeroPerCol; k += 64) {
            __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
            __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
            __m256i indices_p1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 16));
            __m256i indices_n1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 24));
            __m256i indices_p2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 32));
            __m256i indices_n2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 40));
            __m256i indices_p3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 48));
            __m256i indices_n3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 56));
            __m256 pos0 = _mm256_i32gather_ps(X, indices_p0, 4);
            __m256 neg0 = _mm256_i32gather_ps(X, indices_n0, 4);
            __m256 pos1 = _mm256_i32gather_ps(X, indices_p1, 4);
            __m256 neg1 = _mm256_i32gather_ps(X, indices_n1, 4);
            __m256 pos2 = _mm256_i32gather_ps(X, indices_p2, 4);
            __m256 neg2 = _mm256_i32gather_ps(X, indices_n2, 4);
            __m256 pos3 = _mm256_i32gather_ps(X, indices_p3, 4);
            __m256 neg3 = _mm256_i32gather_ps(X, indices_n3, 4);
            res0 = _mm256_add_ps(res0, _mm256_sub_ps(pos0, neg0));
            res1 = _mm256_add_ps(res1, _mm256_sub_ps(pos1, neg1));
            res2 = _mm256_add_ps(res2, _mm256_sub_ps(pos2, neg2));
            res3 = _mm256_add_ps(res3, _mm256_sub_ps(pos3, neg3));
        }
        float* Ybase = result + j * 32;
        _mm256_store_ps(Ybase + 0, res0);
        _mm256_store_ps(Ybase + 8, res1);
        _mm256_store_ps(Ybase + 16, res2);
        _mm256_store_ps(Ybase + 24, res3);
    }
}

void GEMV_CPU_FP32_rowMajor_TCSC_Uniform_G64_CS8_AVX2_OpenMP(const float* X, const int32_t NonZeroPerCol, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 64; j++) {
        __m256 res0 = _mm256_setzero_ps();
        __m256 res1 = _mm256_setzero_ps();
        __m256 res2 = _mm256_setzero_ps();
        __m256 res3 = _mm256_setzero_ps();
        __m256 res4 = _mm256_setzero_ps();
        __m256 res5 = _mm256_setzero_ps();
        __m256 res6 = _mm256_setzero_ps();
        __m256 res7 = _mm256_setzero_ps();
        for (int k = j * 64 * NonZeroPerCol; k < (j + 1) * 64 * NonZeroPerCol; k += 128) {
            __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
            __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
            __m256i indices_p1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 16));
            __m256i indices_n1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 24));
            __m256i indices_p2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 32));
            __m256i indices_n2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 40));
            __m256i indices_p3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 48));
            __m256i indices_n3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 56));
            __m256i indices_p4 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 64));
            __m256i indices_n4 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 72));
            __m256i indices_p5 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 80));
            __m256i indices_n5 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 88));
            __m256i indices_p6 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 96));
            __m256i indices_n6 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 104));
            __m256i indices_p7 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 112));
            __m256i indices_n7 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 120));
            __m256 pos0 = _mm256_i32gather_ps(X, indices_p0, 4);
            __m256 neg0 = _mm256_i32gather_ps(X, indices_n0, 4);
            __m256 pos1 = _mm256_i32gather_ps(X, indices_p1, 4);
            __m256 neg1 = _mm256_i32gather_ps(X, indices_n1, 4);
            __m256 pos2 = _mm256_i32gather_ps(X, indices_p2, 4);
            __m256 neg2 = _mm256_i32gather_ps(X, indices_n2, 4);
            __m256 pos3 = _mm256_i32gather_ps(X, indices_p3, 4);
            __m256 neg3 = _mm256_i32gather_ps(X, indices_n3, 4);
            __m256 pos4 = _mm256_i32gather_ps(X, indices_p4, 4);
            __m256 neg4 = _mm256_i32gather_ps(X, indices_n4, 4);
            __m256 pos5 = _mm256_i32gather_ps(X, indices_p5, 4);
            __m256 neg5 = _mm256_i32gather_ps(X, indices_n5, 4);
            __m256 pos6 = _mm256_i32gather_ps(X, indices_p6, 4);
            __m256 neg6 = _mm256_i32gather_ps(X, indices_n6, 4);
            __m256 pos7 = _mm256_i32gather_ps(X, indices_p7, 4);
            __m256 neg7 = _mm256_i32gather_ps(X, indices_n7, 4);
            res0 = _mm256_add_ps(res0, _mm256_sub_ps(pos0, neg0));
            res1 = _mm256_add_ps(res1, _mm256_sub_ps(pos1, neg1));
            res2 = _mm256_add_ps(res2, _mm256_sub_ps(pos2, neg2));
            res3 = _mm256_add_ps(res3, _mm256_sub_ps(pos3, neg3));
            res4 = _mm256_add_ps(res4, _mm256_sub_ps(pos4, neg4));
            res5 = _mm256_add_ps(res5, _mm256_sub_ps(pos5, neg5));
            res6 = _mm256_add_ps(res6, _mm256_sub_ps(pos6, neg6));
            res7 = _mm256_add_ps(res7, _mm256_sub_ps(pos7, neg7));
        }
        float* Ybase = result + j * 64;
        _mm256_store_ps(Ybase + 0, res0);
        _mm256_store_ps(Ybase + 8, res1);
        _mm256_store_ps(Ybase + 16, res2);
        _mm256_store_ps(Ybase + 24, res3);
        _mm256_store_ps(Ybase + 32, res4);
        _mm256_store_ps(Ybase + 40, res5);
        _mm256_store_ps(Ybase + 48, res6);
        _mm256_store_ps(Ybase + 56, res7);
    }
}

void GEMV_CPU_FP32_rowMajor_TCSC_Uniform_G128_CS8_AVX2_OpenMP(const float* X, const int32_t NonZeroPerCol, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 128; j++) {
        __m256 res0 = _mm256_setzero_ps();
        __m256 res1 = _mm256_setzero_ps();
        __m256 res2 = _mm256_setzero_ps();
        __m256 res3 = _mm256_setzero_ps();
        __m256 res4 = _mm256_setzero_ps();
        __m256 res5 = _mm256_setzero_ps();
        __m256 res6 = _mm256_setzero_ps();
        __m256 res7 = _mm256_setzero_ps();
        __m256 res8 = _mm256_setzero_ps();
        __m256 res9 = _mm256_setzero_ps();
        __m256 res10 = _mm256_setzero_ps();
        __m256 res11 = _mm256_setzero_ps();
        __m256 res12 = _mm256_setzero_ps();
        __m256 res13 = _mm256_setzero_ps();
        __m256 res14 = _mm256_setzero_ps();
        __m256 res15 = _mm256_setzero_ps();
        for (int k = j * 128 * NonZeroPerCol; k < (j + 1) * 128 * NonZeroPerCol; k += 256) {
            __m256i indices_p0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 0));
            __m256i indices_n0 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 8));
            __m256i indices_p1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 16));
            __m256i indices_n1 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 24));
            __m256i indices_p2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 32));
            __m256i indices_n2 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 40));
            __m256i indices_p3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 48));
            __m256i indices_n3 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 56));
            __m256i indices_p4 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 64));
            __m256i indices_n4 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 72));
            __m256i indices_p5 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 80));
            __m256i indices_n5 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 88));
            __m256i indices_p6 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 96));
            __m256i indices_n6 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 104));
            __m256i indices_p7 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 112));
            __m256i indices_n7 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 120));
            __m256i indices_p8 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 128));
            __m256i indices_n8 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 136));
            __m256i indices_p9 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 144));
            __m256i indices_n9 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 152));
            __m256i indices_p10 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 160));
            __m256i indices_n10 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 168));
            __m256i indices_p11 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 176));
            __m256i indices_n11 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 184));
            __m256i indices_p12 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 192));
            __m256i indices_n12 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 200));
            __m256i indices_p13 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 208));
            __m256i indices_n13 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 216));
            __m256i indices_p14 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 224));
            __m256i indices_n14 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 232));
            __m256i indices_p15 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 240));
            __m256i indices_n15 = _mm256_load_si256(reinterpret_cast<const __m256i*>(row_index + k + 248));
            __m256 pos0 = _mm256_i32gather_ps(X, indices_p0, 4);
            __m256 neg0 = _mm256_i32gather_ps(X, indices_n0, 4);
            __m256 pos1 = _mm256_i32gather_ps(X, indices_p1, 4);
            __m256 neg1 = _mm256_i32gather_ps(X, indices_n1, 4);
            __m256 pos2 = _mm256_i32gather_ps(X, indices_p2, 4);
            __m256 neg2 = _mm256_i32gather_ps(X, indices_n2, 4);
            __m256 pos3 = _mm256_i32gather_ps(X, indices_p3, 4);
            __m256 neg3 = _mm256_i32gather_ps(X, indices_n3, 4);
            __m256 pos4 = _mm256_i32gather_ps(X, indices_p4, 4);
            __m256 neg4 = _mm256_i32gather_ps(X, indices_n4, 4);
            __m256 pos5 = _mm256_i32gather_ps(X, indices_p5, 4);
            __m256 neg5 = _mm256_i32gather_ps(X, indices_n5, 4);
            __m256 pos6 = _mm256_i32gather_ps(X, indices_p6, 4);
            __m256 neg6 = _mm256_i32gather_ps(X, indices_n6, 4);
            __m256 pos7 = _mm256_i32gather_ps(X, indices_p7, 4);
            __m256 neg7 = _mm256_i32gather_ps(X, indices_n7, 4);
            __m256 pos8 = _mm256_i32gather_ps(X, indices_p8, 4);
            __m256 neg8 = _mm256_i32gather_ps(X, indices_n8, 4);
            __m256 pos9 = _mm256_i32gather_ps(X, indices_p9, 4);
            __m256 neg9 = _mm256_i32gather_ps(X, indices_n9, 4);
            __m256 pos10 = _mm256_i32gather_ps(X, indices_p10, 4);
            __m256 neg10 = _mm256_i32gather_ps(X, indices_n10, 4);
            __m256 pos11 = _mm256_i32gather_ps(X, indices_p11, 4);
            __m256 neg11 = _mm256_i32gather_ps(X, indices_n11, 4);
            __m256 pos12 = _mm256_i32gather_ps(X, indices_p12, 4);
            __m256 neg12 = _mm256_i32gather_ps(X, indices_n12, 4);
            __m256 pos13 = _mm256_i32gather_ps(X, indices_p13, 4);
            __m256 neg13 = _mm256_i32gather_ps(X, indices_n13, 4);
            __m256 pos14 = _mm256_i32gather_ps(X, indices_p14, 4);
            __m256 neg14 = _mm256_i32gather_ps(X, indices_n14, 4);
            __m256 pos15 = _mm256_i32gather_ps(X, indices_p15, 4);
            __m256 neg15 = _mm256_i32gather_ps(X, indices_n15, 4);
            res0 = _mm256_add_ps(res0, _mm256_sub_ps(pos0, neg0));
            res1 = _mm256_add_ps(res1, _mm256_sub_ps(pos1, neg1));
            res2 = _mm256_add_ps(res2, _mm256_sub_ps(pos2, neg2));
            res3 = _mm256_add_ps(res3, _mm256_sub_ps(pos3, neg3));
            res4 = _mm256_add_ps(res4, _mm256_sub_ps(pos4, neg4));
            res5 = _mm256_add_ps(res5, _mm256_sub_ps(pos5, neg5));
            res6 = _mm256_add_ps(res6, _mm256_sub_ps(pos6, neg6));
            res7 = _mm256_add_ps(res7, _mm256_sub_ps(pos7, neg7));
            res8 = _mm256_add_ps(res8, _mm256_sub_ps(pos8, neg8));
            res9 = _mm256_add_ps(res9, _mm256_sub_ps(pos9, neg9));
            res10 = _mm256_add_ps(res10, _mm256_sub_ps(pos10, neg10));
            res11 = _mm256_add_ps(res11, _mm256_sub_ps(pos11, neg11));
            res12 = _mm256_add_ps(res12, _mm256_sub_ps(pos12, neg12));
            res13 = _mm256_add_ps(res13, _mm256_sub_ps(pos13, neg13));
            res14 = _mm256_add_ps(res14, _mm256_sub_ps(pos14, neg14));
            res15 = _mm256_add_ps(res15, _mm256_sub_ps(pos15, neg15));
        }
        float* Ybase = result + j * 128;
        _mm256_store_ps(Ybase + 0, res0);
        _mm256_store_ps(Ybase + 8, res1);
        _mm256_store_ps(Ybase + 16, res2);
        _mm256_store_ps(Ybase + 24, res3);
        _mm256_store_ps(Ybase + 32, res4);
        _mm256_store_ps(Ybase + 40, res5);
        _mm256_store_ps(Ybase + 48, res6);
        _mm256_store_ps(Ybase + 56, res7);
        _mm256_store_ps(Ybase + 64, res8);
        _mm256_store_ps(Ybase + 72, res9);
        _mm256_store_ps(Ybase + 80, res10);
        _mm256_store_ps(Ybase + 88, res11);
        _mm256_store_ps(Ybase + 96, res12);
        _mm256_store_ps(Ybase + 104, res13);
        _mm256_store_ps(Ybase + 112, res14);
        _mm256_store_ps(Ybase + 120, res15);
    }
}

void GEMV_CPU_FP32_rowMajor_TCSC_Uniform_G16_AVX512_OpenMP(const float* X, const int32_t NonZeroPerCol, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 16; j++) {
        __m512 res0 = _mm512_setzero_ps();
        for (int k = j * 16 * NonZeroPerCol; k < (j + 1) * 16 * NonZeroPerCol; k += 32) {
            __m512i indices_p0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 0));
            __m512i indices_n0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 16));
            __m512 pos0 = _mm512_i32gather_ps(indices_p0, X, 4);
            __m512 neg0 = _mm512_i32gather_ps(indices_n0, X, 4);
            res0 = _mm512_add_ps(res0, _mm512_sub_ps(pos0, neg0));
        }
        float* Ybase = result + j * 16;
        _mm512_store_ps(Ybase + 0, res0);
    }
}

void GEMV_CPU_FP32_rowMajor_TCSC_Uniform_G32_AVX512_OpenMP(const float* X, const int32_t NonZeroPerCol, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 32; j++) {
        __m512 res0 = _mm512_setzero_ps();
        __m512 res1 = _mm512_setzero_ps();
        for (int k = j * 32 * NonZeroPerCol; k < (j + 1) * 32 * NonZeroPerCol; k += 64) {
            __m512i indices_p0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 0));
            __m512i indices_p1 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 16));
            __m512i indices_n0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 32));
            __m512i indices_n1 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 48));
            __m512 pos0 = _mm512_i32gather_ps(indices_p0, X, 4);
            __m512 pos1 = _mm512_i32gather_ps(indices_p1, X, 4);
            __m512 neg0 = _mm512_i32gather_ps(indices_n0, X, 4);
            __m512 neg1 = _mm512_i32gather_ps(indices_n1, X, 4);
            res0 = _mm512_add_ps(res0, _mm512_sub_ps(pos0, neg0));
            res1 = _mm512_add_ps(res1, _mm512_sub_ps(pos1, neg1));
        }
        float* Ybase = result + j * 32;
        _mm512_store_ps(Ybase + 0, res0);
        _mm512_store_ps(Ybase + 16, res1);
    }
}

void GEMV_CPU_FP32_rowMajor_TCSC_Uniform_G64_AVX512_OpenMP(const float* X, const int32_t NonZeroPerCol, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 64; j++) {
        __m512 res0 = _mm512_setzero_ps();
        __m512 res1 = _mm512_setzero_ps();
        __m512 res2 = _mm512_setzero_ps();
        __m512 res3 = _mm512_setzero_ps();
        for (int k = j * 64 * NonZeroPerCol; k < (j + 1) * 64 * NonZeroPerCol; k += 128) {
            __m512i indices_p0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 0));
            __m512i indices_p1 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 16));
            __m512i indices_p2 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 32));
            __m512i indices_p3 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 48));
            __m512i indices_n0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 64));
            __m512i indices_n1 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 80));
            __m512i indices_n2 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 96));
            __m512i indices_n3 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 112));
            __m512 pos0 = _mm512_i32gather_ps(indices_p0, X, 4);
            __m512 pos1 = _mm512_i32gather_ps(indices_p1, X, 4);
            __m512 pos2 = _mm512_i32gather_ps(indices_p2, X, 4);
            __m512 pos3 = _mm512_i32gather_ps(indices_p3, X, 4);
            __m512 neg0 = _mm512_i32gather_ps(indices_n0, X, 4);
            __m512 neg1 = _mm512_i32gather_ps(indices_n1, X, 4);
            __m512 neg2 = _mm512_i32gather_ps(indices_n2, X, 4);
            __m512 neg3 = _mm512_i32gather_ps(indices_n3, X, 4);
            res0 = _mm512_add_ps(res0, _mm512_sub_ps(pos0, neg0));
            res1 = _mm512_add_ps(res1, _mm512_sub_ps(pos1, neg1));
            res2 = _mm512_add_ps(res2, _mm512_sub_ps(pos2, neg2));
            res3 = _mm512_add_ps(res3, _mm512_sub_ps(pos3, neg3));
        }
        float* Ybase = result + j * 64;
        _mm512_store_ps(Ybase + 0, res0);
        _mm512_store_ps(Ybase + 16, res1);
        _mm512_store_ps(Ybase + 32, res2);
        _mm512_store_ps(Ybase + 48, res3);
    }
}

void GEMV_CPU_FP32_rowMajor_TCSC_Uniform_G128_AVX512_OpenMP(const float* X, const int32_t NonZeroPerCol, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 128; j++) {
        __m512 res0 = _mm512_setzero_ps();
        __m512 res1 = _mm512_setzero_ps();
        __m512 res2 = _mm512_setzero_ps();
        __m512 res3 = _mm512_setzero_ps();
        __m512 res4 = _mm512_setzero_ps();
        __m512 res5 = _mm512_setzero_ps();
        __m512 res6 = _mm512_setzero_ps();
        __m512 res7 = _mm512_setzero_ps();
        for (int k = j * 128 * NonZeroPerCol; k < (j + 1) * 128 * NonZeroPerCol; k += 256) {
            __m512i indices_p0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 0));
            __m512i indices_p1 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 16));
            __m512i indices_p2 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 32));
            __m512i indices_p3 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 48));
            __m512i indices_p4 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 64));
            __m512i indices_p5 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 80));
            __m512i indices_p6 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 96));
            __m512i indices_p7 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 112));
            __m512i indices_n0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 128));
            __m512i indices_n1 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 144));
            __m512i indices_n2 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 160));
            __m512i indices_n3 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 176));
            __m512i indices_n4 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 192));
            __m512i indices_n5 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 208));
            __m512i indices_n6 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 224));
            __m512i indices_n7 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 240));
            __m512 pos0 = _mm512_i32gather_ps(indices_p0, X, 4);
            __m512 pos1 = _mm512_i32gather_ps(indices_p1, X, 4);
            __m512 pos2 = _mm512_i32gather_ps(indices_p2, X, 4);
            __m512 pos3 = _mm512_i32gather_ps(indices_p3, X, 4);
            __m512 pos4 = _mm512_i32gather_ps(indices_p4, X, 4);
            __m512 pos5 = _mm512_i32gather_ps(indices_p5, X, 4);
            __m512 pos6 = _mm512_i32gather_ps(indices_p6, X, 4);
            __m512 pos7 = _mm512_i32gather_ps(indices_p7, X, 4);
            __m512 neg0 = _mm512_i32gather_ps(indices_n0, X, 4);
            __m512 neg1 = _mm512_i32gather_ps(indices_n1, X, 4);
            __m512 neg2 = _mm512_i32gather_ps(indices_n2, X, 4);
            __m512 neg3 = _mm512_i32gather_ps(indices_n3, X, 4);
            __m512 neg4 = _mm512_i32gather_ps(indices_n4, X, 4);
            __m512 neg5 = _mm512_i32gather_ps(indices_n5, X, 4);
            __m512 neg6 = _mm512_i32gather_ps(indices_n6, X, 4);
            __m512 neg7 = _mm512_i32gather_ps(indices_n7, X, 4);
            res0 = _mm512_add_ps(res0, _mm512_sub_ps(pos0, neg0));
            res1 = _mm512_add_ps(res1, _mm512_sub_ps(pos1, neg1));
            res2 = _mm512_add_ps(res2, _mm512_sub_ps(pos2, neg2));
            res3 = _mm512_add_ps(res3, _mm512_sub_ps(pos3, neg3));
            res4 = _mm512_add_ps(res4, _mm512_sub_ps(pos4, neg4));
            res5 = _mm512_add_ps(res5, _mm512_sub_ps(pos5, neg5));
            res6 = _mm512_add_ps(res6, _mm512_sub_ps(pos6, neg6));
            res7 = _mm512_add_ps(res7, _mm512_sub_ps(pos7, neg7));
        }
        float* Ybase = result + j * 128;
        _mm512_store_ps(Ybase + 0, res0);
        _mm512_store_ps(Ybase + 16, res1);
        _mm512_store_ps(Ybase + 32, res2);
        _mm512_store_ps(Ybase + 48, res3);
        _mm512_store_ps(Ybase + 64, res4);
        _mm512_store_ps(Ybase + 80, res5);
        _mm512_store_ps(Ybase + 96, res6);
        _mm512_store_ps(Ybase + 112, res7);
    }
}

void GEMV_CPU_FP32_rowMajor_TCSC_Uniform_G32_CS16_AVX512_OpenMP(const float* X, const int32_t NonZeroPerCol, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 32; j++) {
        __m512 res0 = _mm512_setzero_ps();
        __m512 res1 = _mm512_setzero_ps();
        for (int k = j * 32 * NonZeroPerCol; k < (j + 1) * 32 * NonZeroPerCol; k += 64) {
            __m512i indices_p0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 0));
            __m512i indices_n0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 16));
            __m512i indices_p1 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 32));
            __m512i indices_n1 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 48));
            __m512 pos0 = _mm512_i32gather_ps(indices_p0, X, 4);
            __m512 neg0 = _mm512_i32gather_ps(indices_n0, X, 4);
            __m512 pos1 = _mm512_i32gather_ps(indices_p1, X, 4);
            __m512 neg1 = _mm512_i32gather_ps(indices_n1, X, 4);
            res0 = _mm512_add_ps(res0, _mm512_sub_ps(pos0, neg0));
            res1 = _mm512_add_ps(res1, _mm512_sub_ps(pos1, neg1));
        }
        float* Ybase = result + j * 32;
        _mm512_store_ps(Ybase + 0, res0);
        _mm512_store_ps(Ybase + 16, res1);
    }
}

void GEMV_CPU_FP32_rowMajor_TCSC_Uniform_G64_CS16_AVX512_OpenMP(const float* X, const int32_t NonZeroPerCol, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 64; j++) {
        __m512 res0 = _mm512_setzero_ps();
        __m512 res1 = _mm512_setzero_ps();
        __m512 res2 = _mm512_setzero_ps();
        __m512 res3 = _mm512_setzero_ps();
        for (int k = j * 64 * NonZeroPerCol; k < (j + 1) * 64 * NonZeroPerCol; k += 128) {
            __m512i indices_p0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 0));
            __m512i indices_n0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 16));
            __m512i indices_p1 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 32));
            __m512i indices_n1 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 48));
            __m512i indices_p2 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 64));
            __m512i indices_n2 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 80));
            __m512i indices_p3 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 96));
            __m512i indices_n3 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 112));
            __m512 pos0 = _mm512_i32gather_ps(indices_p0, X, 4);
            __m512 neg0 = _mm512_i32gather_ps(indices_n0, X, 4);
            __m512 pos1 = _mm512_i32gather_ps(indices_p1, X, 4);
            __m512 neg1 = _mm512_i32gather_ps(indices_n1, X, 4);
            __m512 pos2 = _mm512_i32gather_ps(indices_p2, X, 4);
            __m512 neg2 = _mm512_i32gather_ps(indices_n2, X, 4);
            __m512 pos3 = _mm512_i32gather_ps(indices_p3, X, 4);
            __m512 neg3 = _mm512_i32gather_ps(indices_n3, X, 4);
            res0 = _mm512_add_ps(res0, _mm512_sub_ps(pos0, neg0));
            res1 = _mm512_add_ps(res1, _mm512_sub_ps(pos1, neg1));
            res2 = _mm512_add_ps(res2, _mm512_sub_ps(pos2, neg2));
            res3 = _mm512_add_ps(res3, _mm512_sub_ps(pos3, neg3));
        }
        float* Ybase = result + j * 64;
        _mm512_store_ps(Ybase + 0, res0);
        _mm512_store_ps(Ybase + 16, res1);
        _mm512_store_ps(Ybase + 32, res2);
        _mm512_store_ps(Ybase + 48, res3);
    }
}

void GEMV_CPU_FP32_rowMajor_TCSC_Uniform_G128_CS16_AVX512_OpenMP(const float* X, const int32_t NonZeroPerCol, const int32_t* row_index, float* result, const int N_COL, const int K) {
#pragma omp parallel for
    for (int j = 0; j < N_COL / 128; j++) {
        __m512 res0 = _mm512_setzero_ps();
        __m512 res1 = _mm512_setzero_ps();
        __m512 res2 = _mm512_setzero_ps();
        __m512 res3 = _mm512_setzero_ps();
        __m512 res4 = _mm512_setzero_ps();
        __m512 res5 = _mm512_setzero_ps();
        __m512 res6 = _mm512_setzero_ps();
        __m512 res7 = _mm512_setzero_ps();
        for (int k = j * 128 * NonZeroPerCol; k < (j + 1) * 128 * NonZeroPerCol; k += 256) {
            __m512i indices_p0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 0));
            __m512i indices_n0 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 16));
            __m512i indices_p1 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 32));
            __m512i indices_n1 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 48));
            __m512i indices_p2 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 64));
            __m512i indices_n2 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 80));
            __m512i indices_p3 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 96));
            __m512i indices_n3 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 112));
            __m512i indices_p4 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 128));
            __m512i indices_n4 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 144));
            __m512i indices_p5 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 160));
            __m512i indices_n5 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 176));
            __m512i indices_p6 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 192));
            __m512i indices_n6 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 208));
            __m512i indices_p7 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 224));
            __m512i indices_n7 = _mm512_load_si512(reinterpret_cast<const __m512i*>(row_index + k + 240));
            __m512 pos0 = _mm512_i32gather_ps(indices_p0, X, 4);
            __m512 neg0 = _mm512_i32gather_ps(indices_n0, X, 4);
            __m512 pos1 = _mm512_i32gather_ps(indices_p1, X, 4);
            __m512 neg1 = _mm512_i32gather_ps(indices_n1, X, 4);
            __m512 pos2 = _mm512_i32gather_ps(indices_p2, X, 4);
            __m512 neg2 = _mm512_i32gather_ps(indices_n2, X, 4);
            __m512 pos3 = _mm512_i32gather_ps(indices_p3, X, 4);
            __m512 neg3 = _mm512_i32gather_ps(indices_n3, X, 4);
            __m512 pos4 = _mm512_i32gather_ps(indices_p4, X, 4);
            __m512 neg4 = _mm512_i32gather_ps(indices_n4, X, 4);
            __m512 pos5 = _mm512_i32gather_ps(indices_p5, X, 4);
            __m512 neg5 = _mm512_i32gather_ps(indices_n5, X, 4);
            __m512 pos6 = _mm512_i32gather_ps(indices_p6, X, 4);
            __m512 neg6 = _mm512_i32gather_ps(indices_n6, X, 4);
            __m512 pos7 = _mm512_i32gather_ps(indices_p7, X, 4);
            __m512 neg7 = _mm512_i32gather_ps(indices_n7, X, 4);
            res0 = _mm512_add_ps(res0, _mm512_sub_ps(pos0, neg0));
            res1 = _mm512_add_ps(res1, _mm512_sub_ps(pos1, neg1));
            res2 = _mm512_add_ps(res2, _mm512_sub_ps(pos2, neg2));
            res3 = _mm512_add_ps(res3, _mm512_sub_ps(pos3, neg3));
            res4 = _mm512_add_ps(res4, _mm512_sub_ps(pos4, neg4));
            res5 = _mm512_add_ps(res5, _mm512_sub_ps(pos5, neg5));
            res6 = _mm512_add_ps(res6, _mm512_sub_ps(pos6, neg6));
            res7 = _mm512_add_ps(res7, _mm512_sub_ps(pos7, neg7));
        }
        float* Ybase = result + j * 128;
        _mm512_store_ps(Ybase + 0, res0);
        _mm512_store_ps(Ybase + 16, res1);
        _mm512_store_ps(Ybase + 32, res2);
        _mm512_store_ps(Ybase + 48, res3);
        _mm512_store_ps(Ybase + 64, res4);
        _mm512_store_ps(Ybase + 80, res5);
        _mm512_store_ps(Ybase + 96, res6);
        _mm512_store_ps(Ybase + 112, res7);
    }
}

void GEMM_CPU_FP32_rowMajor_Direct_OpenMP(float* X, int8_t* W1, float* result, int rows, int columns, int inners) {
#pragma omp parallel for
    for (int i = 0; i < rows; i++) {
        for (int j = 0; j < columns; j++) {
            float res = 0;
#pragma omp simd
            for (int k = 0; k < inners; k++) {
                res += (X[i * inners + k] * W1[k * columns + j]);
            }
            result[i * columns + j] = res;
        }
    }
}

void GEMM_CPU_FP32_rowMajor_TCSC_Naive_oneFor_OpenMP(float* X, int16_t* w_neg_row_ind, int32_t* w_neg_col_ptr, int16_t* w_pos_row_ind, int32_t* w_pos_col_ptr, float* result, int rows, int columns, int inners){
#pragma omp parallel for
    for (int j = 0; j < columns; j++) {
        for (int i = 0; i < rows; i++) {
            float res = 0;

            int neg_start = w_neg_col_ptr[j];
            int pos_start = w_pos_col_ptr[j];

            int neg_end = w_neg_col_ptr[j + 1];
            int pos_end = w_pos_col_ptr[j + 1];

            int neg = neg_end - neg_start;
            int pos = pos_end - pos_start;

            int min_neg_pos = std::min(neg, pos);

            
            for (int k = 0; k < min_neg_pos; k++) {
                res -= X[i * inners + w_neg_row_ind[neg_start + k]];
                res += X[i * inners + w_pos_row_ind[pos_start + k]];
            }
            for (int k = min_neg_pos; k < pos; k++) {
                res += X[i * inners + w_pos_row_ind[pos_start + k]];
            }
            for (int k = min_neg_pos; k < neg; k++) {
                res -= X[i * inners + w_neg_row_ind[neg_start + k]];
            }

            result[i * columns + j] = res;
        }
    }
}

void GEMM_CPU_FP32_rowMajor_TCSC_Naive_oneFor_4x4_Unroll_OpenMP(float* X, int16_t* w_neg_row_ind, int32_t* w_neg_col_ptr, int16_t* w_pos_row_ind, int32_t* w_pos_col_ptr, float* result, int rows, int columns, int inners) {
#pragma omp parallel for
    for (int j = 0; j < columns; j += 4) {
        int neg_0 = w_neg_col_ptr[j];
        int neg_1 = w_neg_col_ptr[j + 1];
        int neg_2 = w_neg_col_ptr[j + 2];
        int neg_3 = w_neg_col_ptr[j + 3];
        int neg_4 = w_neg_col_ptr[j + 4];

        int pos_0 = w_pos_col_ptr[j];
        int pos_1 = w_pos_col_ptr[j + 1];
        int pos_2 = w_pos_col_ptr[j + 2];
        int pos_3 = w_pos_col_ptr[j + 3];
        int pos_4 = w_pos_col_ptr[j + 4];

        int neg0 = neg_1 - neg_0;
        int neg1 = neg_2 - neg_1;
        int neg2 = neg_3 - neg_2;
        int neg3 = neg_4 - neg_3;

        int pos0 = pos_1 - pos_0;
        int pos1 = pos_2 - pos_1;
        int pos2 = pos_3 - pos_2;
        int pos3 = pos_4 - pos_3;

        int min_neg_pos0 = std::min(neg0, pos0);
        int min_neg_pos1 = std::min(neg1, pos1);
        int min_neg_pos2 = std::min(neg2, pos2);
        int min_neg_pos3 = std::min(neg3, pos3);

        for (int i = 0; i < rows; i += 4) {
            float res00 = 0, res01 = 0, res02 = 0, res03 = 0;
            float res10 = 0, res11 = 0, res12 = 0, res13 = 0;
            float res20 = 0, res21 = 0, res22 = 0, res23 = 0;
            float res30 = 0, res31 = 0, res32 = 0, res33 = 0;

            //0
            for (int k = 0; k < min_neg_pos0; k++) {
                res00 -= X[i * inners + w_neg_row_ind[neg_0 + k]];
                res01 -= X[(i + 1) * inners + w_neg_row_ind[neg_0 + k]];
                res02 -= X[(i + 2) * inners + w_neg_row_ind[neg_0 + k]];
                res03 -= X[(i + 3) * inners + w_neg_row_ind[neg_0 + k]];
                res00 += X[i * inners + w_pos_row_ind[pos_0 + k]];
                res01 += X[(i + 1) * inners + w_pos_row_ind[pos_0 + k]];
                res02 += X[(i + 2) * inners + w_pos_row_ind[pos_0 + k]];                        
                res03 += X[(i + 3) * inners + w_pos_row_ind[pos_0 + k]];
            }
            for (int k = min_neg_pos0; k < pos0; k++) {
                res00 += X[i * inners + w_pos_row_ind[pos_0 + k]];
                res01 += X[(i + 1) * inners + w_pos_row_ind[pos_0 + k]];
                res02 += X[(i + 2) * inners + w_pos_row_ind[pos_0 + k]];                        
                res03 += X[(i + 3) * inners + w_pos_row_ind[pos_0 + k]];
            }
            for (int k = min_neg_pos0; k < neg0; k++) {
                res00 -= X[i * inners + w_neg_row_ind[neg_0 + k]];
                res01 -= X[(i + 1) * inners + w_neg_row_ind[neg_0 + k]];
                res02 -= X[(i + 2) * inners + w_neg_row_ind[neg_0 + k]];
                res03 -= X[(i + 3) * inners + w_neg_row_ind[neg_0 + k]];
            }
                
            //1
            for (int k = 0; k < min_neg_pos1; k++) {
                res10 -= X[i * inners + w_neg_row_ind[neg_1 + k]];
                res11 -= X[(i + 1) * inners + w_neg_row_ind[neg_1 + k]];
                res12 -= X[(i + 2) * inners + w_neg_row_ind[neg_1 + k]];
                res13 -= X[(i + 3) * inners + w_neg_row_ind[neg_1 + k]];
                res10 += X[i * inners + w_pos_row_ind[pos_1 + k]];
                res11 += X[(i + 1) * inners + w_pos_row_ind[pos_1 + k]];
                res12 += X[(i + 2) * inners + w_pos_row_ind[pos_1 + k]];                        
                res13 += X[(i + 3) * inners + w_pos_row_ind[pos_1 + k]];
            }
            for (int k = min_neg_pos1; k < pos1; k++) {
                res10 += X[i * inners + w_pos_row_ind[pos_1 + k]];
                res11 += X[(i + 1) * inners + w_pos_row_ind[pos_1 + k]];
                res12 += X[(i + 2) * inners + w_pos_row_ind[pos_1 + k]];                        
                res13 += X[(i + 3) * inners + w_pos_row_ind[pos_1 + k]];
            }
            for (int k = min_neg_pos1; k < neg1; k++) {
                res10 -= X[i * inners + w_neg_row_ind[neg_1 + k]];
                res11 -= X[(i + 1) * inners + w_neg_row_ind[neg_1 + k]];
                res12 -= X[(i + 2) * inners + w_neg_row_ind[neg_1 + k]];
                res13 -= X[(i + 3) * inners + w_neg_row_ind[neg_1 + k]];
            }

            //2
            for (int k = 0; k < min_neg_pos2; k++) {
                res20 -= X[i * inners + w_neg_row_ind[neg_2 + k]];
                res21 -= X[(i + 1) * inners + w_neg_row_ind[neg_2 + k]];
                res22 -= X[(i + 2) * inners + w_neg_row_ind[neg_2 + k]];
                res23 -= X[(i + 3) * inners + w_neg_row_ind[neg_2 + k]];
                res20 += X[i * inners + w_pos_row_ind[pos_2 + k]];
                res21 += X[(i + 1) * inners + w_pos_row_ind[pos_2 + k]];
                res22 += X[(i + 2) * inners + w_pos_row_ind[pos_2 + k]];                        
                res23 += X[(i + 3) * inners + w_pos_row_ind[pos_2 + k]];
            }
            for (int k = min_neg_pos2; k < pos2; k++) {
                res20 += X[i * inners + w_pos_row_ind[pos_2 + k]];
                res21 += X[(i + 1) * inners + w_pos_row_ind[pos_2 + k]];
                res22 += X[(i + 2) * inners + w_pos_row_ind[pos_2 + k]];                        
                res23 += X[(i + 3) * inners + w_pos_row_ind[pos_2 + k]];
            }
            for (int k = min_neg_pos2; k < neg2; k++) {
                res20 -= X[i * inners + w_neg_row_ind[neg_2 + k]];
                res21 -= X[(i + 1) * inners + w_neg_row_ind[neg_2 + k]];
                res22 -= X[(i + 2) * inners + w_neg_row_ind[neg_2 + k]];
                res23 -= X[(i + 3) * inners + w_neg_row_ind[neg_2 + k]];
            }

            //3
            for (int k = 0; k < min_neg_pos3; k++) {
                res30 -= X[i * inners + w_neg_row_ind[neg_3 + k]];
                res31 -= X[(i + 1) * inners + w_neg_row_ind[neg_3 + k]];
                res32 -= X[(i + 2) * inners + w_neg_row_ind[neg_3 + k]];
                res33 -= X[(i + 3) * inners + w_neg_row_ind[neg_3 + k]];
                res30 += X[i * inners + w_pos_row_ind[pos_3 + k]];
                res31 += X[(i + 1) * inners + w_pos_row_ind[pos_3 + k]];
                res32 += X[(i + 2) * inners + w_pos_row_ind[pos_3 + k]];                        
                res33 += X[(i + 3) * inners + w_pos_row_ind[pos_3 + k]];
            }
            for (int k = min_neg_pos3; k < pos3; k++) {
                res30 += X[i * inners + w_pos_row_ind[pos_3 + k]];
                res31 += X[(i + 1) * inners + w_pos_row_ind[pos_3 + k]];
                res32 += X[(i + 2) * inners + w_pos_row_ind[pos_3 + k]];                        
                res33 += X[(i + 3) * inners + w_pos_row_ind[pos_3 + k]];
            }
            for (int k = min_neg_pos3; k < neg3; k++) {
                res30 -= X[i * inners + w_neg_row_ind[neg_3 + k]];
                res31 -= X[(i + 1) * inners + w_neg_row_ind[neg_3 + k]];
                res32 -= X[(i + 2) * inners + w_neg_row_ind[neg_3 + k]];
                res33 -= X[(i + 3) * inners + w_neg_row_ind[neg_3 + k]];
            }

            result[i * columns + j] = res00;
            result[i * columns + j + 1] = res10;
            result[i * columns + j + 2] = res20;
            result[i * columns + j + 3] = res30;

            result[(i + 1) * columns + j] = res01;
            result[(i + 1) * columns + j + 1] = res11;
            result[(i + 1) * columns + j + 2] = res21;
            result[(i + 1) * columns + j + 3] = res31;

            result[(i + 2) * columns + j] = res02;
            result[(i + 2) * columns + j + 1] = res12;
            result[(i + 2) * columns + j + 2] = res22;
            result[(i + 2) * columns + j + 3] = res32;

            result[(i + 3) * columns + j] = res03;
            result[(i + 3) * columns + j + 1] = res13;
            result[(i + 3) * columns + j + 2] = res23;
            result[(i + 3) * columns + j + 3] = res33;
                            
        }
    }
}