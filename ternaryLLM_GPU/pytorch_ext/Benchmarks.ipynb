{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9db1d34-78ca-4ee0-96dc-5a9fda22920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.path)\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97edcfbd-c474-4371-b965-4ada895687d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, json, gc\n",
    "import numpy as np\n",
    "from transformers.models.llama.modeling_llama import LlamaAttention, LlamaRotaryEmbedding\n",
    "from TernaryLLM import TernaryConfig, LlamaTernaryAttention\n",
    "\n",
    "\n",
    "def benchmark_attn(config: TernaryConfig, ternarization: bool = True, seq_len = 256):\n",
    "    if ternarization:\n",
    "        attn_layer = LlamaTernaryAttention(config, 0).to('cuda')\n",
    "    else:\n",
    "        attn_layer = LlamaAttention(config, 0).to('cuda')\n",
    "        # attn_layer = LlamaSdpaAttention(llama_config, 0).to('cuda')\n",
    "    \n",
    "    torch.cuda.init()\n",
    "    torch.cuda.synchronize()\n",
    " \n",
    "    input_hidden_states = torch.rand((1, seq_len, llama_config.hidden_size)).to('cuda')\n",
    "    rotary_emb = LlamaRotaryEmbedding(llama_config).to('cuda')\n",
    "    position_ids = torch.arange(seq_len, dtype=torch.int32).unsqueeze(0).to('cuda')\n",
    "    cos, sin = rotary_emb(input_hidden_states, position_ids)\n",
    "    position_embeddings = (cos, sin)\n",
    "\n",
    "    attn_spans = []\n",
    "    attn_mems = []\n",
    "\n",
    "    # warm-up\n",
    "    for i in range(10):\n",
    "        with torch.no_grad():\n",
    "            output = attn_layer(hidden_states=input_hidden_states, position_embeddings=position_embeddings, attention_mask=None)\n",
    "    # torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    for i in range(20):\n",
    "        start_event = torch.cuda.Event(enable_timing=True, blocking=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True, blocking=True)\n",
    "        start_event.record()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = attn_layer(hidden_states=input_hidden_states.detach(), position_embeddings=position_embeddings,  attention_mask=None)\n",
    "        reserv_mem = torch.cuda.memory_reserved(0)\n",
    "        alloc_mem = torch.cuda.memory_allocated(0)\n",
    "\n",
    "        end_event.record()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        # collect stat\n",
    "        elapsed_time = start_event.elapsed_time(end_event)\n",
    "        attn_spans.append(elapsed_time)\n",
    "        attn_mems.append(alloc_mem)\n",
    "        \n",
    "        del output\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    attn_spans.pop(0)\n",
    "    attn_spans = np.array(sorted(attn_spans, reverse=(not ternarization)))\n",
    "    \n",
    "    return attn_spans, np.array(attn_mems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed953e74-ebd9-4327-93e8-63c5bc8d1b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run benchmark\n",
    "with open('./config/config.json') as f:\n",
    "    llama_3_1b_json = json.load(f)\n",
    "\n",
    "llama_config = TernaryConfig(\n",
    "    vocab_size=llama_3_1b_json[\"vocab_size\"],\n",
    "    hidden_size=llama_3_1b_json[\"hidden_size\"],\n",
    "    intermediate_size=llama_3_1b_json[\"intermediate_size\"],\n",
    "    attention_dropout=llama_3_1b_json[\"attention_dropout\"],\n",
    "    num_attention_heads=llama_3_1b_json[\"num_attention_heads\"],\n",
    "    head_dim=llama_3_1b_json[\"head_dim\"],\n",
    "    num_key_value_heads=llama_3_1b_json[\"num_key_value_heads\"],\n",
    "    max_position_embeddings=llama_3_1b_json[\"max_position_embeddings\"],\n",
    "    rope_theta=llama_3_1b_json[\"rope_theta\"],\n",
    "    rope_scaling=llama_3_1b_json[\"rope_scaling\"],\n",
    "    sparsity=llama_3_1b_json[\"sparsity\"],\n",
    "    uniform_sparsity=llama_3_1b_json[\"uniform_sparsity\"],\n",
    "    uniform_sparsity_block_size=llama_3_1b_json[\"uniform_sparsity_block_size\"],\n",
    "    padding=llama_3_1b_json[\"padding\"],\n",
    "    padding_size=llama_3_1b_json[\"padding_size\"]\n",
    ")\n",
    "\n",
    "sparsities = [i/100 for i in range(70, 100, 1)]\n",
    "# sparsities.append(0.999)\n",
    "\n",
    "import sys\n",
    "ternary_benchmark = False\n",
    "log = True \n",
    "\n",
    "seq_len = 256\n",
    "log_ter_time = open(f'./data/llama_3_1b_ter_attn_nonuniform_time_{seq_len}', \"w\")\n",
    "log_ter_mem = open(f'./data/llama_3_1b_ter_attn_nonuniform_mem_{seq_len}', \"w\")\n",
    "log_vanila_time = open(f'./data/llama_3_1b_vanila_attn_time_{seq_len}', \"w\")\n",
    "log_vanila_mem = open(f'./data/llama_3_1b_vanila_attn_mem_{seq_len}', \"w\")\n",
    "\n",
    "for sp in sparsities:\n",
    "    llama_config.sparsity = sp\n",
    "    ter_attn_spans, ter_attn_mems = benchmark_attn(llama_config, True, seq_len)\n",
    "    print(f\"TerSpMM  {sp:.2f} {ter_attn_spans[0:10].mean():.4f} ms {ter_attn_mems.mean()/1024/1024:.4f} MB\")\n",
    "    log_ter_time.write(f\"{sp} {ter_attn_spans[0:10].mean()}\\n\")\n",
    "    log_ter_mem.write(f\"{sp} {ter_attn_mems.mean()}\\n\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    vanila_attn_spans, vanila_attn_mems = benchmark_attn(llama_config, False, seq_len)\n",
    "    print(f\"nnLinear {sp:.2f} {vanila_attn_spans[0:10].mean():.4f} ms {vanila_attn_mems.mean()/1024/1024:.4f} MB\")\n",
    "    log_vanila_time.write(f\"{sp} {vanila_attn_spans[0:10].mean()}\\n\")\n",
    "    log_vanila_mem.write(f\"{sp} {vanila_attn_mems.mean()}\\n\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "log_ter_time.close()\n",
    "log_ter_mem.close()\n",
    "log_vanila_time.close()\n",
    "log_vanila_mem.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b3e53a5-49f2-4596-b183-2fa1264441ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.llama.modeling_llama import LlamaMLP\n",
    "from TernaryLLM import LlamaTernaryMLP, TernaryConfig\n",
    "\n",
    "import torch, json, time, sys\n",
    "import numpy as np\n",
    "\n",
    "def benchmark_mlp(config: TernaryConfig, ternarization: bool = True, seq_len = 256):\n",
    "    mlp_layer = None\n",
    "    if ternarization:\n",
    "        mlp_layer = LlamaTernaryMLP(config).to('cuda')\n",
    "    else:\n",
    "        mlp_layer = LlamaMLP(config).to('cuda')\n",
    "\n",
    "    torch.cuda.init()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    input = torch.randn((1, seq_len, config.hidden_size)).to('cuda')\n",
    "\n",
    "    mlp_cpu_spans = []\n",
    "    mlp_spans = []\n",
    "    mlp_mems = []\n",
    "\n",
    "    # warm-up\n",
    "    for _ in range(10):\n",
    "        with torch.no_grad():\n",
    "            output = mlp_layer(input)\n",
    "    \n",
    "    # benchmark\n",
    "    for i in range(100):\n",
    "        start_event = torch.cuda.Event(enable_timing=True, blocking=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True, blocking=True)\n",
    "        start_event.record()\n",
    "        start_cpu = time.time()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mlp_layer(input)\n",
    "        alloc_mem = torch.cuda.memory_allocated(0)\n",
    "\n",
    "        end_event.record()\n",
    "        torch.cuda.synchronize()\n",
    "        end_cpu = time.time()\n",
    "\n",
    "        # collect stat\n",
    "        elapsed_time = start_event.elapsed_time(end_event)\n",
    "        elapsed_cpu_time = end_cpu - start_cpu\n",
    "        mlp_spans.append(elapsed_time)\n",
    "        mlp_mems.append(alloc_mem)\n",
    "        mlp_cpu_spans.append(elapsed_cpu_time*1000)\n",
    "    \n",
    "    mlp_spans.pop(0)\n",
    "    mlp_spans = np.array(sorted(mlp_spans, reverse=(not ternarization)))\n",
    "    mlp_cpu_spans = np.array(sorted(mlp_cpu_spans, reverse=(not ternarization)))\n",
    "\n",
    "    return mlp_cpu_spans, mlp_spans, np.array(mlp_mems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd52d78-09bf-4c87-bfe8-0ad784a4c3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./config/config.json') as f:\n",
    "    llama_3_1b_json = json.load(f)\n",
    "\n",
    "llama_config = TernaryConfig(\n",
    "    vocab_size=llama_3_1b_json[\"vocab_size\"],\n",
    "    hidden_size=llama_3_1b_json[\"hidden_size\"],\n",
    "    intermediate_size=llama_3_1b_json[\"intermediate_size\"],\n",
    "    attention_dropout=llama_3_1b_json[\"attention_dropout\"],\n",
    "    num_attention_heads=llama_3_1b_json[\"num_attention_heads\"],\n",
    "    head_dim=llama_3_1b_json[\"head_dim\"],\n",
    "    num_key_value_heads=llama_3_1b_json[\"num_key_value_heads\"],\n",
    "    max_position_embeddings=llama_3_1b_json[\"max_position_embeddings\"],\n",
    "    rope_theta=llama_3_1b_json[\"rope_theta\"],\n",
    "    rope_scaling=llama_3_1b_json[\"rope_scaling\"],\n",
    "    hidden_act=llama_3_1b_json[\"hidden_act\"],\n",
    "    mlp_bias=llama_3_1b_json[\"mlp_bias\"],\n",
    "    sparsity=llama_3_1b_json[\"sparsity\"],\n",
    "    uniform_sparsity=llama_3_1b_json[\"uniform_sparsity\"],\n",
    "    uniform_sparsity_block_size=llama_3_1b_json[\"uniform_sparsity_block_size\"],\n",
    "    padding=llama_3_1b_json[\"padding\"],\n",
    "    padding_size=llama_3_1b_json[\"padding_size\"]\n",
    ")\n",
    "\n",
    "ternary_benchmark = False\n",
    "log = True\n",
    "sparsities = [i/100 for i in range(70, 100, 1)]\n",
    "seq_len = 256\n",
    "\n",
    "log_ter_time = open(f'./data/llama_3_1b_ter_mlp_uniform_time_{seq_len}', \"w\")\n",
    "log_ter_mem = open(f'./data/llama_3_1b_ter_mlp_uniform_mem_{seq_len}', \"w\")\n",
    "log_vanila_time = open(f'./data/llama_3_1b_vanila_mlp_time_{seq_len}', \"w\")\n",
    "log_vanila_mem = open(f'./data/llama_3_1b_vanila_mlp_mem_{seq_len}', \"w\")\n",
    "\n",
    "for sp in sparsities:\n",
    "    llama_config.sparsity = sp\n",
    "    ter_cpu_spans, ter_spans, ter_mems = benchmark_mlp(llama_config, True, seq_len)\n",
    "    print(f\"TerSpMM MLP  {sp:.2f} {ter_spans[0:10].mean():.4f} ms {ter_mems.mean()/1024/1024:.4f} MB\")\n",
    "    log_ter_time.write(f\"{sp} {ter_spans[0:10].mean()}\\n\")\n",
    "    log_ter_mem.write(f\"{sp} {ter_mems.mean()}\\n\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    vanila_cpu_spans, vanila_spans, vanila_mems = benchmark_mlp(llama_config, False, seq_len)\n",
    "    print(f\"nnLinear MLP {sp:.2f} {vanila_spans[0:10].mean():.4f} ms {vanila_mems.mean()/1024/1024:.4f} MB\")\n",
    "    log_vanila_time.write(f\"{sp} {vanila_spans[0:10].mean()}\\n\")\n",
    "    log_vanila_mem.write(f\"{sp} {vanila_mems.mean()}\\n\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38be23d5-12fd-4545-9715-4736a9c3a351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "from TernaryLLM.TernaryLlama import (\n",
    "    add_padding_to_token,\n",
    "    prepare_ternary_model\n",
    ")\n",
    "from TernaryLLM.configuration_ternary import TernaryConfig\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch, time, sys, json\n",
    "import numpy as np\n",
    "from accelerate import init_empty_weights\n",
    "import subprocess, threading\n",
    "\n",
    "\n",
    "power_monitor_lock = threading.Lock()\n",
    "power_monitor = False\n",
    "total_energy_joules = 0\n",
    "walt_trends = []\n",
    "def collect_power(interval: float = 0.1):\n",
    "    global total_energy_joules\n",
    "    global power_monitor\n",
    "    global power_monitor_lock\n",
    "    global walt_trends\n",
    "    while True:\n",
    "        with power_monitor_lock:\n",
    "            if not power_monitor:\n",
    "                break\n",
    "        time.sleep(interval)\n",
    "        output = subprocess.run(\n",
    "            [\"nvidia-smi\", \"--query-gpu=power.draw\", \"--format=csv,nounits\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "        )\n",
    "        power_w = float(output.stdout.split(\"\\n\")[1])\n",
    "        walt_trends.append(power_w)\n",
    "        total_energy_joules += power_w * interval\n",
    "\n",
    "def benchmark_llama_raw(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    config: TernaryConfig, \n",
    "    text: str, \n",
    "    interval: float = 0.1,\n",
    "    ternarization: bool = True\n",
    "):\n",
    "    model = prepare_ternary_model(model, config) if ternarization else model\n",
    "    model.to('cuda')    # move to gpu\n",
    "\n",
    "    # Tokenize the input string and add paddings to tokens\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "    input_ids = add_padding_to_token(input_ids, tokenizer, 4, 'cuda')\n",
    "    print(f\"Sequence length: {input_ids.size(1)}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(5):\n",
    "            output = model(input_ids)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # prepare power benchmark\n",
    "    global total_energy_joules\n",
    "    global power_monitor\n",
    "    global power_monitor_lock\n",
    "    global walt_trends\n",
    "    # start power measurement\n",
    "    with power_monitor_lock:\n",
    "        power_monitor = True\n",
    "    power_monitor_thread = threading.Thread(target=collect_power, args=(interval,), daemon=True)\n",
    "    power_monitor_thread.start()\n",
    "\n",
    "    # prepare time benchmark\n",
    "    gen_mems = []\n",
    "    total_tokens = 0\n",
    "    cpu_ts = time.monotonic()\n",
    "\n",
    "    # start generation\n",
    "    with torch.no_grad():\n",
    "        for i in range(1):\n",
    "\n",
    "            output = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_new_tokens=500,\n",
    "                do_sample=False,\n",
    "                use_cache=False,\n",
    "            )\n",
    "            alloc_mem = torch.cuda.memory_allocated(0)\n",
    "\n",
    "            # Decode the output tokens to a string\n",
    "            print(output.shape[-1])\n",
    "            num_tokens = output.shape[-1] - input_ids.size(1)\n",
    "            total_tokens += num_tokens\n",
    "            gen_mems.append(alloc_mem)\n",
    "            torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    elapsed_time_sec = time.monotonic() - cpu_ts\n",
    "    gen_mems = np.array(gen_mems)\n",
    "    \n",
    "    # end power measurement\n",
    "    with power_monitor_lock:\n",
    "        power_monitor = False\n",
    "    power_monitor_thread.join()\n",
    "    walt_trends = np.array(walt_trends)\n",
    "\n",
    "    return (total_tokens, total_tokens/elapsed_time_sec, (25*input_ids.size(1))/elapsed_time_sec, gen_mems, walt_trends, total_energy_joules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf9d02-30b8-4929-82ca-4624c6f68f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ternary_benchmark = False\n",
    "\n",
    "# load ternary model config\n",
    "with open('./config/config.json') as f:\n",
    "    llama_3_1b_json = json.load(f)\n",
    "ternary_llama_config = TernaryConfig(\n",
    "    ternary_attn_linear=llama_3_1b_json[\"ternary_attn_linear\"],\n",
    "    ternary_mlp=llama_3_1b_json[\"ternary_mlp\"],\n",
    "    sparsity=llama_3_1b_json[\"sparsity\"],\n",
    "    uniform_sparsity=llama_3_1b_json[\"uniform_sparsity\"],\n",
    "    uniform_sparsity_block_size=llama_3_1b_json[\"uniform_sparsity_block_size\"],\n",
    "    padding=llama_3_1b_json[\"padding\"],\n",
    "    padding_size=llama_3_1b_json[\"padding_size\"]\n",
    ")\n",
    "\n",
    "# load and prepare float model\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float32,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "model.tie_weights()\n",
    "model.to_empty(device=\"cuda\")  # Allocates empty tensors on GPU\n",
    "model.load_state_dict(model.state_dict())  # Re-initializes weights\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# load the dataset from ag news\n",
    "dataset = load_dataset(\"fancyzhx/ag_news\")\n",
    "\n",
    "# prepare log file\n",
    "log_time = open('./data/llama_3_1b_ter_llama_generation', \"a\") if ternary_benchmark else open('./data/llama_3_1b_vanilla_llama_generation', \"w\")\n",
    "\n",
    "total_tokens, output_token_throughput, input_token_throughput, mems, walts, total_engry = \\\n",
    "    benchmark_llama_raw(\n",
    "        model, tokenizer, ternary_llama_config, \n",
    "        text=dataset[\"train\"][0]['text'], \n",
    "        interval=0.1, ternarization=ternary_benchmark)\n",
    "print(f\"Total Generated Token: {total_tokens} Throughput: {output_token_throughput} {input_token_throughput} | Memory: {mems.mean()/1024/1024}MB\")\n",
    "print(f\"Power consumption: {total_engry} joules; Avg Watt: {walts.mean():.2f}W\")\n",
    "log_time.write(f\"{llama_3_1b_json[\"sparsity\"]} {output_token_throughput}\\n\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "log_time.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f38e91b-5014-4f3c-b978-9d260269b423",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(torch_gpu)",
   "language": "python",
   "name": "torch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
